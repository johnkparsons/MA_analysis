---
title: "MA_analysis_rmd"
author: "John Parsons"
date: "Compiled on `r format(Sys.Date(), '%B %d, %Y`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Setup
```{r, eval=FALSE}
#install.packages(c("tidyverse", "dplyr", "lme4", "glmmTMB", "effects", "DHARMa", "MuMIn", "plyr", "broom.mixed", "ggthemes", "padr", "ptest", "maps", "mapdata", "hexbin", "car", "lubridate", "slider", "hms", "olsrr", "bbmle", "emmeans", "gt", "webshot", "cowplot", "jtools", "ggstance", "AER", "mgcv", "itsadug", "installr"))
#install.packages("ggthemes")
library(tidyverse)
slice <- dplyr::slice
#library(plyr); library(dplyr)
library(lme4) #modelling
library(glmmTMB) #modelling
library(DHARMa) #model diagnostics
library(sjstats)
library(effects)
library(broom.mixed)
library(MuMIn)
library(ggthemes) #visualizations
library(RColorBrewer) #visualizations
library(padr)
library(ptest)
library(maps)
library(mapdata)
library(ggmap)
library(hexbin) #heatmaps
library(zoo)
library(car)
library(lubridate) #dates
library(slider)
library(hms)
year <- lubridate::year
week <- lubridate::week
library(olsrr)
library(bbmle) #AIC tables
library(emmeans)#effects testing
library(gt)#nice tables
library(webshot) #saving gt tables
library(cowplot) #moo
library(simfit) #simulating models
library(jtools)
library(ggstance)
library(AER)
library(mgcv) #GAMs
library(visreg) #visualizing GAMs
library(itsadug)
library(installr)
#webshot::install_phantomjs() 
#citation("ptest")

updateR()
citation()
RStudio.Version()
```

===========================================================================

# Data Tidying

## 2019 Data Tidying

### Flight-Level Data Tidying (1 row = 1 day) - 2019
```{r}
dat_2019 <- read.csv("data_raw/2019_flight_data.csv")
#view(dat_2019)

dat_2019 <- dat_2019 %>% 
  dplyr::select(!c(3:9,13,14,16:17,20,21,23:25,28,29)) %>% 
  mutate(total_unique = transect_total) %>% 
  mutate(transect_datetime = as_datetime(transect_datetime))

dat_2019[dat_2019 == ""] <- NA

#dat_2019$date <- as.Date(dat_2019$date, "%m/%d/%Y")

dat_2019 <- dat_2019 %>%
  mutate(datetime = as_datetime(transect_datetime)) %>%
  mutate(date_hour = round_date(as_datetime(datetime), "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2019
```{r}
dat_2019 <- dat_2019 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort = 
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = if_else(transect_small == 0, 0,               #transect_small/transect_effort)) %>%
  #mutate(transect_large_density = if_else(transect_large == 0,0,
                                          #transect_large/transect_effort))
#view(dat_2019)
```

### Reading in tide data (Station 9411340) - 2019
```{r}
tide_dat_2019 <- read.csv("data_raw/2019_tide_dat.txt")
#View(tide_dat_2019)
#str(tide_dat_2019)

tide_dat_2019 <- tide_dat_2019 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2019 <- left_join(dat_2019, tide_dat_2019, by = "date_hour")
#view(dat_2019)
```

### Reading in channel buoy data (Station 46053) - 2019
```{r}
channel_dat_2019_raw <- read.table("data_raw/2019_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2019_raw)
#View(channel_dat_2019_raw)

channel_dat_2019 <- channel_dat_2019_raw %>%
                      mutate_if(is.character, as.numeric) %>% 
                      mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
                      mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
                      group_by(MM, DD, hh) %>% 
                      summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
                      ungroup() %>% 
                      filter(MM > 4) %>% 
                      add_column(year = 2019, .before = 1) %>% 
                      mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
                      mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
                      mutate(date_hour = round_date(datetime, "hour")) %>% 
                      select(!datetime) %>% 
                      select(!c(1:4)) %>% 
                      mutate(max_wtemp_previous_24 = 
                              slide_dbl(WTMP, max, .before = 24)) %>% 
                      mutate(min_wtemp_previous_24 = 
                              slide_dbl(WTMP, min, .before = 24)) %>% 
                      mutate(mean_wtemp_previous_24 = 
                              slide_dbl(WTMP, mean, .before = 24))
#view(channel_dat_2019)

#dat_2019$date_hour <- as_datetime(dat_2019$date_hour)
flights_plus_channel_dat_2019 <- left_join(dat_2019, channel_dat_2019, by = "date_hour")

dat_2019 <- flights_plus_channel_dat_2019 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2)

#view(dat_2019)
```

### Size Data Tidying (1 row = 1 individual) - 2019
```{r}
size_dat_2019 <- read.csv("data_raw/size_dat_2019.csv")
#view(size_dat_2019)

size_dat_2019 <- size_dat_2019 %>% 
  select(1:19) %>% 
  dplyr::slice(-(48:49))

size_dat_2019 <- size_dat_2019 %>%
                  mutate(length_adj_NO_RES = as.character(length_adj_NO_RES)) %>% 
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808)

size_dat_2019 %>%
  filter(length_adj_m >= 3) %>% 
  filter(unique == "Y") %>% 
  nrow()
```
*no adults with unadjusted lengths, 15 with adjusted lengths!

## 2020 Data Tidying

### Flight-Level Data Tidying - 2020
```{r}
dat_2020 <- read.csv("data_raw/2020_flight_data.csv")
#view(dat_2020)

#dat_2020$date <- as_date(as.Date(dat_2020$date, "%m/%d/%Y"))

dat_2020$transect_total <- as.numeric(dat_2020$transect_total)

dat_2020$transect_datetime <- as_datetime(dat_2020$transect_datetime)
dat_2020$manual_datetime <- as_datetime(dat_2020$manual_datetime)

dat_2020 <- dat_2020 %>%
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  select(!c(2:8,13,14,16,17,20,21,25,26,28,29:31,38,39))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2020
```{r}
dat_2020 <- dat_2020 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_effort_density =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2020 <- dat_2020 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort =
         manual_total/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340)- 2020
```{r}
tide_dat_2020 <- read.csv("data_raw/2020_tide_dat.txt")
#View(tide_dat_2020)
#str(tide_dat_2020)

tide_dat_2020 <- tide_dat_2020 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>%
  select(-c(1,3,4)) %>%
  rename(tide = 2)

tide_added_2020 <- left_join(dat_2020, tide_dat_2020, by = "date_hour")
#view(tide_added_2020)
```

### Reading in channel buoy data (Station 46053) - 2020
```{r}
channel_dat_2020_raw <- read.table("data_raw/2020_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2020_raw)
#View(channel_dat_2020_raw)

channel_dat_2020 <- channel_dat_2020_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 4) %>% 
  add_column(year = 2020, .before = 1) %>% 
  mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
  mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  select(!datetime) %>% 
  select(!c(1:4)) %>%
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))

#tide_added_2020$date_hour <- as_datetime(tide_added_2020$date_hour)
flights_plus_channel_dat_2020 <- left_join(tide_added_2020, channel_dat_2020, by = "date_hour")
#view(flights_plus_channel_dat_2020)

dat_2020 <- flights_plus_channel_dat_2020 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) 
#view(dat_2020)
```

### Size Data Tidying (1 row = 1 individual) - 2020
```{r}
size_dat_2020 <- read.csv("data_raw/size_dat_2020.csv")
#view(size_dat_2020)

size_dat_2020 <- size_dat_2020 %>%
                  slice(-363) %>%
                  mutate(length_raw = as.numeric(length_raw)) %>%
                  mutate(length_adj = as.numeric(length_adj)) %>% 
                  mutate(order = as.numeric(order)) %>% 
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808) %>%
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) %>%
                  mutate(depth.correction.factor =
                         as.numeric(depth.correction.factor)) %>% 
                  mutate(asl.correction.factor =
                           as.numeric(asl.correction.factor))

size_dat_2020 %>%
  filter(unique == "Y") %>%
  filter(length_raw_m >= 3) %>% 
  nrow()
#18 adults unadjusted
#48 adults adjusted

size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  filter(length_raw_m < 3) %>% 
  nrow()
#171 juveniles unadjusted
#141 juveniles adjusted
```
* roughly 10x more juveniles than adults (uncorrected lengths)

## 2021 Data Tidying

### Flight-Level Data Tidying - 2021
```{r}
dat_2021 <- read.csv("data_raw/2021_flight_data.csv")
#view(dat_2021)

#dat_2021 <- dplyr::rename(dat_2021, date = 1)
#dat_2021$date <- as_date(as.Date(dat_2021$date, "%m/%d/%Y"))

dat_2021$transect_total <- as.numeric(dat_2021$transect_total)
dat_2021$manual_unique <- as.numeric(dat_2021$manual_unique)
dat_2021$manual_total <- as.numeric(dat_2021$manual_total)

dat_2021$transect_datetime <- as_datetime(dat_2021$transect_datetime)
dat_2021$manual_datetime <- as_datetime(dat_2021$manual_datetime)

dat_2021 <- dat_2021 %>%
  select(!c(3:6,9,10,15,16,18,19,22,23,29:32,39:41)) %>% 
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2021
```{r}
dat_2021 <- dat_2021 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2021 <- dat_2021 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort = 
         manual_unique/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340) - 2021
```{r}
tide_dat_2021 <- read.csv("data_raw/2021_tide_dat.txt")
#View(tide_dat_2021)
#str(tide_dat_2021)
#Local time

tide_dat_2021 <- tide_dat_2021 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2021 <- left_join(dat_2021, tide_dat_2021, by = "date_hour")
#view(dat_2021)
```

### Reading in channel buoy data (Station 46053) - 2021
```{r}
channel_dat_2021_jan <- read.table("data_raw/2021_buoy_dat_jan.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_feb <- read.table("data_raw/2021_buoy_dat_feb.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_mar <- read.table("data_raw/2021_buoy_dat_mar.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_apr <- read.table("data_raw/2021_buoy_dat_apr.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_may <- read.table("data_raw/2021_buoy_dat_may.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jun <- read.table("data_raw/2021_buoy_dat_jun.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jul <- read.table("data_raw/2021_buoy_dat_jul.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_aug <- read.table("data_raw/2021_buoy_dat_aug.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_sep <- read.table("data_raw/2021_buoy_dat_sep.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_oct <- read.table("data_raw/2021_buoy_dat_oct.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_nov <- read.table("data_raw/2021_buoy_dat_nov.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_dec <- read.table("data_raw/2021_buoy_dat_dec.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_raw <- dplyr::bind_rows(channel_dat_2021_jan, channel_dat_2021_feb, channel_dat_2021_mar, channel_dat_2021_apr, channel_dat_2021_may, channel_dat_2021_jun, channel_dat_2021_jul, channel_dat_2021_aug, channel_dat_2021_sep, channel_dat_2021_oct, channel_dat_2021_nov, channel_dat_2021_dec)
#View(channel_dat_2021_raw)

channel_dat_2021 <- channel_dat_2021_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 3) %>% 
  add_column(year = 2021, .before = 1) %>% 
  mutate(date_hour = make_datetime(year, MM, DD, hh)) %>% 
  mutate(date_hour = with_tz(date_hour, "US/Pacific")) %>%
  select(!c(1:4)) %>%
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))
#View(channel_dat_2021)

#dat_2021$date_hour <- as_datetime(dat_2021$date_hour)
flights_plus_channel_dat_2021 <- left_join(dat_2021, channel_dat_2021, by = "date_hour")
#View(flights_plus_channel_dat_2021)

dat_2021 <- flights_plus_channel_dat_2021 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) 
```

### Size Data Tidying (1 row = 1 individual) - 2021
```{r}
size_dat_2021 <- read.csv("data_raw/size_dat_2021.csv")
#view(size_dat_2021)

size_dat_2021 <- size_dat_2021 %>%
                  mutate(length_raw = as.numeric(length_raw)) %>%
                  mutate(length_adj = as.numeric(length_adj)) %>%
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808) %>%
                  mutate(lat = as.numeric(lat)) %>% 
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) %>%
                  mutate(depth.correction.factor =
                         as.numeric(depth.correction.factor)) %>% 
                  mutate(asl.correction.factor =
                           as.numeric(asl.correction.factor)) %>% 
                  mutate(altitude = as.numeric(altitude))

size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(length_adj_m >= 3) %>% 
  nrow()
#120 adults

size_dat_2021 %>%
  filter(unique == "Y") %>% 
  filter(length_raw_m < 3) %>% 
  nrow()
#337 juveniles
```
MUCH higher % of adults than 2020 - close to 50-50

## Three-Year ("3yr") dataset tidying

### Combine 2019, 2020, and 2021 datasets
```{r}
daily_2yr_dat <- full_join(dat_2019, dat_2020)
#view(daily_2yr_dat)

daily_3yr_dat <- full_join(daily_2yr_dat, dat_2021)
#view(daily_3yr_dat)

daily_3yr_dat <- daily_3yr_dat %>%
  mutate(year = as_factor(year(mdy(date)))) %>% 
  mutate(day = yday(datetime)) %>% 
  mutate(day2 = (yday(datetime))^2)

#View(daily_3yr_dat)
```

### Reading in Carp LTER data
```{r}
lter_dat_raw <- read.csv("data_raw/lter_dat.csv")
#View(lter_dat_raw)

lter_dat <- lter_dat_raw %>% 
  filter(year > 2018) %>% 
  filter(year < 2022) %>% 
  mutate(hour = (24*decimal_time)) %>%
  mutate(hour = round(hour, digits = 1)) %>% 
  mutate(date_hour = make_datetime(year, month, day, hour)) %>% 
  mutate(date_hour = force_tz(date_hour, tzone = "America/Los_Angeles")) %>%
  rename(day_of_month = day) %>% 
  group_by(date_hour) %>% 
  summarize(Temp_top = mean(Temp_top, na.rm = TRUE), salinity = mean(Salinity, na.rm = TRUE)) %>% 
  filter(Temp_top < 100) %>% 
  filter(salinity < 100)

lter_dat %>% 
  filter(month(date_hour) == 8) %>% 
  filter(day(date_hour) == 29) %>% 
  filter(year(date_hour) == 2021) %>% 
  ggplot(aes(x = date_hour, y = salinity)) +
    geom_point() +
    geom_smooth()
```

### Adding LTER data to daily 3yr dataset 
```{r}
daily_3yr_dat_backup <- left_join(daily_3yr_dat, lter_dat, by = "date_hour")

write.csv(daily_3yr_dat_backup, "data/daily_3yr_dat_backup.csv")

#View(daily_3yr_dat_backup)
```

### Recovery of daily_3yr_dat (so you don't have to re-run everything above if you mess it up)
```{r}
daily_3yr_dat <- read.csv("data/daily_3yr_dat_backup.csv")

#View(daily_3yr_dat)
```

## Building 3yr size dataset
```{r}
size_dat_2yr <- full_join(size_dat_2019, size_dat_2020)

size_dat_3yr <- full_join(size_dat_2yr, size_dat_2021)
#view(size_dat_3yr)

size_dat_3yr <- size_dat_3yr %>% 
                  select(!c(26:31))
```

### New 3yr df with size of each unique shark
(1 row = 1 shark, daily data is repeated for days where multiple sharks were observed)
```{r}
size_dat_unique <- size_dat_3yr %>%
                    filter(unique == "Y")

nrow(size_dat_unique)
#919 sightings across 3 years

size_dat_unique$date <- as.character(size_dat_unique$date)
joined_3yr <- left_join(daily_3yr_dat, size_dat_unique, by = "date")

#View(joined_3yr)
```

## Adding size classes and inner/outer/manual to daily 3yr dataframe
```{r}
ggplot(joined_3yr, aes(x = video_YN)) +
  geom_bar()

#View(joined_3yr)

size_classed_3yr <- joined_3yr %>%
  group_by(date) %>%
  dplyr::summarize(juvenile = sum(length_adj_m < 3 & length_adj_m > 0, na.rm = TRUE), 
                   adult = sum(length_adj_m >= 3, na.rm = TRUE),
                   inner_count = sum(video_YN == "inner", na.rm = TRUE),
                   outer_count = sum(video_YN == "outer", na.rm = TRUE) + 
                                 sum(video_YN == "outer ", na.rm = TRUE),
                   manual_count = sum(video_YN == "manual", na.rm = TRUE) +
                                  sum(video_YN == "manual_1", na.rm = TRUE) +
                                  sum(video_YN == "manual_2", na.rm = TRUE))

#ggplot(data = size_classed_3yr, aes(x = adult)) +
  #geom_bar()

daily_3yr_dat <- left_join(daily_3yr_dat, size_classed_3yr, by = "date")
#View(daily_3yr_dat)

daily_3yr_dat <- daily_3yr_dat  %>%
  mutate(juvenile = ifelse(is.na(juvenile), 0, juvenile)) %>% 
  mutate(adult = ifelse(is.na(adult), 0, adult)) %>% 
  mutate(total_sized = juvenile + adult) %>% 
  filter(detection == "Y" | detection == "N") %>%
  mutate(total_area = ifelse(year == 2019, transect_area_km2,
                             ifelse(is.na(manual_area_km2), transect_area_km2, 
                                    ifelse(is.na(transect_area_km2),
                                           manual_area_km2,
                                           manual_area_km2 + transect_area_km2)))) %>% 
  mutate(area_20m = ifelse(year == 2019, transect_area_km2, manual_area_km2))
```

## CSULB data

### Read and filter
```{r}
#Check deployment log:
temp_deployment <- read.csv("data_raw/CSULB/JWS Receiver Deployment Log - Temperature_Log.csv")
#View(temp_deployment)

# UNFILTERED data for 2019:
OS1_1 <- read.csv("data_raw/CSULB/JWS_SantaClaus_OS1-SN20069755_20190515_20190701.csv")
OS2_1 <- read.csv("data_raw/CSULB/JWS_SantaClaus_OS2-SN20566751_20190515_20190701.csv")
OS2_2 <- read.csv("data_raw/CSULB/JWS_SantaClaus_OS2-SN20069754_20190628_20190829.csv") 
OS2_3 <- read.csv("data_raw/CSULB/JWS_SantaClaus_OS2-SN20031403_20190819_20191109.csv")
OS2_4 <- read.csv("data_raw/CSULB/JWS_SantaClaus_OS2-SN20069752_20191106_20191216.csv")

OS1_filtered <- OS1_1 %>%
  mutate(DateTimePST = as_datetime(DateTimePST)) %>% 
  filter(DateTimePST > as_datetime("2019-05-15 00:00:00") &
         DateTimePST < as_datetime("2019-06-28 00:00:00"))

OS2_1_filtered <- OS2_1 %>% 
  mutate(DateTimePST = as_datetime(DateTimePST)) %>% 
  filter(DateTimePST > as_datetime("2019-05-15 00:00:00") &
         DateTimePST < as_datetime("2019-06-28 00:00:00"))

OS2_2_filtered <- OS2_2 %>% 
  mutate(DateTimePST = as_datetime(DateTimePST)) %>% 
  filter(DateTimePST > as_datetime("2019-06-28 00:00:00") &
         DateTimePST < as_datetime("2019-08-27 00:00:00"))

OS2_3_filtered <- OS2_3 %>% 
  mutate(DateTimePST = as_datetime(DateTimePST)) %>% 
  filter(DateTimePST > as_datetime("2019-08-27 09:50:00") &
         DateTimePST < as_datetime("2019-11-07 10:37:00"))

OS2_4_filtered <- OS2_4 %>% 
  mutate(DateTimePST = as_datetime(DateTimePST)) %>% 
  filter(DateTimePST > as_datetime("2019-11-07 10:37:00") &
         DateTimePST < as_datetime("2019-12-12 00:00:00"))

sft_19 <- bind_rows(OS1_filtered, OS2_1_filtered, OS2_2_filtered,
                    OS2_3_filtered, OS2_4_filtered)

sft_19 <- sft_19 %>% 
  rename(sft = Temp_C, date_hour = DateTimePST) %>% 
  select(!c(1,3,4,6:11)) %>% 
  group_by(date_hour) %>% 
  summarize(sft = mean(sft, na.rm = TRUE))

#filtered temp data for 2020 and 2021:
filtered_temp_20_21_raw <- read.csv("data_raw/CSULB/All_2020-2021_filtered_temp_data.csv")
#View(filtered_temp_20_21_raw)

station_filtered_20_21 <- filtered_temp_20_21_raw %>% 
  filter(Station == "JWS_SantaClaus_OS1" | Station == "JWS_SantaClaus_OS2" | 
         Station == "JWS_Padaro_Inshore_A" | Station == "JWS_Padaro_Inshore_B" |
         Station == "JWS_Padaro_Inshore_C" | Station == "JWS_Padaro_Inshore_D") %>% 
  mutate(depth_level = if_else(LoggerDepth_m > 1, "sft", "sst")) %>% 
  select(!c(3:5)) %>% 
  mutate(DateTimeUTC = as_datetime(DateTimeUTC))

#View(station_filtered_20_21)

sst_20_21 <- station_filtered_20_21 %>% 
  filter(depth_level == "sst") %>% 
  group_by(date(DateTimeUTC), hour(DateTimeUTC)) %>% 
  summarize(sst = mean(Temp_C, na.rm = TRUE)) %>%
  ungroup() %>%
  rename(date = 1, hour = 2) %>% 
  mutate(date = as_date(date)) %>% 
  mutate(date_hour = make_datetime(year(date), month(date),
                                   day(date), hour)) %>% 
  select(!c(1,2)) %>% 
  mutate(date_hour = force_tz(date_hour, tzone = "America/Los_Angeles"))

sft_20_21 <- station_filtered_20_21 %>% 
  filter(depth_level == "sft") %>% 
  group_by(date(DateTimeUTC), hour(DateTimeUTC)) %>% 
  summarize(sft = mean(Temp_C, na.rm = TRUE)) %>%
  ungroup() %>%
  rename(date = 1, hour = 2) %>% 
  mutate(date = as_date(date)) %>% 
  mutate(date_hour = make_datetime(year(date), month(date), day(date), hour)) %>% 
  select(!c(1,2)) %>% 
  mutate(date_hour = force_tz(date_hour, tzone = "America/Los_Angeles"))

sft_sst_20_21 <- left_join(sft_20_21, sst_20_21, by = "date_hour")

#combine 2019 and 2020/2021 sft
sft_19_20_21 <- bind_rows(sft_19, sft_20_21)
#View(sft_19_20_21)

ggplot(sft_sst_20_21, aes(x = date_hour, y = sft)) +
  geom_point()

ggplot(sst_20_21, aes(x = date_hour, y = sst)) +
  geom_point()

ggplot(sft_sst_20_21, aes(x = sst, y = sft)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, size = 1, color = "blue")

sft_sst_lm <- lm(sft ~ sst, data = sft_sst_20_21)
summary(sft_sst_lm)
#r-squared = 0.75, but for any given sft, sst can vary by ~4 degrees C

cor.test(sft_sst_20_21$sft, sft_sst_20_21$sst, method = "pearson")
#highly correlated (0.87)
```

### Append to rest of data
```{r}
daily_3yr_dat <- daily_3yr_dat %>% 
  mutate(date_hour = as_datetime(date_hour))

daily_3yr_dat <- left_join(daily_3yr_dat, sft_19_20_21, by = "date_hour")

daily_3yr_dat <- left_join(daily_3yr_dat, sst_20_21, by = "date_hour")
```

### Adding NOAA satelite chlorophyll data
```{r}
chl_dat_raw <- read.csv("data_raw/chl_dat.csv")
#View(chl_dat_raw)

chl_dat <- chl_dat_raw %>% 
  slice(-1) %>%
  mutate(long = as.numeric(longitude)) %>%
  mutate(lat = as.numeric(latitude)) %>%
  mutate(chl = as.numeric(chlorophyll)) %>% 
  mutate(long = round(long, digits = 4)) %>% 
  mutate(lat = round(lat, digits = 4)) %>%
  filter(lat == 34.4 | lat == 34.4125) %>% 
  filter(long == -119.5625 | long == -119.5500) %>%
  filter(!(lat == 34.4125 & long == -119.5500)) %>%
  group_by(time) %>%
  summarize(chl = mean(chl, na.rm = TRUE)) %>% 
  mutate(chl_interp = na.approx(chl)) %>% 
  mutate(mdy = as_date(time), time = NULL)
#View(chl_dat)

daily_3yr_dat <- daily_3yr_dat %>% 
  mutate(mdy = as_date(datetime))

daily_3yr_dat <- left_join(daily_3yr_dat, chl_dat, by = "mdy")

daily_3yr_dat %>% 
  filter(!is.na(chl)) %>% 
  nrow()

nrow(daily_3yr_dat)
#>100 chl values missing so I added linearly interpolated ones (see below for exploration of that)

chl_dat_raw %>% 
  slice(-1) %>%
  mutate(long = as.numeric(longitude)) %>%
  mutate(lat = as.numeric(latitude)) %>%
  mutate(chl = as.numeric(chlorophyll)) %>% 
  mutate(long = round(long, digits = 4)) %>% 
  mutate(lat = round(lat, digits = 4)) %>%
  filter(lat == 34.4 | lat == 34.4125) %>% 
  filter(long == -119.5625 | long == -119.5500) %>%
  filter(!(lat == 34.4125 & long == -119.5500)) %>%
  group_by(time) %>%
  summarize(chl = mean(chl, na.rm = TRUE)) %>% 
  mutate(chl_interp = na.approx(chl)) %>% 
  mutate(mdy = as_date(time), time = NULL) %>% 
  ggplot(aes(x = mdy)) +
    geom_point(aes(y = chl_interp), color = "red") +
    geom_point(aes(y = chl))

chl_dat_raw %>% 
  slice(-1) %>%
  mutate(long = as.numeric(longitude)) %>%
  mutate(lat = as.numeric(latitude)) %>%
  mutate(chl = as.numeric(chlorophyll)) %>% 
  mutate(long = round(long, digits = 4)) %>% 
  mutate(lat = round(lat, digits = 4)) %>%
  filter(lat == 34.4 | lat == 34.4125) %>% 
  filter(long == -119.5625 | long == -119.5500) %>%
  filter(!(lat == 34.4125 & long == -119.5500)) %>%
  group_by(time) %>%
  summarize(chl = mean(chl, na.rm = TRUE)) %>% 
  mutate(chl_interp = na.approx(chl)) %>% 
  mutate(mdy = as_date(time), time = NULL) %>% 
  ggplot(aes(x = chl_interp)) +
    geom_histogram()

chl_dat_raw %>% 
  slice(-1) %>%
  mutate(long = as.numeric(longitude)) %>%
  mutate(lat = as.numeric(latitude)) %>%
  mutate(chl = as.numeric(chlorophyll)) %>% 
  mutate(long = round(long, digits = 4)) %>% 
  mutate(lat = round(lat, digits = 4)) %>%
  filter(lat == 34.4 | lat == 34.4125) %>% 
  filter(long == -119.5625 | long == -119.5500) %>%
  filter(!(lat == 34.4125 & long == -119.5500)) %>%
  group_by(time) %>%
  summarize(chl = mean(chl, na.rm = TRUE)) %>% 
  mutate(chl_interp = na.approx(chl)) %>% 
  mutate(mdy = as_date(time), time = NULL) %>% 
  ggplot(aes(x = chl)) +
    geom_histogram()

ks.test(daily_3yr_dat$chl, daily_3yr_dat$chl_interp)
#no difference in distribution of actual vs interpolated chl

daily_3yr_dat %>% 
  ggplot() +
    geom_density(aes(chl)) +
    geom_density(aes(chl_interp), color = "red")

daily_3yr_dat %>% 
  ggplot() +
    stat_ecdf(aes(chl)) +
    stat_ecdf(aes(chl_interp), color = "red")
```

## Clean up time
```{r}
#View(daily_3yr_dat)

daily_3yr_dat <- daily_3yr_dat %>% 
  select(-c(1, 20:22, 24:28, 30, 36))

#str(daily_3yr_dat)

write.csv(daily_3yr_dat, "data/daily_3yr_dat_clean.csv")

size_dat_3yr <- size_dat_3yr %>% 
  mutate(length_raw_m = length_raw_m*res_adjustment_factor) %>% 
  mutate(length_adj_m = length_raw_m*depth.correction.factor*asl.correction.factor)

write.csv(size_dat_3yr, "data/size_dat_3yr_clean.csv")
```

## read clean csvs
```{r}
daily_3yr_dat <- read.csv("data/daily_3yr_dat_clean.csv")

size_dat_3yr <- read.csv("data/size_dat_3yr_clean.csv")
```

## Dataframe without 2019
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year != 2019)
```

=============================================================================

# Analyses for Thesis

## Summary statistics
```{r}
nrow(daily_3yr_dat)
#351 total survey days

daily_3yr_dat %>% 
  filter(year == 2019) %>% 
  slice(-c(2:(n()-1)))
#2019 started on 5/30 and ended on 12/6:
as.period(mdy("5/30/2019") %--% mdy("12/6/2019"), "days")
#2019 field season was 190 days long
190/7 #roughly 27 weeks

nrow(filter(daily_3yr_dat, year == 2019))
98/(190/7) #3.6 surveys/week

daily_3yr_dat %>% 
  filter(year == 2020) %>% 
  slice(-c(2:(n()-1)))
#2020 started on 6/24 and ended on 12/11:
as.period(mdy("6/24/2020") %--% mdy("12/11/2020"), "days")
#2020 field season was 170 days long
170/7 #roughly 24 weeks

nrow(filter(daily_3yr_dat, year == 2020))
113/(170/7) #4.7 surveys/week

daily_3yr_dat %>% 
  filter(year == 2021) %>% 
  slice(-c(2:(n()-1)))
#2021 started on 4/20 and ended on 12/18:
as.period(mdy("4/20/2021") %--% mdy("12/18/2021"), "days")
#2021 field season was 242 days long
242/7 #roughly 35 weeks

nrow(filter(daily_3yr_dat, year == 2021))
140/(242/7) #4.0 surveys/week
  
sum(daily_3yr_dat$total_unique, na.rm = TRUE)
#912 total sightings

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(surveys = n(), total_sightings = sum(total_unique, na.rm = TRUE), mean_count = mean(total_unique, na.rm = TRUE), sd_count = sd(total_unique, na.rm = TRUE))
#2019:     98 surveys, 44 total sharks, 0.449 sharks/survey, sd = 1.24
#2020:    113          271              2.42                 sd = 2.87
#2021:    140          597              4.36                 sd = 3.37

year_aov <- aov(total_unique ~ as.factor(year), data = daily_3yr_dat)
summary(year_aov)
TukeyHSD(year_aov)

ggplot(daily_3yr_dat, aes(x = total_unique)) +
  geom_histogram() +
  facet_wrap(vars(year))
#obviously not normally distributed so idk about anova for year

daily_3yr_dat %>%
  filter(year == 2019) %>% 
  summarize(mean_area = mean(transect_area_km2), mean_duration = mean(transect_duration_min))

daily_3yr_dat %>%
  filter(year != 2019) %>% 
  summarize(trans_area = mean(transect_area_km2, na.rm = TRUE), trans_duration = mean(transect_duration_min, na.rm = TRUE), man_area = mean(manual_area_km2, na.rm = TRUE), man_area_sd = sd(manual_area_km2, na.rm = TRUE), man_duration = mean(manual_duration_min, na.rm = TRUE), man_duration_sd = sd(manual_duration_min, na.rm = TRUE))

daily_3yr_dat %>% 
  filter(manual_area_km2 > 0) %>% 
  nrow()
#238 manual surveys conducted

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  nrow()
#203 days w/ at least one shark
203/351

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2019) %>% 
  nrow()
#17 detection days in 2019
17/98

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2020) %>% 
  nrow()
#70 detection days in 2020
70/113

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2021) %>% 
  nrow()
#116 detection days in 2021
116/140

chisq.test(daily_3yr_dat$year, daily_3yr_dat$detection)

daily_3yr_dat %>% 
  filter(year == 2019) %>% 
  summarize(max = max(total_unique))
# max 6

daily_3yr_dat %>% 
  filter(year == 2020) %>% 
  summarize(max = max(total_unique, na.rm = TRUE))
# max 12

daily_3yr_dat %>% 
  filter(year == 2021) %>% 
  summarize(max = max(total_unique, na.rm = TRUE))
#max 15
```

## Rudimentary power/sensitivity analysis - do weekly averages change when only half of the data is used?
```{r}
pwr_dat <- dat_2021 %>% 
  select(datetime, total_unique) %>% 
  mutate(every_other = ifelse(row_number() %% 2 == 1, total_unique, NA)) %>% 
  group_by(week(datetime)) %>% 
  summarize(mean_all = mean(total_unique, na.rm = TRUE),
            sd_all = sd(total_unique, na.rm = TRUE),
            mean_eo = mean(every_other, na.rm = TRUE),
            sd_eo = sd(every_other, na.rm = TRUE)) %>% 
  rename(week = 1)
#not sure sd is relevant here, especially for eo data where there are only two or three observations per week

mean(pwr_dat$mean_all)
#4.23 when all data used

mean(pwr_dat$mean_eo, na.rm = TRUE)
#4.31 when every other survey used

t.test(pwr_dat$mean_all, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")

wilcox.test(pwr_dat$mean_all, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")

pwr_dat %>%
  pivot_longer(c(mean_all,mean_eo), names_to = "Data", values_to = "weekly_mean") %>% 
  ggplot(aes(x = week, y = weekly_mean, fill = Data)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(rows = vars(Data)) +
    theme_clean() +
    labs(x = "Week", y = "Weekly Mean") +
    theme(strip.text.y = element_blank()) +
    scale_fill_manual(labels = c("Every other survey", "All data"),
                      values = c("darkgreen", "orange3"))
```

## Investigating different metrics - raw count (each method and "unique") and density
```{r}
dat_no_19 %>%
  group_by(year(date_hour)) %>% 
  summarize("Unique count" = mean(total_unique, na.rm = TRUE),
            #"Maximum unique count" = max(total_unique, na.rm = TRUE),
            "Manual count" = mean(manual_total, na.rm = TRUE),
            #"Maximum manual count" = max(manual_total, na.rm = TRUE),
            "Transect count" = mean(transect_total, na.rm = TRUE),
            #"Maximum transect count" = max(transect_total, na.rm = TRUE),
            "Unique density" = mean(total_unique/(manual_area_m2 + transect_area_m2), na.rm = TRUE),
            #"Maximum unique density" = max(total_unique/(manual_area_km2 + transect_area_km2), na.rm = TRUE),
            "Manual density" = mean(manual_unique/manual_area_m2, na.rm = TRUE),
            #"Maximum manual density" = max(manual_unique/manual_area_km2, na.rm = TRUE),
            "Transect density" = mean(transect_total/transect_area_m2, na.rm = TRUE),
            #"Maximum transect density" = max(transect_total/transect_area_km2, na.rm = TRUE)
  )
```
* Looks like manual count was higher than transect count in 2020, but not 2021. Suggests that more sharks were close to shore (under the inner transect) in 2021 than in 2020

```{r}
ggplot(dat_no_19, aes(x = manual_unique, y = transect_total)) +
  geom_jitter() +
  geom_smooth(method = "lm")
#outliers here are potentially a big problem for only using one method in analyses

ggplot(daily_3yr_dat, aes(x = transect_total)) +
  geom_bar(stat = "count")

ggplot(daily_3yr_dat, aes(x = manual_unique)) +
  geom_bar(stat = "count")

t_vs_m.lm <- lm(transect_total ~ manual_unique, data = dat_no_19)
summary(t_vs_m.lm)
#slope = 0.60 - one shark sighted in manual = 0.6 sharks sighted in transect
#r2 = 0.2797

ggplot(dat_no_19, aes(x = manual_unique, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_m.lm <- lm(total_unique ~ manual_unique, data = dat_no_19)
summary(u_vs_m.lm)
#slope = 1.16
#r2 = 0.6746

ggplot(dat_no_19, aes(x = transect_total, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_t.lm <- lm(total_unique ~ transect_total, data = dat_no_19)
summary(u_vs_t.lm)
#slope = 1.07
#r2 = 0.76
```
*best correlation is between TRANSECT count and total count (but these numbers are obviously not independent)

```{r}
mean(dat_no_19$transect_total, na.rm = TRUE)
mean(dat_no_19$manual_unique, na.rm = TRUE)

ggplot(dat_no_19, aes(x = transect_total-manual_unique)) +
  geom_histogram(stat = "count")
```
* negative x-axis value means more sharks seen in manual - to me this is good evidence that we should use manual count: it is simply seeing more sharks

### Comparing methods
```{r}
mean(dat_no_19$transect_total/dat_no_19$transect_area_km2, na.rm = TRUE)
mean(dat_no_19$manual_unique/dat_no_19$manual_area_km2, na.rm = TRUE)
#transect actually has higher density

method_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  mutate(manual = manual_unique/manual_area_km2) %>% 
  mutate(transect = transect_total/transect_area_km2) %>% 
  pivot_longer(c(manual, transect), names_to = "method",
                                    values_to = "density")
  
ggplot(method_dat, aes(x = density)) +
  geom_histogram(binwidth = 10) +
  facet_grid(~method)
#normal distribution obviously not going to work (for ANOVA or GLM)
```

```{r}
dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  filter(manual_unique != 0) %>% 
  nrow()
#46 days where transect saw nothing and manual saw something

dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  summarize(mean = mean(manual_unique, na.rm = TRUE))
#when no sharks are detected in transect, manual detects 0.898

dat_no_19 %>% 
  filter(manual_unique == 0) %>%
  filter(transect_total != 0) %>% 
  nrow()
#14 days where manual saw nothing and transect saw something

dat_no_19 %>% 
  filter(manual_unique == 0) %>% 
  summarize(mean = mean(transect_total, na.rm = TRUE))
#when no sharks are detected in manual, transect detects 0.539
```
*This is further support for using manual - but transect densities are going to have to be used anyways for comparing across all three years?

### Wilcoxon signed-rank test on counts
```{r}
method_comp_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  select(transect_area_km2, manual_area_km2,
         transect_total, manual_unique, date) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_area_km2, manual_area_km2), names_to = "area_method",
                                    values_to = "area") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0)

paired_dat <- method_comp_dat %>% 
  select(date, method, count) %>% 
  pivot_wider(names_from = method, values_from = count) %>%
  mutate(diff = manual - transect)
nrow(paired_dat)

ggplot(paired_dat, aes(x = diff)) +
    geom_bar()
#paired differences actually look pretty normally distributed...

shapiro.test(paired_dat$diff)
#jk, not normal
  
transect_counts <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  select(count)
nrow(transect_counts)

manual_counts <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  select(count)
nrow(manual_counts)

# explicitly testing the hypothesis that manual count is higher than transect count:
method_test <- wilcox.test(manual_counts$count, transect_counts$count, paired = TRUE, alternative = "greater")
method_test
#manual counts are higher (p<0.01)
```

### Calculating effect size (basically just % of days where manual saw more, transect saw more, or they were the same)
```{r}
# number of days with no difference:
paired_dat %>%
  filter(diff == 0) %>% 
  nrow()
# 84 days

84/nrow(paired_dat)
# 38% of days

paired_dat %>%
  filter(transect == 0) %>% 
  filter(manual == 0) %>% 
  nrow()
# 61 days where BOTH were zero

61/84
# 72.6% of "manual = transect" days were days where no sharks were seen!

#number of days where manual saw more:
paired_dat %>%
  filter(diff > 0) %>% 
  nrow()
# 84 days (weird), 38%

#number of days where transect saw more:
paired_dat %>%
  filter(diff < 0) %>% 
  nrow()
# 53 days

53/nrow(paired_dat)
# 24% of days

mean(daily_3yr_dat$manual_unique, na.rm = TRUE)
#average manual count is 2.29
sd(daily_3yr_dat$manual_unique, na.rm = TRUE)
#sd is 2.44
median(daily_3yr_dat$manual_unique, na.rm = TRUE)
#2
range(daily_3yr_dat$manual_unique, na.rm = TRUE)

daily_3yr_dat %>%
  filter(year != 2019) %>%
  summarize(mean = mean(transect_total, na.rm = TRUE), sd = sd(transect_total, na.rm = TRUE), median = median(transect_total, na.rm = TRUE), range = range(transect_total, na.rm = TRUE))
# average 2020/2021 transect count is 2.13

daily_3yr_dat %>%
  filter(year == 2019) %>% 
  summarize(mean = mean(transect_total, na.rm = TRUE), sd = sd(transect_total, na.rm = TRUE), median = median(transect_total, na.rm = TRUE), range = range(transect_total, na.rm = TRUE))
# average 2019 transect count is 0.45
```
* Manual survey count was significantly more likely to be higher than transect survey count (p < 0.01), with manual count being higher than transect count for 38% of survey days, lower than transect count for 24% of survey days, and equal to transect count for 38% of survey days. Of the 84 days where manual and transect count were equal, 72.6% (61 days) were days where no sharks were seen.

### Wilcoxon signed-rank test on density
```{r}
paired_dens_dat <- method_comp_dat %>%
  mutate(density = count/area) %>% 
  select(date, method, density) %>% 
  pivot_wider(names_from = method, values_from = density) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_dens_dat$diff)
#not normal

transect_dens <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(transect_dens)

manual_dens <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(manual_dens)

# do methods differ in density (2-sided, n = 221):
method_dens_test <- wilcox.test(manual_dens$density, transect_dens$density, paired = TRUE)
method_dens_test
# NOT more likely to see a higher density of sharks (count per unit area surveyed) in one survey method vs the other

mean((daily_3yr_dat$manual_unique/daily_3yr_dat$manual_area_km2), na.rm = TRUE)
#average manual density is 11.86
sd((daily_3yr_dat$manual_unique/daily_3yr_dat$manual_area_km2), na.rm = TRUE)
#sd is 2.44

daily_3yr_dat %>% 
  filter(year!= 2019) %>% 
  summarize(mean = mean(transect_total/transect_area_km2, na.rm = TRUE), sd = sd(transect_total/transect_area_km2, na.rm = TRUE))
```

### Comparing rates (count per unit time)
```{r}
method_rate_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_duration_min)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_duration_min)) %>%
  select(transect_total, manual_unique, date, transect_duration_min, manual_duration_min) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_duration_min, manual_duration_min), names_to = "time_method",
                                    values_to = "duration") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0) %>% 
  #mutate(year = as.factor(year(date))) %>% 
  mutate(method = as.factor(method))

paired_rate_dat <- method_rate_dat %>%
  mutate(rate = count/duration) %>% 
  select(date, method, rate) %>% 
  pivot_wider(names_from = method, values_from = rate) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_rate_dat$diff)
#not normal

transect_rate <- method_rate_dat %>% 
  filter(method == "transect") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(transect_rate)

manual_rate <- method_rate_dat %>% 
  filter(method == "manual") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(manual_rate)

# do methods differ in rate (2-sided, n = 221):
method_rate_test <- wilcox.test(manual_rate$rate, transect_rate$rate, paired = TRUE)
method_rate_test
# Not more likely for manual or transect to have a higher rate


mean((daily_3yr_dat$manual_unique/daily_3yr_dat$manual_duration_min), na.rm = TRUE)
#average manual rate is 0.21
sd((daily_3yr_dat$manual_unique/daily_3yr_dat$manual_duration_min), na.rm = TRUE)
#sd is 0.25

daily_3yr_dat %>% 
  filter(year!= 2019) %>% 
  summarize(mean = mean(transect_total/transect_duration_min, na.rm = TRUE), sd = sd(transect_total/transect_duration_min, na.rm = TRUE))
```

### Visualizing manual vs transect metrics - data prep
```{r}
manual_counts <- mutate(manual_counts, value = count*10, method = "manual", metric = "Count")
transect_counts <- mutate(transect_counts, value = count*10, method = "transect", metric = "Count")
manual_dens <- mutate(manual_dens, value = density, method = "manual", metric = "Density")
transect_dens <- mutate(transect_dens, value = density, method = "transect", metric = "Density")
manual_rate <- mutate(manual_rate, value = rate*100, method = "manual", metric = "Rate")
transect_rate <- mutate(transect_rate, value = rate*100, method = "transect", metric = "Rate")

comp_vis_dat <- bind_rows(manual_counts, transect_counts, manual_dens, transect_dens, manual_rate, transect_rate)

#View(comp_vis_dat)
```

### Manual vs Transect 3 metric comparison plot (for thesis):
```{r}
method_comp_plot_final <- ggplot(comp_vis_dat, aes(x = metric, y = value)) +
  geom_boxplot(aes(color = method)) +
  theme_cowplot() +
  scale_color_brewer(palette = "Dark2",
                     labels = c("Manual", "Transect")) +
  geom_point(aes(y = 139, shape = metric)) +
  scale_shape_manual(values = c(8, NA, NA)) + #significance asterisk
  guides(shape = "none") + 
  theme(panel.background = element_rect(fill = "white", colour = NA),
        plot.background = element_rect(fill = "white", colour = NA)) +
  labs(x = "Metric", y = "Value", color = "Method")

method_comp_plot_final
ggsave("plots/method_comp_plot_final.png", width = 6, heigh = 7)
# count is scaled up by a factor of 10 and rate is scaled up by a factor of 100. Only sig diff is count.
```

## Patterns of Abundance

### What are the average and range of transect detections?
```{r}
daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize("Total sightings" = sum(transect_total, na.rm = TRUE),
            "Survey days" = n(),
            "Average density" = 1000000*sum(transect_total/transect_area_m2, na.rm = TRUE)/n(),
            "Maximum abundance" = max(transect_total, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Average density" ~ px(80)) %>% 
    cols_align(align = "center")
```

### Are time of day and day of year different between years?
```{r}
time_btwn_yrs_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(time_btwn_yrs_aov)
TukeyHSD(time_btwn_yrs_aov)
#2019 time of day significantly earlier (by about 2.5 hrs)

day_btwn_yrs_aov <- aov(day ~ year, data = daily_3yr_narm)
summary(day_btwn_yrs_aov)
TukeyHSD(day_btwn_yrs_aov)
#day of year not significantly different between 2019 and 2020, but 2021 was significantly earlier than 2019 (24 days) and 2020 (41 days)
```

### Nice timeseries plot where each year is overlaid
```{r}
daily_3yr_dat %>% 
  filter(!is.na(total_unique)) %>% 
  ggplot(aes(x = day, y = rollmean(total_unique, 14, na.pad = TRUE, align = "right"))) +
    geom_line(aes(color = year), size = 1) +
    scale_color_brewer(palette = "Dark2") +
  labs(x = "Month", y = "Count", color = "Year") +
  scale_x_continuous(
  breaks = lubridate::yday(seq(as.Date("2019-01-01"), 
                               by = "1 month", length.out = 12)), 
  labels = month.abb) +
  scale_y_continuous(breaks = c(0,2,4,6,8,10)) +
  theme_cowplot() +
  theme(panel.background = element_rect(fill = "white", colour = NA),
      plot.background = element_rect(fill = "white", colour = NA))

ggsave("plots/abund_years_overlaid.png", width = 7, height = 4)

daily_3yr_dat %>% 
  filter(!is.na(total_unique)) %>%
  filter(ifelse(year == 2019, !is.na(transect_area_km2), !is.na(transect_area_km2+manual_area_km2))) %>%
  mutate(total_area = ifelse(year == 2019, transect_area_km2, transect_area_km2+manual_area_km2)) %>% 
  ggplot(aes(x = day, y = rollmean(total_unique/total_area, 14, na.pad = TRUE, align = "right"))) +
    geom_line(aes(color = year), size = 1) +
    scale_color_brewer(palette = "Dark2") +
  labs(x = "Month", y = "Density (sharks/km2)", color = "Year") +
  scale_x_continuous(
  breaks = lubridate::yday(seq(as.Date("2019-01-01"), 
                               by = "1 month", length.out = 12)), 
  labels = month.abb) +
  scale_y_continuous(breaks = c(0,5,10,15,20,25)) +
  theme_cowplot() +
  theme(panel.background = element_rect(fill = "white", colour = NA),
      plot.background = element_rect(fill = "white", colour = NA))

ggsave("plots/dens_years_overlaid.png", width = 7, height = 4)
```

### Comparing survey windows
```{r}
nrow(dat_2019)
first(dat_2019$date)
last(dat_2019$date)
#107 survey days in 2019 from May 30 to November 22

nrow(dat_2020)
first(dat_2020$date)
last(dat_2020$date)
#135 survey days in 2020 from May 20 to December 11

nrow(dat_2021)
first(dat_2021$date)
last(dat_2021$date)
#142 survey days in 2021 from April 20 to December 18
```
* I definitely *could* do analyses with only the overlapping window but since day is included I think it makes more sense to keep all of the data that I have

### Do manual counts vary across the field season?
```{r}
#adding day to the year model:
daily_3yr_narm_manual <- daily_3yr_dat %>% 
  filter(!is.na(manual_unique))
season_glm_man <- glmmTMB(manual_unique ~ year*day, family = nbinom1, data = daily_3yr_narm_manual, na.action = "na.fail")
summary(season_glm_man)

dredge(season_glm_man)
# day + year, day*year, and day-only are all equally weighted for manual count model
```

## Size 

### first off, a justification of ASL and Depth corrections:
```{r}
BOI_1_sub_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_submerged_JP_calculations.csv")
#View(BOI_1_sub_dat)

BOI_1_sub_dat$test_alt <- as.factor(as.character(BOI_1_sub_dat$test_alt))

BOI_1_sub_dat %>%
  pivot_longer(cols = c(5:7), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = 1.97, linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comp_submerged.png", width = 10)
#visually apparent that at 20m, correcting for both ASL and depth get us the closest when target is submerged 1.5 m

BOI_1_surface_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_surface_JP_calculations.csv")
View(BOI_1_surface_dat)

BOI_1_surface_dat$test_alt <- as.factor(as.character(BOI_1_surface_dat$test_alt))

BOI_1_surface_dat %>%
  pivot_longer(cols = c(5:8), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = c(1.97, 2.93), linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comps_surface.png", width = 10)
#visually apparent that ASL correction gets us closer to true size at 20m (no depth correction for surface target)
```

### determining accuracy and error margins from above tests:
```{r}
sub_dat_20 <- BOI_1_sub_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(sub_dat_20$size_asl_plus_depth)
mean_size <- mean(sub_dat_20$size_asl_plus_depth)
std_err <- sd(sub_dat_20$size_asl_plus_depth)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01136 m

#95% ci:
mean_size - margin_of_error #1.925 m
mean_size + margin_of_error #1.948 m

mean_size

known_size_pvc <- 1.97

sub_dat_20 %>% 
  mutate(error = size_asl_plus_depth - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average size estimate of pvc target at 1.5m depth was 0.0335 m too small 

sub_dat_20 %>% 
  mutate(error = (size_asl_plus_depth - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for submerged target is 1.702% with an sd of 1.545% - this is pretty good!

sub_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average UNADJUSTED size estimate of pvc target at 1.5m depth was 0.252 m too small!

sub_dat_20 %>% 
  mutate(error = (size_pvc - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for UNADJUSTED submerged target is 12.799% with an sd of 1.357%
# adjustments reduce mean error by over 11%!

surf_dat_20 <- BOI_1_surface_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(surf_dat_20$size_pvc_asl)
mean_size <- mean(surf_dat_20$size_pvc_asl)
std_err <- sd(surf_dat_20$size_pvc_asl)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01032 m

#95% ci:
mean_size - margin_of_error #1.989 m
mean_size + margin_of_error #2.009 m

known_size_pvc <- 1.97

surf_dat_20 %>% 
  mutate(error = size_pvc_asl - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#size estimate of pvc target at surface was 0.029 m too large on average

surf_dat_20 %>% 
  mutate(error = (size_pvc_asl - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
#mean error is 1.466%, with an sd of 1.403%

surf_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#UNADJUSTED size estimate of pvc target at surface was 0.0725 m too big on average
```
* I have SE, margin of error, confidence intervals, and average error - not sure which of these metrics is best to report

### average size of each depth category
```{r}
size_dat_3yr %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj > 0) %>% 
  summarize(mean_surface = mean(length_adj[depth..JP. == "surface"]),
            mean_shallow = mean(length_adj[depth..JP. == "shallow"]),
            mean_deep = mean(length_adj[depth..JP. == "deep"]))

#significant difference?

depth_aov_dat <- size_dat_2021 %>% 
                  filter(unique == "Y") %>%
                  filter(!is.na(length_adj)) %>% 
                  filter(length_adj > 1) %>% 
                  filter(depth..JP. == "surface" | depth..JP. == "shallow" | depth..JP. == "deep")

shapiro.test(log(depth_aov_dat$length_adj))
#def not normal but log fixes it

depth_aov <- aov(log(length_adj) ~ depth..JP., data = depth_aov_dat)

summary(depth_aov)
plot(depth_aov)

TukeyHSD(depth_aov)
```
*mean sizes not significantly different between depth categories

### histograms of each depth category
```{r}
size_dat_3yr %>% 
  filter(depth..JP. == "surface") %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>% 
  nrow()
#54 unique surface sharks...

size_dat_3yr %>% 
  filter(depth..JP. == "shallow") %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>% 
  nrow()
#...289 shallow sharks...

size_dat_3yr %>% 
  filter(depth..JP. == "deep") %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>% 
  nrow()
#180 deep sharks

size_dat_3yr %>% 
  filter(!is.na(depth..JP.)) %>%
  filter(unique == "Y") %>%
  filter(length_adj_m > 0) %>% 
  nrow()
#...out of 524 adjusted-sized sharks

54/524
# 10.3% surface

289/524
# 55.2% shallow

180/524
# 34.4% deep

size_dat_2021 %>% 
  filter(length_adj_m > 0) %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = length_adj)) +
    geom_histogram() +
    geom_vline(xintercept = 9.8, lty = 2, size = 1, color = "red") +
    geom_vline(xintercept = 8, lty = 2, size = 1, color = "blue") +
    theme_bw() +
    scale_x_continuous(breaks = c(4,6,8,10,12,14,16,18,20)) +
    labs(x = "Length (ft)", y = "Count")
ggsave("plots/2021_size_histo.png", height = 5, width = 7)

size_dat_3yr %>% 
  filter(length_adj_m > 0) %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = length_adj_m)) +
    geom_histogram() +
    geom_vline(xintercept = 3.5, lty = 2, size = 1, color = "red") +
    geom_vline(xintercept = 3, lty = 2, size = 1, color = "blue") +
    theme_bw() +
    scale_x_continuous(breaks = c(1,2,3,4,5)) +
    labs(x = "Length (meters)", y = "Count")
ggsave("plots/3yr_size_histo.png", height = 5, width = 7)

daily_3yr_dat %>%
  summarize(juveniles = sum(juvenile), adults = sum(adult))
#330 juveniles and 173 adults

size_dat_2021 %>% 
  filter(depth..JP. == "surface") %>% 
  ggplot(aes(x = length_raw_m)) +
    geom_histogram()
#this isn't going to be very helpful for comparing between depths

size_dat_2021 %>% 
  filter(length_adj_m > 0) %>%
  filter(unique == "Y") %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), position = "dodge") +
    facet_wrap(~depth..JP.)
#this is ok
```

### Do adjustments affect size distribution?
```{r}
size_dat_3yr %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj_m > 1) %>%
  mutate(year = year(as_date(date, format = '%m/%d/%Y'))) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "Measure", values_to = "Length") %>% 
  ggplot(aes(x = Length)) +
    geom_histogram(aes(fill = Measure, color = Measure), alpha = 0.6, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    scale_color_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    theme_cowplot() +
    theme(legend.position = "bottom", legend.spacing.x = unit(1, 'cm'),
          panel.background = element_rect(fill = "white", colour = NA),
          plot.background = element_rect(fill = "white", colour = NA)) +
    guides(fill = guide_legend(label.position = "bottom")) +
    facet_grid(rows = vars(year)) +
    theme_clean() +
    #theme(strip.text.y = element_blank()) +
    labs(x = "Length (m)", y = "Count", fill = "Measure:", color = "Measure:")
ggsave("plots/3yr_faceted_size_adjustment_comp.png", height = 7, width = 4.6)
 
size_dat_3yr %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj_m > 1) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "Measure", values_to = "Length") %>% 
  ggplot(aes(x = Length)) +
    geom_histogram(aes(fill = Measure, color = Measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    scale_color_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    theme_cowplot() +
    theme(legend.position = "bottom", legend.spacing.x = unit(1, 'cm'),
          panel.background = element_rect(fill = "white", colour = NA),
          plot.background = element_rect(fill = "white", colour = NA)) +
    guides(fill = guide_legend(label.position = "bottom")) +
    labs(x = "Length (m)", y = "Count", fill = "Measure:", color = "Measure:")
ggsave("plots/size_adjustment_comp.png", height = 7, width = 4.6)
#shows how peaks in raw get shifted to the right when the adjustment is made. looks cool when narrow. Probably a supplementary figure

size_dat_3yr %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj_m)) %>% 
  filter(!is.na(length_raw_m)) %>%
  filter(length_adj_m > 1) %>%
  summarize(median_adj_length = median(length_adj_m), median_raw_length = median(length_raw_m), mean_adjustment = mean(length_adj_m - length_raw_m), sd_adjustment = sd(length_adj_m - length_raw_m), max_adj = max(length_adj_m - length_raw_m), min_adj = min(length_adj_m - length_raw_m))

#looking at how adjustments affect juvie:adult ratio with the 3m cutoff
size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>%
  filter(length_adj_m > 1) %>%
  mutate(class_raw = ifelse(length_raw_m < 3, "Juvenile", "Adult")) %>% 
  mutate(class_adj = ifelse(length_adj_m < 3, "Juvenile", "Adult")) %>% 
  count(class_adj)
#113 adults and 411 juveniles with raw measurements, 183 adults and 341 juveniles with adjusted measurements - adjustments bring 70 sharks from juvenile to adult length

#looking at how adjustments affect shallow sharks (overlapping histograms):
size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth..JP. == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure, color = measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
# I actually like this dodged the most, shows how peaks in raw get shifted to the right

# same as above but faceted:
size_dat_3yr %>% 
  filter(depth..JP. == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), binwidth = 0.1) +
    facet_grid(measure~.) +
    scale_fill_brewer(palette = "Dark2")
# not sure what this really tells me. Overall smoother distribution for the adjusted lengths, which I suppose makes sense. Adjusted lengths are shifted to the right...

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>% 
  summarize(median_adj = median(length_adj_m), median_raw = median(length_raw_m))
#by approximately 0.3 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average shallow adjustment was an increase of 0.33 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "surface") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average surface adjustment was an increase of 0.13 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "deep") %>% 
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>% 
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average deep adjustment was increase of 0.47 meters
```

### Size classes
```{r}
ggplot(data = daily_3yr_dat, aes(x = juvenile/total_sized)) +
  geom_histogram()

ggplot(data = daily_3yr_dat, aes(x = date, y = juvenile/total_sized)) +
  geom_point()

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  ggplot(aes(x = year, y = juvenile/total_sized)) +
    geom_boxplot()
#not sure why 2020 looks weird 

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  filter(year == "2020") %>% 
  ggplot(aes(x = juvenile/total_sized)) +
    geom_bar()

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  ggplot(aes(x = juvenile/total_sized, fill = as_factor(year))) +
    geom_histogram(position = "dodge", bins = 10)
#2021 has a LOT of days where there were only adult-sized sharks, and 2020 has tons of days where only juvenile-sized sharks were seen - I want a better way to visualize this for Ch. 2

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  ggplot(aes(x = juvenile/total_sized, color = as_factor(year))) +
    geom_density(size = 1.1, adjust = 0.5)
ggsave("plots/yearly_juv_prop_density.png")
```

## Modelling effects of environmental and detection-related variables on transect density

### Build the dataset to be used in the GAM
```{r}
gam_dat <- daily_3yr_dat %>% 
  filter(!is.na(transect_total)) %>%
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month_fac = as.factor(month)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(Salinity < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>%
  filter(!is.na(scripps_temp)) %>%
  filter(!is.na(scripps_temp_1wk)) %>%
  filter(!is.na(scripps_temp_2wk)) %>%
  filter(!is.na(scripps_temp_3wk)) %>%
  filter(!is.na(scripps_temp_4wk)) %>%
  filter(!is.na(cabrillo_temp)) %>% 
  filter(!is.na(cabrillo_temp_1wk)) %>%
  filter(!is.na(cabrillo_temp_2wk)) %>%
  filter(!is.na(cabrillo_temp_3wk)) %>%
  filter(!is.na(cabrillo_temp_4wk)) %>%
  filter(!is.na(tide)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0))

#view(gam_dat)
#idk how to get rid of NAs and "NA"s

gam_dat2 <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 100) %>%
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(month = month(mdy))  %>%
  filter(!is.na(transect_total)) %>%
  filter(!is.na(year)) %>% 
  filter(!is.na(hour)) %>% 
  filter(!is.na(month)) %>%                                 
  filter(!is.na(beaufort)) %>%                                 
  filter(!is.na(vis)) %>%                                
  filter(!is.na(tide)) %>%                                
  filter(!is.na(Temp_top)) %>%   
  filter(!is.na(WVHT)) %>%
  filter(!is.na(transect_area_km2))

gam_dat_lb <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(month = month(mdy)) %>%
  filter(!is.na(transect_total)) %>%
  filter(!is.na(year)) %>% 
  filter(!is.na(hour)) %>% 
  filter(!is.na(month)) %>%                                 
  filter(!is.na(beaufort)) %>%                                 
  filter(!is.na(vis)) %>%                                
  filter(!is.na(tide)) %>%                                
  filter(!is.na(sft)) %>%   
  filter(!is.na(WVHT)) %>%
  filter(!is.na(salinity)) %>%
  filter(!is.na(chl)) %>%
  filter(!is.na(transect_area_km2))
View(gam_dat_lb)

gam_dat_lb_full <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(month = month(mdy)) 
  

gam_dat3 <- daily_3yr_dat %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month_fac = as.factor(month)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>%
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  filter(!is.na(cabrillo_temp_3wk)) %>%
  filter(!is.na(scripps_temp_3wk)) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(vis)) %>% 
  filter(!is.na(beaufort)) %>% 
  filter(!is.na(tide)) %>% 
  filter(!is.na(transect_total)) %>%
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) 
#this is to be used when dredging, as dredge() requires no na's in the dataset. Use dat2 for modelling, as n will increase from 209 to 213
```

### Correlations between continuous predictors (Rohner et al. 2013)
```{r}
cor.test(gam_dat2$scripps_temp, gam_dat2$Temp_top, method = "pearson")
#Scripps and carp temp are correlated at 0.49 (moderate correlation)

cor.test(gam_dat2$cabrillo_temp, gam_dat2$Temp_top, method = "pearson")
#Cabrillo and Carp temp are correlated at 0.19

cor.test(gam_dat2$scripps_temp_3wk, gam_dat2$Temp_top, method = "pearson")
# 3 week lagged Scripps and carp temp are correlated at 0.36

cor.test(gam_dat2$cabrillo_temp_3wk, gam_dat2$Temp_top, method = "pearson")
# 3 week lagged Cabrillo and Carp temp are correlated at 0.27

gam_dat2 %>% 
  select(c(18,22,28,52,55,78,83,85)) %>% 
  cor(use = "complete.obs")
#tide is correlated with month (0.45), LTER temp (0.29), channel temp (0.28), and hour (0.26)
#hour is correlated with tide, channel temp, month, and LTER temp
#Carp LTER temp is highly correlated with channel buoy temp (0.70), also with other temp locations

ggplot(gam_dat2, aes(x = month, y = tide)) +
  geom_point() +
  geom_smooth()

ggplot(gam_dat2, aes(x = tide, y = Temp_top)) +
  geom_point() +
  geom_smooth(method = "lm")
#temp increases with tide

ggplot(gam_dat2, aes(x = hour, y = tide)) +
  geom_point() +
  geom_smooth(method = "lm")
#higher tides later in the day?
```

### Different temperature sources (LTER vs CSULB)
```{r}
#CSULB sst:
cor.test(gam_dat2$sst, gam_dat2$Temp_top, method = "pearson")
#0.80

ggplot(gam_dat2, aes(x = sst, y = Temp_top)) +
  geom_point()
#lots of missing data

daily_3yr_dat %>% 
  filter(!is.na(sst)) %>% 
  nrow()
#only 135 days with CSULB sst

#CSULB sft:
cor.test(gam_dat2$sft, gam_dat2$Temp_top, method = "pearson")
#0.86

ggplot(gam_dat2, aes(x = sft, y = Temp_top)) +
  geom_point()

daily_3yr_dat %>% 
  filter(!is.na(sft)) %>% 
  nrow()
#316 days with CSULB sft

daily_3yr_dat %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(Temp_top < 100) %>% 
  nrow()
#compared to 322 with Carp LTER sft

#Visualizing data coverage:
daily_3yr_dat %>% 
  filter(Temp_top < 100) %>% 
  ggplot(aes(x = date_hour)) +
    geom_point(aes(y = sft), color = "red") +
    geom_point(aes(y = sst), color = "green") +
    geom_point(aes(y = Temp_top), color = "blue")
```

### Temp and season correlation
```{r}
month_temp_aov <- aov(Temp_top~as_factor(month), gam_dat2)
summary(month_temp_aov)

TukeyHSD(month_temp_aov)

ggplot(aes(x = as_factor(month), y = Temp_top), data = gam_dat2) +
  geom_boxplot()

day_temp_lm <- lm(Temp_top~day, gam_dat2)
summary(day_temp_lm)

ggplot(aes(x = day, y = Temp_top), data = gam_dat2) +
  geom_point() +
  geom_smooth()
```

### Concurvity
```{r}
mgcv::concurvity(glob_gam,full=TRUE)
#all terms have high concurvity w/ the "rest of the model"

mgcv::concurvity(glob_gam,full=FALSE)
#Cabrillo x Carp = 0.234
#Scripps x Carp = 0.297
#Cabrillo x Scripps = 0.229
#Tide x WVHT is very high (0.9), WVHT is also high with all three temp variables (>0.4)
#Many environmental variables concurved (?) with the temporal ones but that is to be expected

mgcv::concurvity(lbgam_tot_full, full = FALSE)
#sft and hour 2019 is only combo with worst > 0.8
```

### Trends with detectability variables
```{r}
ggplot(gam_dat, aes(x = as.numeric(beaufort), y = transect_total)) + 
  geom_point() +
  geom_smooth()

gam_dat %>% 
  group_by(beaufort) %>% 
  summarize(presence = mean(presence, na.rm = TRUE)) %>% 
  ggplot(aes(x = beaufort, y = presence)) +
    geom_bar(stat = "identity")
#unexpected INCREASE in presence as beaufort increases

ggplot(gam_dat, aes(x = as.numeric(vis), y = transect_total)) +
  geom_point() +
  geom_smooth()
```

### Trends with LTER variables
```{r}
ggplot(gam_dat2, aes(x = Temp_top, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = Temp_top, y = total_unique)) + 
  geom_jitter() +
  geom_smooth(method = "lm")

ggplot(gam_dat, aes(x = Salinity, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
#very tiny variation in salinity, this won't be useful

ggplot(gam_dat, aes(x = Fluoroescence, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
#I guess there are no valid fluorescence (chlorophyll) measurements
```

### Trends with tide and swell
```{r}
ggplot(gam_dat, aes(x = tide, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = WVHT, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
```
*not much going on with either of these

### transect total GAM, no interactions
```{r}
gam1 <- mgcv::gam(transect_total ~ year + 
                                   s(hour, bs = "cc", k = 10) +
                                   s(day, bs = "cc", k = 20) +
                                   beaufort + vis + 
                                   s(tide, bs = "cr", k = 10) +
                                   s(Temp_top, bs = "cr", k = 10) +
                                   s(WVHT, bs = "cr", k = 30) +
                                   offset(transect_area_km2),
                                   data = gam_dat,
                                   method = "REML",
                                   family = nb,
                                   na.action = "na.fail")
#REML used for small dataset (observations in the 100's), to prevent over-smoothing
#knots from gam2 below

summary(gam1)
# 37% of deviance explained

gam.check(gam1)
#negative binomial fixes qq plot (nb was also best for temporal GLM)

plot.gam(gam1)
#not sure why day is flat

visreg(gam1)

anova.gam(gam1)
#this is analogous to "drop1" function for GLM
#year, beaufort, hour, and wave height are significant predictors (matches dredge below)

dredge(gam1, m.min = 8)
#this takes a long time to run, especially if no limits are set on minimum number of terms
#global model has 9 terms, so m.min = 8 will return every model with just one term missing. Models with beaufort, hour, year, and wave height are all worse than global model, supporting the anova.gam result above and the full dredge result
#best model is beaufort, hour, waveheight, and year, but top 12 models (6 pairs with/without day) are all within delta of 2
#models with or without day have the same AIC, so something weird is happening
```

### transect total GAM, with year interactions
```{r}
gam2 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 10, by = year) +
                                   s(day, bs = "cc", k = 20, by = year) + 
                                   beaufort + vis + s(tide, by = year, k = 10) +
                                   s(Temp_top, by = year, k = 10) +
                                   s(WVHT, by = year, k = 30) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")
#knots hand-selected to be as large as possible without throwing 
#"more coefficients than data" error

summary(gam2)
#adding year interactions increases deviance explained from 37 to 64.3 percent
gam.check(gam2)
#issues with WVHT (k = 30 is too low)

#plot.gam(gam2)
visreg(gam2)

anova.gam(gam2)
#year and beaufort significant as parametric terms
#hour significant in all years but 2019
#day significant in all years
#tide n.s.
#temp n.s.
#WVHT n.s. (only difference from gam1)

dredge(gam2, m.min = 8)
# dropping hour, day, or year from global model make fit worse

dredge(gam2)

#wald_gam(gam2)
#report_stats(gam2)
#plot_diff2(gam2, comp = list(year = c('2019', '2020')), view = c('hour', 'tide'))
```

### interaction model but with month rather than day of year:
```{r}
gam3 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 10, by = year) +
                                   s(month, bs = "cc", k = 8, by = year) + 
                                   beaufort + vis + s(tide, by = year, k = 10) +
                                   s(Temp_top, by = year, k = 10) +
                                   s(WVHT, by = year, k = 40) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")

summary(gam3)
#month model explains 60.4% of deviance compared to 64% for day model

gam.check(gam3)
#month by year doesn't work
visreg(gam3)

anova.gam(gam3)
#year, beaufort, hour, and month significant
```

### "Local" month model with year interaction only for hour and month (no N and S temps) - CARP LTER SFT DATA
```{r}
gam4 <- mgcv::gam(transect_total ~ year +
                                   s(hour, by = year) +
                                   s(month, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide) +
                                   s(Temp_top) +
                                   s(WVHT) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML",
                  family = nb, select = TRUE)
#select = TRUE does automated term penalization to remove terms that are "infinitely smooth"

summary(gam4)
#55.6% of deviance explained

gam.check(gam4, rep = 500)
#month and wvht p < 0.05, but edf much lower than k'

anova.gam(gam4)
#p-values for smooth terms show that the nonlinear effect of the term is different from zero (a flat horizontal line)
#year, beaufort, vis all (marginally) significant parametric terms (see post-hoc tests below)
#month (all years) and hour (2020 and 2021) significant.
#tide, temp, and WVHT n.s.

visreg(gam4)
plot(gam4)

gam4_dredge_drop1 <- dredge(gam4, m.min = 8, fixed = "offset(transect_area_km2)")
gam4_dredge_drop1
#dropping tide, vis, waveheight, and temp all (marginally) improves fit over global model
#dropping beaufort, hour, month, and year all significantly worsen fit compared to global model

#gam4_full_dredge <- dredge(gam4, m.min = 3, fixed = "offset(transect_area_km2)")
gam4_full_dredge

#gam4_top3_dredge <- dredge(gam4, m.max = 4, fixed = "offset(transect_area_km2)")
gam4_top3_dredge

visreg(gam4)
visreg4_data <- visreg(gam4)

fits4 <- as.data.frame(visreg4_data[[3]][["fit"]][["visregFit"]])
days4 <- as.data.frame(visreg4_data[[3]][["fit"]][["month"]])

gam4_day_fits <- bind_cols(fits4, days4)
#main peak is July, secondary peak in May, and minimum is September/October

gam4_year_emms <- emmeans(gam4, "year")
pwpm(gam4_year_emms, type = "response")
#2019 lower than 2020 and 2021, but 2020 and 2021 not sig. diff:

ggplot(data = gam_dat, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

gam4_beaufort_emms <- emmeans(gam4, "beaufort")
pwpm(gam4_beaufort_emms, type = "response")
# no sig. contrasts

gam4_vis_emms <- emmeans(gam4, "vis")
pwpm(gam4_vis_emms, type = "response")
#2 marginally lower than 4 (p = 0.080)
```

### "Local" month model with year interaction only for hour and month (no N and S temps) - CSULB SFT DATA
```{r}
gam4lb <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   s(chl) +
                                   offset(transect_area_km2),
                  data = gam_dat_lb_full, method = "REML", family = nb, select = TRUE)
#select = TRUE does automated term penalization to remove terms that are "infinitely smooth"
#REML for (relatively) small dataset

summary(gam4lb)
#59.6% of deviance explained
#n = 247

gam.check(gam4lb, rep = 500)
#issue with month. wvht is p < 0.05 but edf <<< k

anova.gam(gam4lb)
#p-values for smooth terms show that the nonlinear effect of the term is different from zero (a flat horizontal line)
#year, beaufort, and vis are all significant as parametric terms (see post-hoc tests below)
#sft, month (all years) and hour (2020 and 2021) significant.
#tide, WVHT, salinity n.s.

visreg(gam4lb)

plot(gam4lb)

gam4lb_dredge_drop1 <- dredge(gam4lb, m.min = 9, fixed = "offset(transect_area_km2)")
gam4lb_dredge_drop1
# models w/out salinity, tide, and waveheight are as good as global model

#gam4lb_full_dredge <- dredge(gam4lb, m.min = 4, fixed = "offset(transect_area_km2)")
gam4lb_full_dredge

#gam4lb_top3_dredge <- dredge(gam4lb, m.max = 4, fixed = "offset(transect_area_km2)")
gam4lb_top3_dredge

visreg(gam4lb)
visreg4lb_data <- visreg(gam4lb)

fits4lb <- as.data.frame(visreg4lb_data[[3]][["fit"]][["visregFit"]])
days4lb <- as.data.frame(visreg4lb_data[[3]][["fit"]][["month"]])

gam4lb_day_fits <- bind_cols(fits4lb, days4lb)
View(gam4lb_day_fits)
#main peak is July, secondary peaks (local maxima) in May and November, and minimum is September/October

#post-hoc tests of significant parametric terms yar, beaufort, and vis:

gam4lb_year_emms <- emmeans(gam4lb, "year")
pwpm(gam4lb_year_emms, type = "response")
#2019 lower than 2020 and 2021, but 2020 and 2021 not sig. diff:

ggplot(data = gam_dat_lb, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

gam4lb_beaufort_emms <- emmeans(gam4lb, "beaufort")
pwpm(gam4lb_beaufort_emms, type = "response")
# no sig. contrasts

gam4lb_vis_emms <- emmeans(gam4lb, "vis")
pwpm(gam4lb_vis_emms, type = "response")
#2 marginally lower than 3 (p = 0.0555), lower than 4 (p < 0.01), marginally lower than 5 (p < 0.0661)
```

### Comparing gam4 models (LTER vs CSULB)
```{r}
AICc(gam4, gam4lb)
#lb data significantly lower AICc (delta = 52 where a delta of 2 is considered threshold)
```

### Total count w/CSULB data
```{r}
gam_dat_lb_tot <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(month = month(mdy)) %>%
  filter(!is.na(total_unique)) %>%
  filter(!is.na(year)) %>% 
  filter(!is.na(hour)) %>% 
  filter(!is.na(month)) %>%                                 
  filter(!is.na(beaufort)) %>%                                 
  filter(!is.na(vis)) %>%                                
  filter(!is.na(tide)) %>%                                
  filter(!is.na(sft)) %>%   
  filter(!is.na(WVHT)) %>%
  filter(!is.na(salinity)) %>%
  filter(!is.na(chl)) %>% 
  filter(!is.na(total_area))

lbgam_tot <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(month, k = 8, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot, 
                       method = "REML", family = nb, select = TRUE,
                       na.action = "na.fail")
```

### Working version as of 7/6: day instead of month, CSULB or LTER sft data, total count
```{r}
gam_dat_lb_tot_full <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(month = month(mdy)) %>%
  mutate(yday = yday(mdy)) %>% 
  mutate(month_fac = as.factor(month))
View(gam_dat_lb_tot_full)

lbgam_tot_full <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 40) +
                                      s(sft, k = 40) +
                                      s(WVHT, k = 30) +
                                      s(salinity, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)

summary(lbgam_tot_full)
#DE = 57.0%, n = 299
visreg(lbgam_tot_full)

anova(lbgam_tot_full)
#year and vis significant parametrics
#hour sig in 2020 and 2021, not 2019
#day sig. in all years
#tide and sft significant
#wvht and salinity n.s.

gam.check(lbgam_tot_full)
#problem w/ wvht - could consider dropping

mgcv::concurvity(lbgam_tot_full, full = TRUE)
#basically everything is highly concurved w/ the "rest of the model" (indicating some variables may need to be removed)

mgcv::concurvity(lbgam_tot_full, full = FALSE)
#basically all of the envt'l variables are highly concurved w/ the temporal variables (day and hour)
#what does it mean that day and hour are concurved within each year? 
```

### Adding interpolated chl to the above
```{r}
lbgam_tot_chl <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 35, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 35) +
                                      s(sft, k = 35) +
                                      s(WVHT, k = 25) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 30) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)

summary(lbgam_tot_chl)
#DE = 57, n = 285
#DE of non-chl model = 57

visreg(lbgam_tot_chl)
gam.check(lbgam_tot_chl)
#again, waveheight is only issue - could drop

anova(lbgam_tot_chl)
#year and vis significant
#hour only significant in 2021
#day significant in all three years
#tide and sft significant
#chl and waveheight penalized to zero
#salinity not significant

#adding chl removed effect of sft...
lbgam_tot_chl_no_sft <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 25, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(WVHT, k = 25) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 30) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_chl_no_sft)
anova(lbgam_tot_chl_no_sft)

mgcv::concurvity(lbgam_tot_chl, full = F)
mgcv::concurvity(lbgam_tot_chl_no_sft, full = T)
mgcv::concurvity(lbgam_tot_full, full = T)
#no-sft model has lower concurvity of envt'l terms w/rest of model

mgcv::concurvity(lbgam_tot_chl_no_sft, full = F)
```

### Comparing a model w/ sft as only envt'l variable to one with all other envt's variables but no sft
```{r}
lbgam_sft_only <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 25, by = year) +
                                      s(sft, k = 50) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
gam.check(lbgam_sft_only)
summary(lbgam_sft_only)
#50.7 DE! n = 313

anova(lbgam_sft_only)

visreg(lbgam_sft_only)

summary(lbgam_tot_chl_no_sft)
#59.1% DE n = 237

summary(lbgam_tot_chl)
#58.4% n = 235

summary(lbgam_tot_full)
#DE = 62.2%, n = 249

AICc(lbgam_sft_only, lbgam_tot_chl_no_sft, lbgam_tot_chl, lbgam_tot_full)
#AIC is much higher, along with the model w/out chl

ggplot(daily_3yr_dat, aes(x = sft, y = chl_interp)) +
  geom_point() +
  geom_smooth()

lbgam_sft_chl_only <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 25, by = year) +
                                      beaufort +
                                      vis +
                                      s(sft, k = 50) +
                                      s(chl_interp, k = 5) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_sft_chl_only)
#chl marginal and sft penalized to zero
```

### Dropping terms to test sensitivity to concurvity - no chl version
```{r}
lbgam_tot_no_wvht <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(sft, k = 50) +
                                      s(salinity, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_no_wvht)
#same terms sig.
visreg(lbgam_tot_no_wvht)

mgcv::concurvity(lbgam_tot_no_wvht, full = TRUE)

lbgam_tot_no_wvht_or_sal <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(sft, k = 50) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_no_wvht_or_sal)
#same terms sig.
mgcv::concurvity(lbgam_tot_no_wvht_or_sal, full = TRUE)

lbgam_tot_no_sft <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(WVHT, k = 15) +
                                      s(salinity, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_no_sft)
visreg(lbgam_tot_no_sft)
#no change

lbgam_tot_no_sft_or_hour <- mgcv::gam(total_unique ~ year +
                                      s(yday, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(WVHT, k = 15) +
                                      s(salinity, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_no_sft_or_hour)
#tide became marginal
visreg(lbgam_tot_no_sft_or_hour)
#no change

lbgam_tot_no_day <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 50) +
                                      s(sft, k = 50) +
                                      s(WVHT, k = 15) +
                                      s(salinity, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_no_day)
#sft becomes totally insignificant and is penalized to zero when day is removed!
visreg(lbgam_tot_no_day)

# Concurvity test conclusion: mgcv deals with concurvity well. the only variable that, when removed, changed the significance of another variable was day of year.
```

### LTER data instead of CSULB data
```{r}
ltergam_tot_full <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(day, k = 30, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(Temp_top, k = 10) +
                                      s(WVHT, k = 10) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)

summary(ltergam_tot_full)
#DE = 99.6, n = 251 - should definitely use CSULB data!
anova(ltergam_tot_full)
#year, beaufort, vis significant parametrics
#day sig. in 2020 and marginal in 2021
#tide and sft sig.
gam.check(ltergam_tot_full)
#marginal issues with hour (month had worse issues)
```

###comparing models without hour and without sft (only concurved variables)
```{r}
lbgam_tot_full_no_sft <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 12, by = year) +
                                      month_fac +
                                      beaufort +
                                      vis +
                                      s(WVHT, k = 10) +
                                      s(tide, k = 10) +
                                      s(chl) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lbgam_tot_full_no_sft)
anova(lbgam_tot_full)
visreg(lbgam_tot_full)

lbgam_tot_full_no_hr <- mgcv::gam(total_unique ~ year +
                                      beaufort +
                                      vis +
                                      s(sft, k = 50) +
                                      s(WVHT, k = 10) +
                                      s(tide, k = 10) +
                                      s(chl) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)

summary(lbgam_tot_full_no_sft)
#wvht ns, vis 0.06, DE = 99.7
anova(lbgam_tot_full_no_sft)

summary(lbgam_tot_full_no_hr)
#sft, wvht, tide ns, vis 0.08, beau = 0.09, month_fac = 0.055, DE = 99.5
anova(lbgam_tot_full_no_hr)

visreg(lbgam_tot_full)

gam.check(lbgam_tot_full)
```

### comparing sst and sft using 2021
```{r}
gam_dat_2021 <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  filter(year == 2021) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(yday = yday(mdy))

sst_2021_gam <- mgcv::gam(total_unique ~
                                      s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sst, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(total_area),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(sst_2021_gam)
#47.6% DE, n = 120
visreg(sst_2021_gam)

sft_2021_gam <- mgcv::gam(total_unique ~
                                      s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(chl_interp) +
                                      offset(total_area),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(sft_2021_gam)
#45.1% DE, n = 120

anova(sst_2021_gam)
#vis, hour, day, tide, SST significant
#beaufort, chl, and wvht n.s.

anova(sft_2021_gam)
#vis, hour, day, tide significant
#SFT NOT SIGNIFICANT
#beaufort, wvht, chl also n.s.

### Same as above but just using temperature and temporal variables:

sst_only_2021_gam <- mgcv::gam(total_unique ~
                                      s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      s(sst, k = 10) +
                                      offset(total_area),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(sst_only_2021_gam)
#30.1% DE, n = 123

sft_only_2021_gam <- mgcv::gam(total_unique ~
                                      s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      s(sft, k = 10) +
                                      offset(total_area),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(sft_only_2021_gam)
#21.9% DE, n = 123

anova(sst_only_2021_gam)
visreg(sst_only_2021_gam)
#SST is significant and more or less linear (higher sst = higher density)

anova(sft_only_2021_gam)
visreg(sft_only_2021_gam)
#SFT is NOT significant, penalized to 0.2 edf

AICc(sst_2021_gam, sft_2021_gam, sst_only_2021_gam, sft_only_2021_gam)
```

### 2019 and 2020 single-year models and compare to 2021
```{r}
gam_dat_2019 <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  filter(year == 2019) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(yday = yday(mdy))

lb_gam_2019 <- mgcv::gam(total_unique ~ s(hour, k = 10) +
                         s(yday, k = 10) +
                         beaufort +
                         vis +
                         s(tide, k = 10) +
                         s(sft, k = 10) +
                         s(WVHT, k = 10) +
                         s(chl_interp) +
                         offset(total_area),
                       data = gam_dat_2019, 
                       method = "REML", family = nb, select = TRUE)

gam_dat_2020 <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  filter(year == 2020) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  mutate(yday = yday(mdy))

lb_gam_2020 <- mgcv::gam(total_unique ~ s(hour, k = 10) +
                         s(yday, k = 10) +
                         beaufort +
                         vis +
                         s(tide, k = 10) +
                         s(sft, k = 10) +
                         s(WVHT, k = 10) +
                         s(chl_interp) +
                         offset(total_area),
                       data = gam_dat_2020, 
                       method = "REML", family = nb, select = TRUE)

summary(lb_gam_2019)
#tide, sft, waveheight penalized to zero
#DE = 90.4(!), n = 77

summary(lb_gam_2020)
#hour penalized to zero
#DE = 48.3, n = 95

summary(sft_2021_gam)
#wave height penalized to zero
#DE = 45.1, n = 120

anova(lb_gam_2019)
#HOUR, DAY, and ChlA are significant in 2019

anova(lb_gam_2020)
#DAY, TIDE, TEMP, and WAVE HEIGHT are significant in 2020

anova(sft_2021_gam)
#VIS, HOUR, DAY and TIDE are significant in 2021
```

### Juvenile and Adult density models with CSULB sft data (all years)
```{r}
lb_juv_gam <- mgcv::gam(juvenile ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 35, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 30) +
                                      s(sft, k = 30) +
                                      s(WVHT, k = 30) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 30) +
                                      offset(area_20m),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lb_juv_gam)
#DE = 29.8%, n = 280

lb_adult_gam <- mgcv::gam(adult ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 35, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 30) +
                                      s(sft, k = 30) +
                                      s(WVHT, k = 30) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 30) +
                                      offset(area_20m),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(lb_adult_gam)
#32.7% DE, n = 280

anova(lb_juv_gam)
#year and vis significant
#hour in 2021 significant
#day in 2019 and 2020 significant, marginal in 2021
#tide marginal
#sft significant!
#wvht, salinity, and chl all penalized to zero

anova(lb_adult_gam)
#year significant, vis and sea state not
#hour significant in 2019 only
#day significant in 2020 and 2021
#no envt'l or detection variables significant
```

### Juvenile and Adult density, 2019
```{r}
juv_gam_2019 <- mgcv::gam(juvenile ~  s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2019, 
                       method = "REML", family = nb, select = TRUE)

adult_gam_2019 <- mgcv::gam(adult ~   s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2019, 
                       method = "REML", family = nb, select = TRUE)

summary(juv_gam_2019)
#DE = 81.7%, n = 77
#tide, sft, waveheight, and salinity penalized to zero


summary(adult_gam_2019)
#DE = 78.6, n = 77
#day, tide, sft, and waveheight penalized to zero

anova(juv_gam_2019)
#DAY and CHL significant in 2019 for juveniles

anova(adult_gam_2019)
#HOUR and CHL significant in 2019 for adults
```

### Juvenile and Adult density, 2020
```{r}
juv_gam_2020 <- mgcv::gam(juvenile ~  s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2020, 
                       method = "REML", family = nb, select = TRUE)

adult_gam_2020 <- mgcv::gam(adult ~   s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2020, 
                       method = "REML", family = nb, select = TRUE)

summary(juv_gam_2020)
#DE = 27%, n = 95
#hour and salinity penalized to zero

summary(adult_gam_2020)
#DE = 27.9%
#hour, day, tide, sft, and salinity penalized to zero

anova(juv_gam_2020)
#DAY and TEMP significant for juveniles in 2020

anova(adult_gam_2020)
#Wave height significant for adults in 2020

```

### Juvenile and Adult density, 2021
```{r}
juv_gam_2021 <- mgcv::gam(juvenile ~  s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)

adult_gam_2021 <- mgcv::gam(adult ~   s(hour, k = 10) +
                                      s(yday, k = 10) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sft, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)

summary(juv_gam_2021)
#DE = 13.2%, n = 108
#tide, temp, salinity, and chl penalized to zero

summary(adult_gam_2021)
#DE = 21.7%

anova(juv_gam_2021)
#HOUR significant for juveniles in 2021

anova(adult_gam_2021)
#tide and sft significant for adults in 2021

```


### Juvenile and Adult density, 2021
```{r}


```

### Juvenile and adult density with 2021 SST
```{r}
lb_juv_gam_sst <- mgcv::gam(juvenile ~
                                      s(hour, k = 10) +
                                      s(yday, k = 20) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 20) +
                                      s(sst, k = 25) +
                                      s(WVHT, k = 10) +
                                      s(salinity, k = 10) +
                                      s(chl_interp, k = 10) +
                                      offset(area_20m),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(lb_juv_gam_sst)
#DE = 13.2%, n = 108
visreg(lb_juv_gam_sst)

lb_adult_gam_sst <- mgcv::gam(adult ~
                                 s(hour, k = 10) +
                                 s(yday, k = 20) +
                                 vis +
                                 s(tide, k = 20) +
                                 s(sst, k = 25) +
                                 s(WVHT, k = 10) +
                                 s(salinity, k = 10) +
                                 s(chl_interp, k = 10) +
                                 offset(area_20m),
                       data = gam_dat_2021, 
                       method = "REML", family = nb, select = TRUE)
summary(lb_adult_gam_sst)
#DE = 26.1%, n = 108
visreg(lb_adult_gam_sst)
#hmm

gam.check(lb_adult_gam_sst)

ggplot(gam_dat_2021, aes(x = beaufort)) +
  geom_histogram(stat = "count")

ggplot(daily_3yr_dat, aes(x = as.numeric(beaufort), y = as.numeric(vis))) +
  geom_jitter()

anova(lb_juv_gam_sst)
#hour only significant variable, day marginal

anova(lb_adult_gam_sst)
#tide and sst significant
```

### comparing juvenile/adult/all models with sft, and with 2021 sst
```{r}
AICc(lbgam_tot_chl, sst_2021_gam, lb_juv_gam, lb_adult_gam, lb_juv_gam_sst, lb_adult_gam_sst, sft_2021_gam)
#this isn't valid due to different subsets of data being used (different response variables and only using 2021 for the sst)

```

### sst with all years (DELETE)
```{r}
sst_gam_tot_full <- mgcv::gam(total_unique ~ year +
                                      s(hour, k = 10, by = year) +
                                      s(yday, k = 8, by = year) +
                                      beaufort +
                                      vis +
                                      s(tide, k = 10) +
                                      s(sst, k = 10) +
                                      s(WVHT, k = 10) +
                                      s(chl_interp) +
                                      offset(total_area),
                       data = gam_dat_lb_tot_full, 
                       method = "REML", family = nb, select = TRUE)
summary(sst_gam_tot_full)
#DE = 47.2%, n = 123
anova(sst_gam_tot_full)
visreg(sst_gam_tot_full)

summary(lbgam_tot)
#SFT and WVHT penalized to near-zero
#DE of total density is 51.2%...
summary(gam4lb)
#vs 59.6% for transect count

anova(lbgam_tot)
#year and vis significant parametric terms, hour in 2020 and 2021, and month (all years)
#transect-only model had beaufort and sft in addition to the above
visreg(lbgam_tot)

lbgam_tot_dredge_drop1 <- dredge(lbgam_tot, m.min = 9, fixed = "offset(total_area)")
lbgam_tot_dredge_drop1

lbgam_tot_dredge_all <- dredge(lbgam_tot, fixed = "offset(total_area)")
lbgam_tot_dredge_all

AICc(lbgam_tot, gam4lb)
#different datasets and response variables so this isn't super valid
```

### determining deviance explained of each term by dropping it from the global TRANSECT model - CSULB VERSION - as of 6/16 
*note from Simon Wood: predictors aren't strictly orthogonal, so DE of each term might not sum to DE of global model
```{r}
summary(gam4lb)
# global model explains 59.6% of deviance

gam4lb_year <- mgcv::gam(transect_total ~ s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML", 
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_year)
59.6 - 47.3
#year DE = 12.3%

gam4lb_hour <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML", 
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_hour)
59.6 - 49.3
#hour DE = 10.3%

gam4lb_month <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                          data = gam_dat_lb, method = "REML", 
                          family = nb, select = TRUE,
                          na.action = "na.fail")
summary(gam4lb_month)
59.6 - 41.7
#month DE = 17.9%

gam4lb_beau <- mgcv::gam(transect_total ~ year +
                                          s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_beau)
59.6 - 52.7
#beaufort DE = 6.9%

gam4lb_vis <- mgcv::gam(transect_total ~ year +
                                         s(hour, k = 10, by = year) + 
                                         s(month, k = 8, by = year) +
                                         beaufort +
                                         s(tide, k = 10) +
                                         s(Temp_top, k = 10) +
                                         s(WVHT, k = 50) +
                                         offset(transect_area_km2),
                        data = gam_dat_lb, method = "REML",
                        family = nb, select = TRUE,
                        na.action = "na.fail")
summary(gam4lb_vis)
59.6 - 53.3
#vis DE = 6.3%

gam4lb_tide <- mgcv::gam(transect_total ~ year +
                                          s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_tide)
59.6 - 55.9
#tide DE = 3.7%

gam4lb_temp <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_temp)
59.6 - 56.5
#temp DE = 3.1%

gam4lb_wvht <- mgcv::gam(transect_total ~ year +
                                          s(hour, k = 10, by = year)+
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(gam4lb_wvht)
59.6 - 56.1
#wvht DE = 3.5%
```

### determining deviance explained of each term by dropping it from the global TOTAL model - CSULB VERSION - as of 6/17
*note from Simon Wood: predictors aren't strictly orthogonal, so DE of each term might not sum to DE of global model
```{r}
summary(lbgam_tot)
# global model explains 51.2% of deviance

lbgam_tot_year <- mgcv::gam(total_unique ~ s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML", 
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_year)
51.2 - 41.5
#year DE = 9.7%

lbgam_tot_hour <- mgcv::gam(total_unique ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML", 
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_hour)
51.2 - 47.4
#hour DE = 3.8%

lbgam_tot_month <- mgcv::gam(total_unique ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                          data = gam_dat_lb_tot, method = "REML", 
                          family = nb, select = TRUE,
                          na.action = "na.fail")
summary(lbgam_tot_month)
51.2 - 45.5
#month DE = 5.7%

lbgam_tot_beau <- mgcv::gam(total_unique ~ year +
                                          s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_beau)
51.2 - 52.2
#beaufort DE = -1%

lbgam_tot_vis <- mgcv::gam(total_unique ~ year +
                                         s(hour, k = 10, by = year) + 
                                         s(month, k = 8, by = year) +
                                         beaufort +
                                         s(tide, k = 10) +
                                         s(Temp_top, k = 10) +
                                         s(WVHT, k = 50) +
                                         offset(transect_area_km2),
                        data = gam_dat_lb_tot, method = "REML",
                        family = nb, select = TRUE,
                        na.action = "na.fail")
summary(lbgam_tot_vis)
51.2 - 49.4
#vis DE = 1.8%

lbgam_tot_tide <- mgcv::gam(total_unique ~ year +
                                          s(hour, k = 10, by = year) +
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(Temp_top, k = 10) +
                                          s(WVHT, k = 50) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_tide)
51.2 - 54.2
#tide DE = -3%

lbgam_tot_temp <- mgcv::gam(total_unique ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_temp)
51.2 - 54.4
#temp DE = -3.2%

lbgam_tot_wvht <- mgcv::gam(total_unique ~ year +
                                          s(hour, k = 10, by = year)+
                                          s(month, k = 8, by = year) +
                                          beaufort +
                                          vis +
                                          s(tide, k = 10) +
                                          s(Temp_top, k = 10) +
                                          offset(transect_area_km2),
                         data = gam_dat_lb_tot, method = "REML",
                         family = nb, select = TRUE,
                         na.action = "na.fail")
summary(lbgam_tot_wvht)
51.2 - 54.4
#wvht DE = 3.5%
```

### determining deviance explained of each term by dropping it from the global model LTER VERSION - as of 6/16
```{r}
summary(gam4)
# global model explains 55.6% of deviance

gam4year <- mgcv::gam(transect_total ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4year)
55.6 - 47.9
#year DE = 7.7%

gam4hour <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4hour)
55.6 - 48.5
#hour DE = 7.1%

gam4month <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action= "na.fail")
summary(gam4month)
55.6 - 38
#month DE = 17.6%

gam4beau <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4beau)
55.6 - 52.4
#beaufort DE = 3.2%

gam4vis <- mgcv::gam(transect_total ~ year +
                                      s(hour, k = 10, by = year) + 
                                      s(month, k = 8, by = year) +
                                      beaufort +
                                      s(tide, k = 10) +
                                      s(Temp_top, k = 10) +
                                      s(WVHT, k = 50) +
                                      offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4vis)
55.6 - 53.4
#vis DE = 2.2

gam4tide <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.actipon = "na.fail")
summary(gam4tide)
55.6 - 55.6
#tide DE = 0.0%

gam4temp <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4temp)
55.6 - 53.4
#temp DE = 2.2%

gam4wvht <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis +
                                       s(tide, k = 10) +
                                       s(Temp_top, k = 10) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE, na.action = "na.fail")
summary(gam4wvht)
55.6 - 54.3
#wvht DE = 1.3%
```

### Comparing gam4 models (local LTER vs regional)
```{r}
MuMIn::AICc(gam4, glob_gam, cabrillo3_gam4, scripps3_gam4, cabrillo_3, scripps_3)
#cabrillo+carp and just Cabrillo have lowest AIC

summary(gam4) #55.2% DE with just Carp temp - Cabrillo temp is definitely doing a lot of work
summary(cabrillo3_gam4) #64.3% DE with both Carp and Cabrillo temp
summary(cabrillo_3) #63.1% DE with just Cabrillo temp
```

### Global LTER GAM, but with total unique rather than transect count
```{r}
glob_gam_tot <- mgcv::gam(total_unique ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_3wk, k = 12) +
                                   s(scripps_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(total_area),
                  data = gam_dat2, method = "REML", family = nb)
summary(glob_gam_tot)
#64.5% DE, n = 202
summary(glob_gam)
#69% DE, n = 208

anova.gam(glob_gam_tot)
anova.gam(glob_gam)
#same terms are significant when transect or total count is used

visreg(glob_gam_tot)
```

## Why is SFT significant in transect model, but not total model?
```{r}
sst_20_21 %>% 
  filter(year(date_hour) == 2021) %>% 
ggplot(aes(x = date_hour, y = sst)) +
  geom_point() +
  

ggplot(data = size_dat_3yr, aes(x = video_YN)) +
  geom_bar()
View(daily_3yr_dat)

ggplot(data = daily_3yr_dat, aes(x = as.factor(year), y = sft)) +
  geom_boxplot()

daily_3yr_dat %>% 
  group_by(as.factor(year)) %>% 
  summarize(mean_temp = mean(sft))

ggplot(daily_3yr_dat, aes(x = sft)) +
  geom_point(aes(y = transect_total), color = "blue") + 
  geom_point(aes(y = total_unique), color = "red") +
  geom_smooth(aes(y = transect_total), color = "blue", SE = FALSE) +
    geom_smooth(aes(y = total_unique), color = "red", SE = FALSE)

daily_3yr_dat %>% 
  ggplot(aes(x = sft)) +
    geom_point(aes(y = inner_count)) +
    geom_point(aes(y = outer_count), color = "red") +
    geom_point(aes(y = manual_count), color = "blue")

daily_3yr_dat %>% 
  ggplot(aes(x = sft)) +
    geom_smooth(aes(y = inner_count), color = "red") +
    geom_smooth(aes(y = manual_count), color = "blue")

daily_3yr_dat %>% 
  ggplot(aes(x = sft)) +
    geom_smooth(aes(y = transect_total), color = "red") +
    geom_smooth(aes(y = manual_unique), color = "blue") +
    geom_smooth(aes(y = total_unique), color = "black")

size_dat_with_sft <- left_join(size_dat_unique, daily_3yr_dat, by = "date")

size_dat_with_sft %>% 
  filter(video_YN == "manual" | video_YN == "manual_1" | video_YN == "manual_2") %>% 
  summarize(mean = mean(sft, na.rm = TRUE))

size_dat_with_sft %>% 
  filter(video_YN == "inner" | video_YN == "inner ") %>% 
  summarize(mean = mean(sft, na.rm = TRUE))

size_dat_with_sft %>% 
  filter(video_YN == "outer" | video_YN == "outer ") %>% 
  summarize(mean = mean(sft, na.rm = TRUE))
#mean temp of manual sightings is lower than transect sightings

size_dat_with_sft %>% 
  filter(lat > 0) %>%
  filter(video_YN == "manual" | video_YN == "manual_1" | video_YN == "manual_2") %>% 
ggplot(aes(x = long, y = lat, color = sft)) +
         geom_point(size = 2, alpha = 0.8) +
         scale_color_distiller(palette = "YlOrRd", trans = "reverse")

```

### determining deviance explained of each term by dropping it from the global Cabrillo model - as of 5/13 
*note from Simon Wood: predictors aren't strictly orthogonal, so DE of each term might not sum to DE of global model
```{r}
summary(cabrillo_3)
#local global model explains 63.1% of deviance

anova(cabrillo_3)
#year, beaufort, vis, month (all years), hour (2020 & 2021), cabrillo temp and WVHT all significant

cabrillo3year <- mgcv::gam(transect_total ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3year)
63.1 - 50.5
#year DE = 12.6%

cabrillo3hour <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3hour)
63.1 - 57.5
#hour DE = 5.6%

cabrillo3month <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3month)
63.1 - 43.1
#month DE = 20%

cabrillo3beau <- mgcv::gam(transect_total ~ year +
                                            s(hour, k = 10, by = year) + 
                                            s(month, k = 8, by = year) +
                                            vis +
                                            s(tide, k = 10) +
                                            s(cabrillo_temp_3wk, k = 10) +
                                            s(WVHT, k = 50) +
                                            offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3beau)
63.1 - 55.5
#beaufort DE = 7.6%

cabrillo3vis <- mgcv::gam(transect_total ~ year +
                                      s(hour, k = 10, by = year) + 
                                      s(month, k = 8, by = year) +
                                      beaufort +
                                      s(tide, k = 10) +
                                      s(cabrillo_temp_3wk, k = 10) +
                                      s(WVHT, k = 50) +
                                      offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3vis)
63.1 - 53.2
#vis DE = 9.9

cabrillo3tide <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3tide)
63.1 - 62.8
#tide DE = 0.1%

cabrillo3temp <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3temp)
63.1 - 53.4
#temp DE = 9.7%

cabrillo3wvht <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis +
                                       s(tide, k = 10) +
                                       s(cabrillo_temp_3wk, k = 10) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3wvht)
63.1 - 58.7
#wvht DE = 4.4%
```

### month model, no interactions
```{r}
gam5 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 12) +
                                   s(month, bs = "cc", k = 8) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 20) +
                                   s(Temp_top, k = 100) +
                                   s(WVHT, k = 90) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")
#knots hand-selected to be as large as possible

gam.check(gam5)
#issue with month

summary(gam5)
#only 38.1% of deviance explained

visreg(gam5)

anova.gam(gam5)
#year, beaufort, hour and wave height significant. Month and tide are being modeled as essentially linear

dredge(gam5, m.min = 8)
#dropping vis, temp, tide, month, and area all improve fit

gam5_full_dredge <- dredge(gam5, m.min = 3)
gam5_full_dredge
#tied-for-best models include beaufort, hour, wave height, and year, +/- month
```

### day model, year interactions with hour and day (like gam4 but with day)
```{r}
gam6 <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) +
                                   s(day, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2), data = gam_dat, method = "REML", family = nb, na.action = "na.fail")

summary(gam6)
#61.2% of deviance explained
#hour and tide are being modeled as linear (edf close to 1)

gam.check(gam6, rep = 500)
#low p-value for waveheight and month, but edf not close to k' so it's not a problem with the number of knots (which are set to max anyways)

anova.gam(gam6)
#year, beaufort, hour, and day are significant

visreg(gam6)
visreg_data <- visreg(gam6)

fits <- as.data.frame(visreg_data[[3]][["fit"]][["visregFit"]])
days <- as.data.frame(visreg_data[[3]][["fit"]][["day"]])

gam6_day_fits <- bind_cols(fits, days)
#main peak is day 202 (July 21), secondary peak at day 124 (May 4), and minimum is day 275 (October 2)

#wald_gam(gam6, comp=list(beaufort=levels(gam_dat$beaufort), year=c("2019", "2020", "2021")))
#is this comparing for all years or just 2021?

gam6_year_emms <- emmeans(gam6, "year")
pwpm(gam6_year_emms, type = "response")
#2020 NOT sig. diff. from 2021 when all other factors are taken into account - but weird that 2020 estimates are higher:

ggplot(data = gam_dat, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

gam6_beaufort_emms <- emmeans(gam6, "beaufort")
pwpm(gam6_beaufort_emms, type = "response")
#levels of beaufort not sig. diff. from each other even though beaufort is a significant parametric term?

#gam6_drop1_dredge <- dredge(gam6, m.min = 8)
gam6_drop1_dredge
#best 8-term model is the one without vis

#gam6_full_dredge <- dredge(gam6, m.min = 3, fixed = "offset(transect_area_km2)")
gam6_full_dredge
```

### comparing global models:
```{r}
MuMIn::AICc(gam1, gam2, gam3, gam4, gam5, gam6) 
# corrected AIC for small sample size

summary(gam3)#DE = 60.4%
summary(gam4)#DE = 60.7%
summary(gam5)#DE = 38.1%
summary(gam6) #DE = 61.2%

```
*gam4 and gam6 (year interaction with hour and month/day only) have lowest AIC and highest deviance explained. Differences in AIC and DE are marginal, so I should choose whichever does a better job of answering my questions

## Modelling proportion of juveniles (probably want to do separate models instead)

### Build the dataset
```{r}
prop_dat <- daily_3yr_dat %>%
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(total_sized)) %>%
  filter(total_sized > 0) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))
```

### Examine relationships
```{r}
ggplot(prop_dat, aes(x = as.numeric(beaufort), y = juvenile/total_sized)) + 
  geom_point() +
  geom_smooth()

ggplot(prop_dat, aes(x = as.numeric(vis), y = juvenile/total_sized)) +
  geom_point() +
  geom_smooth()

ggplot(prop_dat, aes(x = Temp_top, y = juvenile/total_sized)) + 
  geom_jitter() +
  geom_smooth() + 
  facet_grid(year~.)
```

### Modelling proportion of juveniles
```{r}
propgam1 <- mgcv::gam(juvenile ~ year +
                                 s(hour, k = 10, by = year) +
                                 s(month, k = 8, by = year) + 
                                 beaufort +
                                 vis +
                                 s(tide, k = 10) +
                                 s(Temp_top, k = 10) +
                                 s(WVHT, k = 50) +
                                 offset(total_sized), data = prop_dat,
                                 method = "REML", family = nb,
                                 na.action = "na.fail", select = TRUE)

summary(propgam1)
# only 11.9 % of deviance explained. almost everything is being modelled as a flat line

gam.check(propgam1)
#all good

visreg(propgam1)

anova(propgam1)
#month in 2021 is significant, hour in 2021 is marginal (p = 0.555)

#propgam1_dredge_drop_1 <- dredge(propgam1, m.min = 8, fixed = "offset(total_sized)")
propgam1_dredge_drop_1

#propgam1_dredge_full <- dredge(propgam1, m.min = 3, fixed = "offset(total_sized)")
propgam1_dredge_full
```

### determining deviance explained of each term by dropping it from the global model
```{r}
summary(propgam1)
#global model explains 11.9% of deviance

propgam1year <- mgcv::gam(juvenile ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1year)
11.9 - 10.6
#year DE = 1.3%

propgam1hour <- mgcv::gam(juvenile ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1hour)
11.9 - 8.36
#hour DE = 3.54% (not sure why it gave me an extra decimal place here)

propgam1month <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1month)
11.9 - 8.62
#month DE = 3.28%

propgam1beau <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1beau)
11.9 - 12.1
#beaufort DE = -0.2 !?!?!?!

propgam1vis <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1vis)
11.9 - 11.8
#vis DE = 0.1%

propgam1tide <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1tide)
11.9 - 10.9
#tide DE = 1.0%

propgam1temp <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(tide, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1temp)
11.9 - 11.9
#temp DE = 0%

propgam1wvht <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1wvht)
11.9 - 11.9
#wvht DE = 0%
```

## Modelling density of juveniles

### Building the dataset
```{r}
juv_dat <- daily_3yr_dat %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))

juv_dat_all <- daily_3yr_dat %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month = as.factor(month)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000)

juv_datlb <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(month = month(mdy)) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(vis = as.factor(vis)) %>%
  mutate(beaufort = as.factor(beaufort)) %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(sft)) %>% 
  filter(!is.na(tide))
```

### Examine relationships
```{r}
ggplot(juv_dat, aes(x = as.numeric(beaufort), y = juvenile)) + 
  geom_point() +
  geom_smooth()

ggplot(juv_dat, aes(x = as.numeric(vis), y = juvenile)) +
  geom_point() +
  geom_smooth()

ggplot(juv_dat, aes(x = Temp_top, y = juvenile)) + 
  geom_jitter() +
  geom_smooth() + 
  facet_grid(year~.)
```

### Modelling density of juveniles - LTER DATA
```{r}
juvgam1 <- mgcv::gam(juvenile ~ year +
                                s(hour, k = 10, by = year) +
                                s(month, k = 8, by = year) + 
                                beaufort +
                                vis +
                                s(tide, k = 10) +
                                s(Temp_top, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m), data = juv_dat,
                                method = "REML", family = nb, select = TRUE, na.action = "na.fail")

summary(juvgam1)
# 27.5% of deviance explained, n = 254

gam.check(juvgam1)
#issues with month

visreg(juvgam1)

anova(juvgam1)
#year, month in 2019 and 2021 are significant, temperature p = 0.068

juvgam1_dredge_drop_1 <- dredge(juvgam1, m.min = 8, fixed = "offset(area_20m)")
juvgam1_dredge_drop_1
# no vis and no beaufort are best two

#juvgam1_dredge_full <- dredge(juvgam1, m.min = 3, fixed = "offset(area_20m)")
juvgam1_dredge_full

juv_year_emms <- emmeans(juvgam1, "year")
pwpm(juv_year_emms, type = "response")
#2020 and 2021 significantly higher than 2019
```

### determining deviance explained of each term by dropping it from the global model - LTER DATA
```{r}
summary(juvgam1)
#global model explains 27.5% of deviance

juvgam1year <- mgcv::gam(juvenile ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1year)
27.5 - 20.7
#year DE = 6.8%

juvgam1hour <- mgcv::gam(juvenile ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1hour)
27.5 - 25.6
#hour DE = 1.9%

juvgam1month <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1month)
27.5 - 19.4
#month DE = 8.1

juvgam1beau <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    vis +
                                    s(tide, k = 10) +
                                    s(Temp_top, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1beau)
27.5 - 26.1
#beaufort DE = 1.4

juvgam1vis <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1vis)
27.5 - 25.5
#vis DE = 2.0%

juvgam1tide <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1tide)
27.5 - 27.5
#tide DE = 0%

juvgam1temp <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1temp)
27.5 - 25.7
#temp DE = 1.8%

juvgam1wvht <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(Temp_top, k = 10) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1wvht)
27.5 - 27.5
#wvht DE = 0%
```

### Modelling density of juveniles - CSULB DATA
```{r}
juvgam_lb <- mgcv::gam(juvenile ~ year +
                                s(hour, k = 10, by = year) +
                                s(month, k = 8, by = year) + 
                                beaufort +
                                vis +
                                s(tide, k = 10) +
                                s(sft, k = 10) +
                                s(WVHT, k = 50) +
                         offset(area_20m), data = juv_datlb,
                                method = "REML", family = nb,
                       select = TRUE, na.action = "na.fail")

summary(juvgam_lb)
# 27.3% of deviance explained, n = 248

gam.check(juvgam_lb)
#issues with month, not sure if one order of magnitude counts as "edf much lower than k"

visreg(juvgam_lb)

anova(juvgam_lb)
#year, month in 2019 and 2021 are significant

juvgamlb_dredge_drop_1 <- dredge(juvgamlb, m.min = 8, fixed = "offset(area_20m)")
juvgamlb_dredge_drop_1
# no vis and no beaufort are best two

#juvgamlb_dredge_full <- dredge(juvgam_lb, m.min = 3, fixed = "offset(area_20m)")
juvgamlb_dredge_full

#post-hoc test of significant parametric terms:
juvlb_year_emms <- emmeans(juvgam_lb, "year")
pwpm(juvlb_year_emms, type = "response")
#2020 and 2021 significantly higher than 2019
```

### determining deviance explained of each term by dropping it from the global model - CSULB DATA
```{r}
summary(juvgam_lb)
#global model explains 27.3% of deviance

juvgam_lb_year <- mgcv::gam(juvenile ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_year)
27.3 - 21.1
#year DE = 6.2%

juvgam_lb_hour <- mgcv::gam(juvenile ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_hour)
27.3 - 25.9
#hour DE = 3.0%

juvgam_lb_month <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_month)
27.3 - 19.3
#month DE = 8.0

juvgam_lb_beau <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    vis +
                                    s(tide, k = 10) +
                                    s(sft, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_beau)
27.5 - 26.2
#beaufort DE = 1.3%

juvgam_lb_vis <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_vis)
27.3 - 24.6
#vis DE = 2.7%

juvgam_lb_tide <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_tide)
27.3 - 27.3
#tide DE = 0%

juvgam_lb_temp <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_temp)
27.3 - 26.7
#temp DE = 0.6%

juvgam_lb_wvht <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(sft, k = 10) +
                                    offset(area_20m),
                  data = juv_datlb, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam_lb_wvht)
27.3 - 27.3
#wvht DE = 0%
```

## Modelling density of adults

### Building the dataset
```{r}
ad_dat <- daily_3yr_dat %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(adult)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))

ad_datlb <- daily_3yr_dat %>%
  mutate(year = as.factor(as.character(year))) %>%
  mutate(month = month(mdy)) %>%
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(vis = as.factor(vis)) %>%
  mutate(beaufort = as.factor(beaufort)) %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(adult)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(sft)) %>% 
  filter(!is.na(tide))
```

### Modelling density of adults - CSULB DATA
```{r}
adgam_lb <- mgcv::gam(adult ~ year +
                                s(hour, k = 10, by = year) +
                                s(month, k = 8, by = year) + 
                                beaufort +
                                vis +
                                s(tide, k = 10) +
                                s(sft, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m), data = ad_datlb,
                      method = "REML", family = nb,
                      select = TRUE, na.action = "na.fail")

summary(adgam_lb)
# 37% of deviance explained, n = 248

gam.check(adgam_lb)
#issues with month, not sure if 7 vs 0 counts as "edf much lower than k"

visreg(adgam_lb)

anova(adgam_lb)
#year, hour (in 2019 and 2021), month (0.0556 in 2019 and p < 0.05 in 2021), and tide are significant

#adgamlb_dredge_drop_1 <- dredge(adgam_lb, m.min = 8, fixed = "offset(area_20m)")
adgamlb_dredge_drop_1
#dropping beaufort and vis improve fit over global model, dropping temp is equal to global

#adgamlb_dredge_full <- dredge(adgam_lb, m.min = 3, fixed = "offset(area_20m)")
adgamlb_dredge_full

#post-hoc test of significant parametric terms:
adlb_year_emms <- emmeans(adgam_lb, "year")
pwpm(adlb_year_emms, type = "response")
#2020 and 2021 significantly higher than 2019
```

### Determining DE by dropping each term from the adult model - CSULB DATA
```{r}
summary(adgam_lb)
#global model explains 37% of deviance

adgam_lb_year <- mgcv::gam(adult ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                           data = ad_datlb, method = "REML",
                           family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_year)
37 - 29
#year DE = 8.0%

adgam_lb_hour <- mgcv::gam(adult ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                           data = ad_datlb, method = "REML",
                           family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_hour)
37 - 32.9
#hour DE = 4.1%

adgam_lb_month <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                            data = ad_datlb, method = "REML", family = nb,
                            na.action = "na.fail", select = TRUE)
summary(adgam_lb_month)
37 - 29.6
#month DE = 7.4

adgam_lb_beau <- mgcv::gam(adult ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    vis +
                                    s(tide, k = 10) +
                                    s(sft, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                           data = ad_datlb, method = "REML",
                           family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_beau)
37 - 36.2
#beaufort DE = 0.8%

adgam_lb_vis <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                          data = ad_datlb, method = "REML",
                          family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_vis)
37 - 35.7
#vis DE = 2.7%

adgam_lb_tide <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(sft, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                           data = ad_datlb, method = "REML", family = nb,
                           na.action = "na.fail", select = TRUE)
summary(adgam_lb_tide)
37 - 31.6
#tide DE = 5.4%

adgam_lb_temp <- mgcv::gam(adult ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                           data = ad_datlb, method = "REML",
                           family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_temp)
37 - 37
#temp DE = 0%

adgam_lb_wvht <- mgcv::gam(adult ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(sft, k = 10) +
                                    offset(area_20m),
                           data = ad_datlb, method = "REML",
                           family = nb, na.action = "na.fail", select = TRUE)
summary(adgam_lb_wvht)
37 - 37
#wvht DE = 0%
```

### Modelling density of adults - LTER DATA
```{r}
adgam1 <- mgcv::gam(adult ~ year +
                                 s(hour, k = 10, by = year) +
                                 s(month, k = 8, by = year) + 
                                 beaufort +
                                 vis +
                                 s(tide, k = 10) +
                                 s(Temp_top, k = 10) +
                                 s(WVHT, k = 50) +
                                 offset(area_20m), data = ad_dat,
                                 method = "REML", family = nb,
                                 na.action = "na.fail", select = TRUE)

summary(adgam1)
#38% DE, n = 254

gam.check(adgam1)

visreg(adgam1)

anova(adgam1)
#year, hour (2019 & 2021), month (2019 & 2021), and tide significant

#adgam1_dredge_drop_1 <- dredge(adgam1, m.min = 8, fixed = "offset(area_20m)")
adgam1_dredge_drop_1
#no beaufort and no vis are best (same as juvenile model)

#adgam1_dredge_full <- dredge(adgam1, m.min = 3, fixed = "offset(total_sized)")
adgam1_dredge_full
```

### determining deviance explained of each term by dropping it from the global model
```{r}
summary(adgam1)
#global model explains 38% of deviance

adgam1year <- mgcv::gam(adult ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1year)
38 - 29.8
#year DE = 8.2%

adgam1hour <- mgcv::gam(adult ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1hour)
38 - 33.8
#hour DE = 4.2

adgam1month <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1month)
38 - 32
#month DE = 6%

adgam1beau <- mgcv::gam(adult ~ year +
                                s(hour, k = 10, by = year) + 
                                s(month, k = 8, by = year) +
                                vis +
                                s(tide, k = 10) +
                                s(Temp_top, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1beau)
38 - 37.5
#beaufort DE = 0.5

adgam1vis <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1vis)
38 - 37.5
#vis DE = 0.5

adgam1tide <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1tide)
38 - 30.3
#tide DE = 7.7%

adgam1temp <- mgcv::gam(adult ~ year +
                                s(hour, k = 10, by = year) + 
                                s(month, k = 8, by = year) +
                                beaufort +
                                vis+
                                s(tide, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m),
                              data = ad_dat, method = "REML", family = nb,
                              na.action = "na.fail", select = TRUE)
summary(adgam1temp)
38 - 38.3
#temp DE = -0.3

adgam1wvht <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1wvht)
38 - 38.3
#wvht DE = -0.3%
```

## Modelling length of each shark (not sure how this will work)
```{r}
length_dat <- daily_3yr_dat %>%
  filter(!is.na(total_area)) %>% 
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))

```

===============================================================================

# Exploratory Visualizations
* Older individual year visualizations are below 3yr visualizations

## Time of day (using data for year GLM)
```{r}
ggplot(daily_3yr_narm, aes(x = hour, y = transect_total)) +
  geom_jitter() #+
  #geom_smooth(method = "lm")

hour_lm <- lm(transect_total ~ hour, data = daily_3yr_narm)
summary(hour_lm)
#see 0.3 more sharks every hour (p < 0.0001)

hour_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(hour_aov)

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean_time = mean(hour))
```

## 3yr abundance visualizations

Summary table
```{r}
yearly_abund_summary <- daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize("Total sightings" = sum(total_unique, na.rm = TRUE),
            "Survey days" = n(),
            "Sightings per day" = sum(total_unique, na.rm = TRUE)/n(),
            "Maximum abundance" = max(total_unique, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year(date)" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Sightings per day" ~ px(80)) %>% 
    cols_align(align = "center")
gtsave(yearly_abund_summary, "plots/yearly_abund_summary.png")
```

Years stacked (pretty ugly):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity", position = "stack") +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_stacked.png", width = 8, height = 5)
```

Years faceted (good option, potentially as a second graph after years averaged?):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity") +
    facet_grid(as_factor(year)~.) +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    theme(strip.background = element_blank(),
          strip.text.y = element_blank(),
          panel.spacing.y = unit(1, "lines"),
          panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_yrs_facet.png", width = 8, height = 5)
```

Years averaged:
```{r}
daily_3yr_dat %>% 
  group_by(week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)") %>%
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean)) +
    geom_bar(stat = "identity") +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Abundance") +
    theme_clean()
```

Rolling abundance:
WHAT LENGTH WINDOW DO I WANT?
```{r}
daily_3yr_dat %>%
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 10, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month")
ggsave("plots/3yr_rolling_abund_10day.png", width = 9, height = 5)
```

Rolling abundance, only including days within survey window for all three years:
```{r}
#find latest start and earliest stop dates:
daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize(min_day = min(yday(date)),
            max_day = max(yday(date)))
#2019 is latest start date (day 150) and 2021 is earliest stop date (day 312)

daily_3yr_dat %>%
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 7, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0), limits = c(0,6)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month",
                 limits = c(as_date(155), as_date(315)))
    
ggsave("plots/3yr_rolling_abund_bounded_7day.png", width = 9, height = 3)
```

## 3yr environmental variable visualizations
```{r}
daily_3yr_dat %>% 
  gather("data", "value", WDIR:WTMP) %>% 
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~data, scales = "free")

daily_3yr_dat %>%
 ggplot(aes(x = wharf_temp, y = WTMP)) +
  geom_point()

wharf_channel_temp_mod <- lm(WTMP ~ wharf_temp, data = daily_3yr_dat)
summary(wharf_channel_temp_mod)
# r2 = 0.43, p < 0.001

daily_3yr_dat %>%
 ggplot(aes(x = ((9/5)*(wharf_temp) + 32), y = water_temp_padaro)) +
  geom_point() +
  geom_smooth(method = "lm") +
  coord_cartesian(xlim = c(58, 70))

wharf_padaro_temp_mod <- lm(((9/5)*(wharf_temp) + 32) ~ water_temp_padaro, data = daily_3yr_dat)
summary(wharf_padaro_temp_mod)
#r2 = 0.69, p < 0.001
```

# Correlations of Abundance with Environmental Variables

### Channel Water Temp
```{r}
dat_no_19 %>% 
  ggplot(aes(x = WTMP)) +
    geom_histogram(binwidth = 0.5)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WTMP, y = total_unique)) +
    geom_point()

temp_mod <- lm(log1p(total_unique) ~ WTMP, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
# no linear relationship between channel water temp and total unique
plot(temp_mod)

temp_mod_transect <- 

```

### Wharf Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = wharf_temp)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

temp_mod <- lm(log(total_unique) ~ wharf_temp, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ wharf_temp, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod)

#no linear relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, wharf_temp2 = (wharf_temp)^2)

temp_mod2 <- lm(log(total_unique) ~ wharf_temp + wharf_temp2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ wharf_temp + wharf_temp2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Padaro Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = water_temp_bucket)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = log1p(total_unique))) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/bucket_lm_plot.png")

temp_mod <- lm(log(total_unique) ~ water_temp_bucket, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ water_temp_bucket, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod_0)

# significant linear relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, water_temp_bucket2 = (water_temp_bucket)^2)

temp_mod2 <- lm(log(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Swell
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = WVHT_m)) +
    geom_histogram(binwidth = 0.1)

dat_no_19 %>%
  ggplot(aes(x = total_unique)) +
    geom_histogram(binwidth = 1)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WVHT_m, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/wvht_lm_plot.png")

wvht_mod <- lm(log1p(total_unique) ~ log(WVHT_m), data = (dat_no_19))
summary(wvht_mod)
# highly significant correlation between wave height and abundance, r2 = 0.15
#slope is negative (see predictions below)
plot(wvht_mod)

shapiro.test(dat_no_19 %>% mutate(total_unique = log1p(total_unique)) %>% .$total_unique)
# not normal even w/ log transformation
dat_no_19 %>% 
  filter(total_unique > 0) %>% 
  #mutate(total_unique = log10(total_unique)) %>% 
  ggplot(aes(x = total_unique)) +
    geom_histogram()

shapiro.test(dat_no_19 %>%  mutate(WVHT_m = log(WVHT_m)) %>% .$WVHT_m)
# normal w/ log transformation

wvht_sim <- simulateResiduals(fittedModel = wvht_mod, plot = F, n = 1000)
testResiduals(wvht_sim, plot = T)
#looks good
predict(wvht_mod, data.frame(WVHT_m = 1))
# 0.5 m swell = 1.41 individuals, 1 m swell = 0.78, 1.5 m swell = 0.40, 2 m swell = 0.14

daily_3yr_dat %>% 
  ggplot(aes(x = MWD)) +
    geom_histogram(binwidth = 5)

dat_no_19 %>% 
  ggplot(aes(x = MWD, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

wvdir_mod <- lm(log1p(total_unique) ~ MWD, data = (dat_no_19))
summary(wvdir_mod)
```

### Other
```{r}
daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = pressure, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = salinity, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = chlorophyll, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
```

## 2019 Time Series of Transect Data with 8 ft. cutoff
```{r}
dat_2019 %>% 
  dplyr::slice(1:103) %>%
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>%
  filter(transect_total != 0) %>% 
  dplyr::select(Date, Small, Large) %>% 
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2019-05-30")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) +
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "1 month", date_labels = "%b %e",
                 limits = c(as.Date("2019-08-15"), as.Date("2019-12-06")))+
    scale_y_continuous(breaks = 1:7, limits = c(-0.1, 10)) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + 
    theme(plot.title = element_text(size=40), legend.text = element_text(size = 16),
          axis.text = element_text(size = 16)) +
    labs(title = "2019 Great White Shark Abundance, 5/30 - 11/22")
ggsave("plots/wsj_abund_plot_2019.png", width = 16, height = 9)
```

## Effort over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = manual_area_km2)) +
    geom_bar(stat = "identity")
```

## Abundance Over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = total_unique)) +
    geom_point() +
    geom_smooth()

dat_2020 %>% 
  dplyr::rename(Date = date, Small = small_unique, Large = large_unique) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "3 weeks", date_labels = "%b %e",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = 1:12) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + theme(plot.title = element_text(size = 40), legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - 12/18")

ggsave("plots/wsj_abund_plot.png", width = 16, height = 9)

#rolling average, right-aligned 7-day window, total transect density
dat_2020 %>%
  dplyr::rename(Date = date, Transects = transect_total) %>% 
  dplyr::select(Date, Transects) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Transects)) %>%
  ggplot(aes(x = Date, y = rollmean(Transects, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(color = "red3", size = 2) +
    scale_x_date(date_breaks = "2 weeks", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2.5, by = 0.5)) +
    theme_classic() +
    theme(legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1))
# 0.17 is survey area in km^2 and 9.6 is duration of survey in minutes

ggsave("plots/2020_abund_plot.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (used for GRFP, see updated version below)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-10-10")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 10/10/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))
ggsave("plots/grfp_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (all 2020 data)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 12/11/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))

ggsave("plots/2020_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, small sharks only
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  filter(Size == "Small") %>% 
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25))

#plotting time series of abundance by size, excluding days with no sharks
dat_2020 %>% 
  dplyr::filter(transect_total != 0) %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_point(aes(color = Size), size = 1.5) +
    geom_smooth(aes(color = Size), method = "lm", se = FALSE) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))
```

## Basic size class visualizations
```{r}
#View(size_classed_2020)

sum(size_classed_2020$juvenile)
#233 juvenile/YOY
sum(size_classed_2020$adult)
#29 adult/sub-adult

dat_2020_final %>% 
  gather("Sizeclass", "Abundance", juvenile:adult_density) %>% 
  ggplot(aes(x = Sizeclass, y = Abundance)) +
    geom_bar(stat = "identity")

sum(dat_2020_final$juvenile, na.rm = TRUE)/sum(dat_2020_final$adult, na.rm = TRUE)
# Eight times as many juveniles as adults

sum(dat_2020_final$juvenile_density, na.rm = TRUE)/sum(dat_2020_final$adult_density, na.rm = TRUE)
# Juveniles have 8.5 times the average density
```

## Size-classed time series (Abundance and Density)
```{r}
dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult), stat = "identity", fill = "red4") +
    labs(y = "Abundance", x = "Date")

dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile_density), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult_density), stat = "identity", fill = "red4") +
    labs(y = "Density (individuals/km2/min)", x = "Date")
```

## Morning vs afternoon abundance
```{r}
daily_3yr_dat %>%
  filter(hh < 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

daily_3yr_dat %>% 
  filter(hh >= 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

dat_2020 %>% 
  ggplot(aes(x = hms::as_hms(datetime), y = total_unique)) +
    geom_point()

dat_2020 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(bins = 6, color = "white")
ggsave("plots/2020_time_histogram.png", width = 7, height = 5)

dat_2019 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(binwidth = 1, color = "white")
ggsave("plots/2019_time_histogram.png", width = 7, height = 5)

View(dat_2021)

daily_3yr_dat %>%
  #filter(month(datetime) > 6, month(datetime) < 8) %>% 
  dplyr::group_by(hh) %>%
  drop_na(total_unique) %>%
  dplyr::summarize(Abundance = list(mean_se(total_unique))) %>%
  unnest(Abundance) %>% 
  dplyr::rename(Hour = 1, Abundance = y) %>% 
  ggplot(aes(x = Hour, y = Abundance)) +
    geom_bar(stat = "identity") +
    #geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 0.5, width = 0.2) +
    geom_vline(xintercept = 11.5, size = 1) +
    annotate("text", label = "Morning mean: 1.66 (n = 82)", x = 9, y = 3.5) +
    annotate("text", label = "Afternoon mean: 3.19 (n = 70)", x = 15, y = 5.2) +
    theme_classic()
ggsave("plots/3yr_am_vs_pm.png", width = 7, height = 5)
```

## Channel water temperature vs manual density, 2020
```{r}
flights_plus_channel_dat_2020 %>%
  #filter(manual_total_density != 0) %>% 
  ggplot(aes(x = mean_wtemp_previous_24, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

wtemp_quadratic <- lm(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2)

summary(wtemp_quadratic)

plot(dat_2020_full$mean_wtemp_previous_24, fitted(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2))
```

## 2020 Tide Visualizations
```{r}
tide_added_2020 %>% 
  ggplot(aes(x = tide.y)) +
    geom_histogram()

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = small_unique/total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
#huh

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_freqpoly()
#bimodal distribution of age structure

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_histogram(bins = 10)
```

## 2020 Size-Classed Visualizations
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(length)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    stat_function(fun = function(x) 
    dnorm(x, mean = mean(size_dat_2020$length), sd = sd(size_dat_2020$length)) * 0.2 * sum(!is.na(size_dat_2020$length)))

size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    geom_vline(xintercept = 3, size = 1.5, color = "red3", linetype = "dashed") +
    geom_vline(xintercept = mean(size_dat_2020$length_m, na.rm = TRUE),
               size = 1.5, color = "dodgerblue") +
    theme_classic() +
    scale_x_continuous(name = "Length (m)", breaks = seq(from = -1, to = 5, by = 0.5))
ggsave("plots/2020_size_histogram.png", width = 7, height = 6)

size_dat_2020 %>%
  drop_na(unique, size_class) %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = week(date), fill = size_class)) +
    geom_bar(position = "stack", stat = "count") +
    scale_fill_manual(values = c("red3", "dodgerblue")) +
    scale_x_continuous(name = "Week") +
    theme_classic()
ggsave("plots/2020_size_classed_time_series.png", width = 7, height = 4)
```


```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
Mode(size_dat_2020$length)

size_dat_2020 %>% 
  slice(1:201) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(1:201) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

dat_2020 %>% 
  slice(1:86) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/86)

dat_2020 %>% 
  slice(86:nrow(dat_2020)) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/((nrow(dat_2020))-86))
```

## Size over Time

### Individual years
```{r}
size_dat_2020 %>%
  drop_na(length) %>%
  filter(unique == "Y") %>% 
  dplyr::group_by(date) %>% 
  dplyr::summarize(mean_length = mean(length_m)) %>% 
  ggplot(aes(x = date, y = mean_length)) + 
    geom_point() +
    geom_smooth()

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = date, y = length_m)) + 
    geom_point() + 
    geom_smooth

#2021:
size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(length_adj_m > 0) %>% 
  ggplot(aes(x = mdy(date), y = length_adj_m)) + 
    geom_point() +
    geom_smooth(method = "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression.png", width = 5, height = 5)

size_lm_2021 <- lm(length_m ~ date, data = size_dat_2021)
summary(size_lm_2021)

#2021, quadratic:
size_dat_2021 <- mutate(size_dat_2021, date2 = (as.numeric(date))^2)

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = mdy(date), y = length_adj_m)) + 
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression_quad.png", width = 5, height = 5)

#need date as numeric or something for this to work
size_lm_2021_quad <- lm(length_adj_m ~ date + (date)^2, data = size_dat_2021)
summary(size_lm_2021_quad)
```

### Comparing size across years

Histogram of Daily Age Structure:
```{r}
size_classed_3yr %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#no 2019:
size_classed_3yr %>%
  dplyr::slice(-c(1:19)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2020 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:19, 71:144)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2021 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:70)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()
ggsave("plots/adult_swim.png")
#VERY pronounced bimodal distribution in 2021
#despite adults being just 40% of observations, there are MORE days where we saw only adults than where we saw only juveniles
#WHAT factors determine "adult swim" days???
```

Histogram faceted by year:
```{r}
size_dat_unique %>%
  filter(length_adj_m > 0) %>% 
  mutate(year = as_factor(year(mdy(date)))) %>%
  ggplot(aes(x = length_adj_m, fill = year)) +
    geom_histogram() +
    facet_grid(rows = vars(year)) +
    theme_clean() +
    labs(y = "Count", x = "Length (m)", fill = "Year") +
    theme(strip.text.y = element_blank()) +
    scale_fill_brewer(palette = "Dark2")
ggsave("plots/size_3yrs_histo.png", height = 5, width = 5)
```

looking at one month only:
```{r}
size_dat_unique %>% 
  filter(month(date) == 8) %>% 
  ggplot(aes(x = yday(date), y = length)) +
    geom_point() +
    facet_grid(cols = vars(year(date)))
```

Years overlaid:
```{r}
size_dat_unique %>%
  filter(length_adj_m > 0) %>% 
  mutate(day = as_date(yday(mdy(date)))) %>% 
  mutate(year = as_factor(year(mdy(date)))) %>% 
  ggplot(aes(x = day, y = length_adj_m)) +
    geom_point(aes(color = year), alpha = 0.8) +
    geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Dark2")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)
```

years overlaid, separate regressions:
```{r}
size_dat_unique %>% 
  ggplot(aes(x = as_date(yday(date)), y = length, color = as.factor(year))) +
    geom_point(alpha = 0.2) +
    geom_smooth(method= "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%b", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_scatter_regressions.png", height = 5, width = 5)

#years overlaid, windowed:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as_date(yday(date)), y = length)) +
    geom_point() +
    #geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)

#years separated, violin plot, July-September:
size_dat_unique %>% 
  filter(month(date) == c(7:9)) %>% 
  ggplot(aes(x = as.factor(year), y = length)) +
    geom_violin(fill = "dodgerblue") #+
    #geom_hline(yintercept = 3)
ggsave("plots/size_violin_july-sept.png")

#years separated, box plot:
size_dat_unique %>% 
  filter(length < 5) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    theme(panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box.png", width = 4, height = 5)

year_size_mod <- aov(length ~ as.factor(year(date)), data = size_dat_unique)
summary(year_size_mod)
plot(year_size_mod)
TukeyHSD(year_size_mod)
```

```{r}
#years separated, overlap window, box plot:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box_windowed.png", width = 4, height = 5)

#size summary table in FEET
size_dat_unique %>% 
  group_by(year) %>% 
  summarize(min = min(length_ft, na.rm = TRUE),
            max = max(length_ft, na.rm = TRUE),
            median = round(mean(length_ft, na.rm = TRUE), digits = 1),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = sum(size_class == "Juvenile", na.rm = TRUE)/
                                    sum(size_class == "Adult", na.rm = TRUE))

#size summary table in METERS, percentage adults
yearly_size_summary <- size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Percent Adults" = 
              sum(size_class == "Adult", na.rm = TRUE)/
                (sum(size_class == "Juvenile", na.rm = TRUE) +
                 sum(size_class == "Adult", na.rm = TRUE))) %>%
  gt() %>%
    fmt_number(2:4, decimals = 1) %>% 
    fmt_number(7, decimals = 2) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range") %>% 
    cols_width("Percent Adults" ~ px(100)) %>% 
    cols_align(align = "center")
yearly_size_summary
gtsave(yearly_size_summary, "plots/yearly_size_summary_table.png")

#size summary table in METERS, juveniles per adult
size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = 
              sum(size_class == "Juvenile", na.rm = TRUE)/
              sum(size_class == "Adult", na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(7, decimals = 1) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range")
```

## Haphazard vs Transect Visualizations
```{r}
dat_2020 %>% 
  filter(total_unique > 0) %>% 
  filter(!is.na(manual_total), !is.na(transect_total)) %>% 
  dplyr::rename(Date = date, Haphazard = manual_total, Transect = transect_total) %>% 
  dplyr::select(Date, Haphazard, Transect) %>%
  gather("Flight", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Flight = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Flight), stat = "identity", position = "dodge") +
    scale_x_date(date_breaks = "1 week", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-15")))+
    scale_y_continuous(breaks = 1:10) +
    scale_fill_manual(values = brewer.pal(8, "Dark2")[c(8,1,2)], "") + theme_wsj() + theme(plot.title = element_text(size=40), legend.text = element_text(size = 16), axis.text = element_text(size = 16)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - DD/MM")
ggsave("plots/wsj_survey_comp_plot.png", width = 16, height = 9)

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>% 
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>% 
  gather("Method", "Abundance", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Abundance = list(mean_se(Abundance))) %>%
  unnest(Mean_Abundance) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Abundance") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
      
ggsave("plots/3yr_methods_abundance.png", width = 7, height = 7)

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>% 
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>% 
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Density = list(mean_se(Density))) %>%
  unnest(Mean_Density) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
  
ggsave("plots/3yr_methods_density.png", width = 7, height = 7)

daily_3yr_dat %>%
  mutate(transect_frequency = transect_total/transect_duration_min,
         manual_frequency = manual_unique/manual_duration_min) %>% 
  select(transect_frequency, manual_frequency) %>%
  dplyr::rename(Transect = transect_frequency, Manual = manual_frequency) %>%
  gather("Method", "Frequency", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Freq = list(mean_se(Frequency))) %>%
  unnest(Mean_Freq) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Frequency (individuals/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))

# Density WITHOUT time:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
```

## Comparative Analysis of Methods
```{r}
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  t.test(Density ~ Method, .)
  
shapiro.test(daily_3yr_dat$transect_total_density) 
shapiro.test(daily_3yr_dat$manual_unique) 

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total_density)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_total_density)) +
    geom_density()

# Abundance
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual abundance (mean = 2.3, SE = 0.20) was higher than transect abundance (mean = 1.3, SE = 0.14, p < 0.001)

# Effort-Corrected Density:
daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>%
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>% 
  rename("Density" = y) %>% 
  ggplot(aes(x = Method, y = Density)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.4)
ggsave("plots/methods_comp_plot.png")

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual effort-corrected density (mean = 0.64, SE = 0.1) was higher than transect abundance (mean = 0.48, SE = 0.06, p < 0.001)

# Raw Density:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens)

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total/transect_area_km2)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_unique/manual_area_km2)) +
    geom_density()

dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)
## Manual density (mean = 7.12, SE = 1.05) was not higher than transect density (mean = 6.78, SE = 0.06, p < 0.001)
```

## Heatmaps

### All 3 years
```{r}
size_dat_3yr %>%
  filter(video_YN == "manual" | video_YN == "manual_1") %>% 
  filter(unique == "Y") %>%
  #filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point()
    #geom_smooth()
#follows a certain distance from the coastline

size_dat_3yr %>%
  filter(video_YN == "manual" | video_YN == "manual_1") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405), alpha = 0.5)
ggsave("plots/heatmap_3yrs.png", width = 10)

size_dat_3yr %>%
  filter(video_YN == "inner" | video_YN == "outer") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  dplyr::filter(lat > 0) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 15) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405))

View(size_dat_3yr)
```

### All Sharks, 2020
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point(aes(color = size_class))

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>%
  ggplot(aes(x = long, y = lat)) +
  geom_hex(bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
ggsave("grfp_hexmap.png")

register_google(key = "[AIzaSyDmz6xT3R938U6s5VWXuguMSq6ROr3X2KM]", write = TRUE)

location <- c(-120, 34, -119, 35)

carp <- get_map(location = location, source = "osm", zoom = 7)

ggmap(carp) +
  geom_hex(data = size_dat_2020, aes(x = long, y = lat), bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
```

### Juveniles, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Juvenile") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_juvenile_heatmap.png", width = 7, height = 6)
```

### Adults, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Adult") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_adult_heatmap.png", width = 7, height = 6)

mean(size_dat_2020$length_m, na.rm = TRUE)
median(size_dat_2020$length, na.rm = TRUE)

sum(size_dat_2020$size_class == "Adult" & size_dat_2020$unique == "Y", na.rm = "TRUE")/sum(size_dat_2020$unique == "Y", na.rm = "TRUE")
```



# Modelling

## Factors affecting total manual density
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year(transect_datetime) != 2019) %>% 
  mutate(hour = hour(transect_datetime))

total_density_mod <- glmmTMB(manual_unique ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = poisson, ziformula = ~1)

total_density_sim <- simulateResiduals(fittedModel = total_density_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

dredge(total_density_mod)
#time of day, wharf temp, and manual effort are all predictors in best fit model, model without hour has weight of 0.234

summary(total_density_mod)
#hour significant, wharf temp not

plot(allEffects(total_density_mod))
```

## Factors affecting ratio of juveniles to adults NOT WORKING
```{r}
dat_no_19 <- dat_no_19 %>% 
  mutate(hour = hour(transect_datetime))

View(dat_no_19)

ratio_mod <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = binomial)

ratio_sim <- simulateResiduals(fittedModel = ratio_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

ratio_mod2 <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp, data = dat_no_19, family = binomial)

AICctab(ratio_mod, ratio_mod2)

dredge(ratio_mod)
#not working?

summary(ratio_mod)
# hour significant, wharf temp not
plot(allEffects(ratio_mod))

summary(ratio_mod2)
# higher AIC than temp + hour model

emmeans(ratio_mod)
```

## Modelling Transect vs Manual abundance estimates
```{r}


```

## Modelling Time of Day Effects on Abundance, 2020
```{r}
View(dat_2020_full)
ggplot(aes(x = total_unique), data = dat_2020) +
  geom_bar()

diel_model <- glmmTMB(total_unique ~ hour(datetime.x) + mean_wtemp_previous_24 + WVHT_m + vis + beaufort, data = dat_2020_full, family = nbinom2)
dredge(diel_model)

diel_mod_1 <- glmmTMB(total_unique ~ mean_wtemp_previous_24 + WVHT_m + vis, data = dat_2020_full, family = nbinom2)

fam_list <- list(family = alist(
    binomial = binomial,
    genpois = genpois,
    poisson = poisson,
    nbinom1 = nbinom1,
    nbinom2 = nbinom2,
    ))

getAllTerms(diel_mod_1)

dredge(diel_mod_1, fixed = ~ cond(mean_wtemp_previous_24) + cond(vis) + cond(WVHT_m), varying = fam_list)

diel_mod <- update(diel_mod_1, family = nbinom1)
dredge(diel_mod)

summary(diel_mod)

diel_simulationOutput <- simulateResiduals(fittedModel = diel_mod, n = 250)
plotSimulatedResiduals(simulationOutput = diel_simulationOutput)
testOverdispersion(diel_mod)

plot(allEffects(diel_model), partial.residuals = TRUE)
```

## Abundance Histogram, 2020
```{r}
dat_2020 %>% 
  #filter(total_unique != 0) %>%
  gather("Size", "Abundance", 20:21) %>% 
  ggplot(aes(x = Abundance, fill = Size)) +
    geom_bar(position = "dodge") +
    scale_x_continuous(breaks = 0:10) +
    theme_classic()
```

## Comparing size between methods
```{r}
joined_2020 %>% 
  filter(video.y == "outer") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on outer transect is 2.88

joined_2020 %>% 
  filter(video.y == "outer") %>%
  nrow()
# only 13 sharks seen in outer video?

joined_2020 %>% 
  filter(video.y == "inner") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on inner transect is 2.49

joined_2020 %>% 
  filter(video.y == "inner") %>%
  nrow()
# 65 sharks in inner video

joined_2020 %>% 
  filter(video.y != "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size for transect flights overall is 2.55

joined_2020 %>% 
  filter(video.y == "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size on manual flights in 2.43 - bias towards larger sharks in transect flights?

#Comparing abundance estimates between methods:
aov_method_abund_dat <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      gather("method", "abundance", 2:3)

aov_method_abund_2_dat <- dat_2020 %>% 
                        dplyr::select(date, transect_total, haphazard_unique) %>% 
                        slice(23:nrow(dat_2020)) %>%
                        gather("method", "abundance", 2:3) %>% 
                        filter(!is.na(abundance))

aov_method_abund_dat_no_zero <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      filter(transect_total != 0) %>% 
                      filter(manual_unique != 0) %>%
                      gather("method", "abundance", 2:3)

pairwise.t.test(log1p(aov_method_abund_dat$abundance), aov_method_abund_dat$method)
# p = 0.017
# IMPORTANT: I'm not sure how the NAs affect this - First version removes all dates with an NA for either method, while second version only removes the method that has the NA. First version makes more sense to me.

leveneTest(log1p(abundance) ~ method, data = aov_method_abund_dat)
#approximately equal variances

qqPlot(log1p(aov_method_abund_dat$abundance))
# lol

aov_method_abund_dat %>% 
  ggplot(aes(x = abundance)) +
    geom_histogram()

pairwise.t.test(log1p(aov_method_abund_dat_no_zero$abundance), aov_method_abund_dat_no_zero$method)
#p = 0.042

leveneTest(abundance ~ method, data = aov_method_abund_dat_no_zero)
# fine

qqPlot(log(aov_method_abund_dat_no_zero$abundance))

shapiro.test(aov_method_abund_dat_no_zero$abundance)

#Comparing size estimates between methods:

size_dat_unique <- size_dat_unique %>% 
  mutate(method = ifelse(video == "manual", "manual", "transect"))

pairwise.t.test(size_dat_unique$length_m, size_dat_unique$method)
# p = 0.029

leveneTest(length_m ~ method, data = size_dat_unique)
#equal variances

qqPlot(size_dat_unique$length_m)
# lookin good

shapiro.test(size_dat_unique$length_m)
# hmm

size_dat_unique %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram()
```


## Water temperature (old)
```{r}
mean(flights$water_temp, na.rm=T)

# number of days with each temp 
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# same as above but with channel temps 
ggplot(flights, aes(x=water_temp_channel))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# HOBO temp over time + shark data
temp_plot <- ggplot(flights, aes(x=date, y=water_temp, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp), color="blue")+
  geom_smooth(se=T, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
temp_plot
ggsave("temp_plot.png", width=10, height=7)

# same as above but with channel temps
ggplot(flights, aes(x=date, y=water_temp_channel, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp_channel)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp_channel), color="blue")+
  geom_smooth(se=F, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()

# Air and water temperature over time
ggplot(temps.long, aes(x=date, y=temp, group=type, color=type))+
  geom_line()+
  geom_hline(yintercept=64.90645)

#Air vs channel water temperature
ggplot(flights, aes(x=air_temp, y=water_temp_channel))+
  geom_point()+
  coord_cartesian(xlim=c(55,80))+
  geom_hline(yintercept=64.90645, color="red")+
  stat_smooth(method=lm, se=F, fullrange=T)
```

## Wind
```{r}
wind_avg <- flights %>% ddply(.(wind_dir), summarize, mean=mean(wind_spd)) %>% na.omit(.)
#View(wind_avg)

# Average speed of each direction
ggplot(wind_avg, aes(x=wind_dir, y=mean))+
  geom_bar(stat="identity")+
  coord_polar(theta="x", start=2.7, direction=-1)

# Histogram of wind directions
flights %>% drop_na("wind_dir") %>%
  ggplot(., aes(x=wind_dir))+
    geom_bar()+
    coord_polar(theta="x",start=2.7, direction=-1)

# Wind vs water temp
ggplot(flights, aes(x=wind_spd..kts., y=water_temp))+
  geom_point()+
  geom_smooth(method="lm", se=F)

# Wind vs sky
ggplot(flights, aes(x=wind_spd, y=sky))+
  geom_count()

# Wind vs waves
ggplot(flights, aes(x=wind_spd, y=swell))+
  geom_jitter()+
  geom_smooth(method=lm)
```

## Sky
```{r}
# sky condition vs detections/DNF
ggplot(flights, aes(x=sky, fill=detection))+
  geom_bar(position="stack")+
  scale_fill_manual(values=c("black", "forestgreen", "red"))
```


## Visibility
```{r}
# beaufort vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(beaufort) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
  ggplot(aes(x = beaufort, y = y) )+
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)

# vis vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(vis) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
ggplot(aes(x = vis, y = y) )+
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)
```

## Tide
```{r}
# tide over time + shark data
ggplot(flights, aes(x=date, y=tide, group=beach))+
  geom_line(size=0.5, linetype=2)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
```

# Detection analysis

```{r}
#count(flights, detection)
ggplot(dat_2019, aes(y=wind_spd, x=detection))+
  geom_point()+
  geom_smooth(se=F)
```

## t-test of small vs large abundance, 2020
```{r}
aov_dat <- dat_2020 %>% 
            dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2020)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## t-test of small vs large abundance, 2021
```{r}
aov_dat <- dat_2021 %>% 
            dplyr::rename(Date = date, Small = transect_small,
                          Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2021)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## GLMs to test effects of water temp and sky condition on detections
```{r}
flights_DNF.rm <- filter(flights, detection!="DNF")
det_mod <- glm(detection~water_temp*sky,data=flights_DNF.rm,family=binomial(link="logit"),na.action="na.fail") 
dredge(det_mod)
```
* best model is null model, followed by ~wind_gust, then ~wind_speed

```{r}
DNF_1 <- glm(detection~wind_gust,data=flights,family=binomial(link="logit"))
summary(DNF_1)

ggplot(flights, aes(x=wind_gust, y=detection))+
  geom_count()
```
* even most-correlated variable wind_gust has insignificant effect on detection (p=0.441)

## Diagnostics
```{r}
DNF_simulationOutput <- simulateResiduals(fittedModel = DNF_1, n = 250)
plotSimulatedResiduals(simulationOutput = DNF_simulationOutput)
plotResiduals(flights$wind_spd, DNF_simulationOutput$scaledResiduals) #wonky residuals
#not sure what the differences between the two above lines are

testUniformity(simulationOutput = DNF_simulationOutput)
#testU gives QQ plot, looks normal

#effect plot
plot(allEffects(DNF_1), partial.residuals=TRUE)
```

# Swell analysis
```{r}
ggplot(flights, aes(x=log(swell)))+
  geom_histogram(binwidth=0.25)
```

## GLMs to test effects of wind speed, gust, and direction on swell height
```{r}
swell_mod <- glm(swell~wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="log"), na.action="na.fail")
dredge(swell_mod)
```
* ~wind gust is best, but only somewhat better than null model and ~wind_speed

```{r}
swell_1 <- glm(swell~wind_gust, data=flights, family=gaussian(link="log"))
summary(swell_1)
```
* 1 kt increase in wind gust speed increases log wave height by 0.028; not a significant effect

```{r}
#diagnostics
swell_simulationOutput <- simulateResiduals(fittedModel = swell_1, n = 250)
plotSimulatedResiduals(simulationOutput = swell_simulationOutput)

plot(allEffects(swell_1), partial.residuals=TRUE)
```

# Water Temperature Analysis

```{r}
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=1)
```


# GLMs to test effects of air temp, wind speed, gust and direction on water temp
```{r}
wt_mod <- glm(water_temp~air_temp*wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="identity"), na.action="na.fail")
View(dredge(wt_mod))
```
* best two models (equal AIC) are 
  + 1) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd+wind_gust*wind_dir
  + 2) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir

```{r}
wt_1 <- glm(water_temp~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir, data=flights, family=gaussian(link="identity"))
summary(wt_1)
```
* way too many coefficients, re-doing dredge with max terms = 4

```{r}
dredge(wt_mod, m.max=4)
```
* ~air_temp is best model

```{r}
wt_2 <- glm(water_temp~air_temp, data=flights, family=gaussian(link="identity"))
summary(wt_2)
```
* 1 degree increase in air temp increases water temp by 0.27 degrees, marginally significant effect (p<0.1)