---
title: "MA_analysis_rmd"
author: "John Parsons"
date: "Compiled on `r format(Sys.Date(), '%B %d, %Y`"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Setup
```{r, eval=FALSE}
#install.packages(c("tidyverse", "dplyr", "lme4", "glmmTMB", "effects", "DHARMa", "MuMIn", plyr", "broom.mixed", "ggthemes", "padr", , "ptest", "maps", "mapdata", hexbin", "zoo", "car", "lubridate", "slider", "hms", "olsrr", "bbmle", "emmeans", "gt", "webshot", "cowplot", "jtools", "ggstance", "AER", "mgcv", "itsadug", "installr"))
#install.packages("installr")
library(tidyverse)
slice <- dplyr::slice
#library(plyr); library(dplyr)
library(lme4) #modelling
library(glmmTMB) #modelling
library(DHARMa) #model diagnostics
library(sjstats)
library(effects)
library(broom.mixed)
library(MuMIn)
library(ggthemes) #visualizations
library(RColorBrewer) #visualizations
library(padr)
library(ptest)
library(maps)
library(mapdata)
library(ggmap)
library(hexbin) #heatmaps
library(zoo)
library(car)
library(lubridate) #dates
library(slider)
library(hms)
year <- lubridate::year
week <- lubridate::week
library(olsrr)
library(bbmle) #AIC tables
library(emmeans)#effects testing
library(gt)#nice tables
library(webshot) #saving gt tables
library(cowplot) #moo
library(simfit) #simulating models
library(jtools)
library(ggstance)
library(AER)
library(mgcv) #GAMs
library(visreg) #visualizing GAMs
library(itsadug)
library(installr)
#webshot::install_phantomjs() 
#citation("ptest")

#updateR()
#citation()
```

===========================================================================

# Data Tidying

## 2019 Data Tidying

### Flight-Level Data Tidying (1 row = 1 day) - 2019
```{r}
dat_2019 <- read.csv("data_raw/2019_flight_data.csv")
#view(dat_2019)

dat_2019 <- dat_2019 %>% 
  dplyr::select(!c(3:9,13,14,16:17,20,21,23:25,28,29)) %>% 
  mutate(total_unique = transect_total) %>% 
  mutate(transect_datetime = as_datetime(transect_datetime))

dat_2019[dat_2019 == ""] <- NA

#dat_2019$date <- as.Date(dat_2019$date, "%m/%d/%Y")

dat_2019 <- dat_2019 %>%
  mutate(datetime = as_datetime(transect_datetime)) %>%
  mutate(date_hour = round_date(as_datetime(datetime), "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2019
```{r}
dat_2019 <- dat_2019 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort = 
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = if_else(transect_small == 0, 0,               #transect_small/transect_effort)) %>%
  #mutate(transect_large_density = if_else(transect_large == 0,0,
                                          #transect_large/transect_effort))
#view(dat_2019)
```

### Reading in tide data (Station 9411340) - 2019
```{r}
tide_dat_2019 <- read.csv("data_raw/2019_tide_dat.txt")
#View(tide_dat_2019)
#str(tide_dat_2019)

tide_dat_2019 <- tide_dat_2019 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2019 <- left_join(dat_2019, tide_dat_2019, by = "date_hour")
#view(dat_2019)
```

### Reading in channel buoy data (Station 46053) - 2019
```{r}
channel_dat_2019_raw <- read.table("data_raw/2019_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2019_raw)
#View(channel_dat_2019_raw)

channel_dat_2019 <- channel_dat_2019_raw %>%
                      mutate_if(is.character, as.numeric) %>% 
                      mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
                      mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
                      group_by(MM, DD, hh) %>% 
                      summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
                      ungroup() %>% 
                      filter(MM > 4) %>% 
                      add_column(year = 2019, .before = 1) %>% 
                      mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
                      mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
                      mutate(date_hour = round_date(datetime, "hour")) %>% 
                      select(!datetime) %>% 
                      select(!c(1:4)) %>% 
                      mutate(max_wtemp_previous_24 = 
                              slide_dbl(WTMP, max, .before = 24)) %>% 
                      mutate(min_wtemp_previous_24 = 
                              slide_dbl(WTMP, min, .before = 24)) %>% 
                      mutate(mean_wtemp_previous_24 = 
                              slide_dbl(WTMP, mean, .before = 24))
#view(channel_dat_2019)

#dat_2019$date_hour <- as_datetime(dat_2019$date_hour)
flights_plus_channel_dat_2019 <- left_join(dat_2019, channel_dat_2019, by = "date_hour")

dat_2019 <- flights_plus_channel_dat_2019 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2)

#view(dat_2019)
```

### DO NOT RUN: Reading in SB harbor data (Station 9411340) - 2019 (as of 4/9 planning on replacing this with CSULB or LTER data)
```{r}
harbor_dat_2019 <- read.csv("data_raw/2019_harbor_dat.csv")
#View(harbor_dat_2019)

harbor_dat_2019 <- harbor_dat_2019 %>% 
              rename(time = 2, harbor_windspd = 3,
                     harbor_winddir = 4, harbor_windgust = 5,
                     harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2019 <- left_join(dat_2019, harbor_dat_2019, by = "date_hour")
```

### Size Data Tidying (1 row = 1 individual) - 2019
```{r}
size_dat_2019 <- read.csv("data_raw/size_dat_2019.csv")
#view(size_dat_2019)

size_dat_2019 <- size_dat_2019 %>% 
  select(1:19) %>% 
  dplyr::slice(-(48:49))

size_dat_2019 <- size_dat_2019 %>%
                  mutate(length_adj_NO_RES = as.character(length_adj_NO_RES)) %>% 
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808)

size_dat_2019 %>%
  filter(length_adj_m >= 3) %>% 
  filter(unique == "Y") %>% 
  nrow()
```
*no adults with unadjusted lengths, 15 with adjusted lengths!

## 2020 Data Tidying

### Flight-Level Data Tidying - 2020
```{r}
dat_2020 <- read.csv("data_raw/2020_flight_data.csv")
#view(dat_2020)

#dat_2020$date <- as_date(as.Date(dat_2020$date, "%m/%d/%Y"))

dat_2020$transect_total <- as.numeric(dat_2020$transect_total)

dat_2020$transect_datetime <- as_datetime(dat_2020$transect_datetime)
dat_2020$manual_datetime <- as_datetime(dat_2020$manual_datetime)

dat_2020 <- dat_2020 %>%
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  select(!c(2:8,13,14,16,17,20,21,25,26,28,29:31,38,39))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2020
```{r}
dat_2020 <- dat_2020 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_effort_density =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2020 <- dat_2020 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort =
         manual_total/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340)- 2020
```{r}
tide_dat_2020 <- read.csv("data_raw/2020_tide_dat.txt")
#View(tide_dat_2020)
#str(tide_dat_2020)

tide_dat_2020 <- tide_dat_2020 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>%
  select(-c(1,3,4)) %>%
  rename(tide = 2)

tide_added_2020 <- left_join(dat_2020, tide_dat_2020, by = "date_hour")
#view(tide_added_2020)
```

### Reading in channel buoy data (Station 46053) - 2020
```{r}
channel_dat_2020_raw <- read.table("data_raw/2020_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2020_raw)
#View(channel_dat_2020_raw)

channel_dat_2020 <- channel_dat_2020_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 4) %>% 
  add_column(year = 2020, .before = 1) %>% 
  mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
  mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  select(!datetime) %>% 
  select(!c(1:4)) %>%
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))

#tide_added_2020$date_hour <- as_datetime(tide_added_2020$date_hour)
flights_plus_channel_dat_2020 <- left_join(tide_added_2020, channel_dat_2020, by = "date_hour")
#view(flights_plus_channel_dat_2020)

dat_2020 <- flights_plus_channel_dat_2020 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) 
#view(dat_2020)
```

### DO NOT RUN: Reading in SB harbor data (Station 9411340) - 2020 (as of 4/9 planning on replacing this with CSULB or LTER data)
```{r}
harbor_dat_2020 <- read.csv("data_raw/2020_harbor_dat.csv")
#View(harbor_dat_2020)

harbor_dat_2020 <- harbor_dat_2020 %>% 
              rename(time = 2, harbor_windspd = 3, harbor_winddir = 4, harbor_windgust = 5, harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2020 <- left_join(dat_2020, harbor_dat_2020, by = "date_hour")
#View(dat_2020)
```

### Size Data Tidying (1 row = 1 individual) - 2020
```{r}
size_dat_2020 <- read.csv("data_raw/size_dat_2020.csv")
#view(size_dat_2020)

size_dat_2020 <- size_dat_2020 %>%
                  slice(-363) %>%
                  mutate(length_raw = as.numeric(length_raw)) %>%
                  mutate(length_adj = as.numeric(length_adj)) %>% 
                  mutate(order = as.numeric(order)) %>% 
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808) %>%
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) %>%
                  mutate(depth.correction.factor =
                         as.numeric(depth.correction.factor)) %>% 
                  mutate(asl.correction.factor =
                           as.numeric(asl.correction.factor))

size_dat_2020 %>%
  filter(unique == "Y") %>%
  filter(length_raw_m >= 3) %>% 
  nrow()
#18 adults unadjusted
#48 adults adjusted

size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  filter(length_raw_m < 3) %>% 
  nrow()
#171 juveniles unadjusted
#141 juveniles adjusted
```
* roughly 10x more juveniles than adults (uncorrected lengths)

## 2021 Data Tidying

### Flight-Level Data Tidying - 2021
```{r}
dat_2021 <- read.csv("data_raw/2021_flight_data.csv")
#view(dat_2021)

#dat_2021 <- dplyr::rename(dat_2021, date = 1)
#dat_2021$date <- as_date(as.Date(dat_2021$date, "%m/%d/%Y"))

dat_2021$transect_total <- as.numeric(dat_2021$transect_total)
dat_2021$manual_unique <- as.numeric(dat_2021$manual_unique)
dat_2021$manual_total <- as.numeric(dat_2021$manual_total)

dat_2021$transect_datetime <- as_datetime(dat_2021$transect_datetime)
dat_2021$manual_datetime <- as_datetime(dat_2021$manual_datetime)

dat_2021 <- dat_2021 %>%
  select(!c(3:6,9,10,15,16,18,19,22,23,29:32,39:41)) %>% 
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2021
```{r}
dat_2021 <- dat_2021 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2021 <- dat_2021 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort = 
         manual_unique/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340) - 2021
```{r}
tide_dat_2021 <- read.csv("data_raw/2021_tide_dat.txt")
#View(tide_dat_2021)
#str(tide_dat_2021)
#Local time

tide_dat_2021 <- tide_dat_2021 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2021 <- left_join(dat_2021, tide_dat_2021, by = "date_hour")
#view(dat_2021)
```

### Reading in channel buoy data (Station 46053) - 2021
```{r}
channel_dat_2021_jan <- read.table("data_raw/2021_buoy_dat_jan.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_feb <- read.table("data_raw/2021_buoy_dat_feb.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_mar <- read.table("data_raw/2021_buoy_dat_mar.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_apr <- read.table("data_raw/2021_buoy_dat_apr.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_may <- read.table("data_raw/2021_buoy_dat_may.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jun <- read.table("data_raw/2021_buoy_dat_jun.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jul <- read.table("data_raw/2021_buoy_dat_jul.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_aug <- read.table("data_raw/2021_buoy_dat_aug.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_sep <- read.table("data_raw/2021_buoy_dat_sep.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_oct <- read.table("data_raw/2021_buoy_dat_oct.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_nov <- read.table("data_raw/2021_buoy_dat_nov.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_dec <- read.table("data_raw/2021_buoy_dat_dec.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_raw <- dplyr::bind_rows(channel_dat_2021_jan, channel_dat_2021_feb, channel_dat_2021_mar, channel_dat_2021_apr, channel_dat_2021_may, channel_dat_2021_jun, channel_dat_2021_jul, channel_dat_2021_aug, channel_dat_2021_sep, channel_dat_2021_oct, channel_dat_2021_nov, channel_dat_2021_dec)
#View(channel_dat_2021_raw)

channel_dat_2021 <- channel_dat_2021_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 3) %>% 
  add_column(year = 2021, .before = 1) %>% 
  mutate(date_hour = make_datetime(year, MM, DD, hh)) %>% 
  mutate(date_hour = with_tz(date_hour, "US/Pacific")) %>%
  select(!c(1:4)) %>%
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))
#View(channel_dat_2021)

#dat_2021$date_hour <- as_datetime(dat_2021$date_hour)
flights_plus_channel_dat_2021 <- left_join(dat_2021, channel_dat_2021, by = "date_hour")
#View(flights_plus_channel_dat_2021)

dat_2021 <- flights_plus_channel_dat_2021 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) 
#view(dat_2021)
```

### DO NOT RUN: Reading in SB harbor data (Station 9411340) - 2021 (as of 4/9 planning on replacing this with CSULB or LTER data)
```{r}
harbor_dat_2021 <- read.csv("data_raw/2021_harbor_dat.txt")
#View(harbor_dat_2021)

harbor_dat_2021 <- harbor_dat_2021 %>% 
              rename(time = 2, harbor_windspd = 3,
                     harbor_winddir = 4, harbor_windgust = 5,
                     harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2021 <- left_join(dat_2021, harbor_dat_2021, by = "date_hour")
```

### Size Data Tidying (1 row = 1 individual) - 2021
```{r}
size_dat_2021 <- read.csv("data_raw/size_dat_2021.csv")
#view(size_dat_2021)

size_dat_2021 <- size_dat_2021 %>%
                  mutate(length_raw = as.numeric(length_raw)) %>%
                  mutate(length_adj = as.numeric(length_adj)) %>%
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808) %>%
                  mutate(lat = as.numeric(lat)) %>% 
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) %>%
                  mutate(depth.correction.factor =
                         as.numeric(depth.correction.factor)) %>% 
                  mutate(asl.correction.factor =
                           as.numeric(asl.correction.factor)) %>% 
                  mutate(altitude = as.numeric(altitude))

size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(length_adj_m >= 3) %>% 
  nrow()
#120 adults

size_dat_2021 %>%
  filter(unique == "Y") %>% 
  filter(length_raw_m < 3) %>% 
  nrow()
#337 juveniles
```
MUCH higher % of adults than 2020 - close to 50-50

## Three-Year ("3yr") dataset tidying

### Combine 2019, 2020, and 2021 datasets
```{r}
daily_2yr_dat <- full_join(dat_2019, dat_2020)
#view(daily_2yr_dat)

daily_3yr_dat <- full_join(daily_2yr_dat, dat_2021)
#view(daily_3yr_dat)

daily_3yr_dat <- daily_3yr_dat %>%
  mutate(year = as_factor(year(mdy(date)))) %>% 
  mutate(day = yday(datetime)) %>% 
  mutate(day2 = (yday(datetime))^2)
```

### Reading in Carp LTER data
```{r}
lter_dat_raw <- read.csv("data_raw/lter_dat.csv")
#view(lter_dat_raw)

lter_dat <- lter_dat_raw %>% 
  filter(year > 2018) %>% 
  filter(year < 2022) %>% 
  mutate(hour = (24*decimal_time)) %>%
  filter(row_number() %% 3 == 1) %>% 
  mutate(hour = round(hour, digits = 1)) %>% 
  mutate(date_hour = make_datetime(year, month, day, hour)) %>% 
  mutate(date_hour = force_tz(date_hour, tzone = "America/Los_Angeles")) %>%
  rename(day_of_month = day) %>% 
  select(-c(1, 6:53, 58:75)) %>% 
  select(!year)

#view(lter_dat)

lter_dat %>% 
  filter(Temp_top < 9000) %>% 
  filter(month == 8) %>% 
  filter(day(date_hour) == 29) %>% 
  filter(year(date_hour) == 2021) %>% 
  ggplot(aes(y = Temp_top)) +
    geom_point(aes(x = date_hour)) +
    geom_point(aes(x = date_hour_pst), color = "red")
```

### Adding LTER data to daily 3yr dataset 
```{r}
daily_3yr_dat_1 <- left_join(daily_3yr_dat, lter_dat, by = "date_hour")
view(daily_3yr_dat_1)

daily_3yr_dat <- select(daily_3yr_dat_1, -c(49,50,56,73))
write.csv(daily_3yr_dat, "data/daily_3yr_dat.csv")
```

## Building 3yr size dataset
```{r}
size_dat_2yr <- full_join(size_dat_2019, size_dat_2020)

size_dat_3yr <- full_join(size_dat_2yr, size_dat_2021)
#view(size_dat_3yr)

size_dat_3yr <- size_dat_3yr %>% 
                  select(!c(26:31))
#write.csv(size_dat_3yr, "data/size_dat_3yr.csv")
```

### New 3yr df with size of each unique shark
(1 row = 1 shark, daily data is repeated for days where multiple sharks were observed)
```{r}
#size_dat_3yr <- read.csv("data/size_dat_3yr.csv")

size_dat_unique <- size_dat_3yr %>%
                    filter(unique == "Y")

nrow(size_dat_unique)
#919 sightings across 3 years

size_dat_unique$date <- as.character(size_dat_unique$date)
joined_3yr <- left_join(daily_3yr_dat, size_dat_unique, by = "date")
#view(joined_3yr)
```

## Adding size classes to daily 3yr dataframe
```{r}
#this is only needed if there are adjusted lengths from 2020/2021 transect flights:
size_classed_3yr_20m_test <- joined_3yr %>%
  filter(!is.na(length_adj_m)) %>%
  filter(length_adj_m > 0) %>% 
  group_by(date) %>%
  dplyr::summarize(juvenile = sum(length_adj_m < 3), 
                   adult = sum(length_adj_m >= 3),
                   juvenile_20m = sum(length_adj_m < 3 & ifelse(year == "2019", length_adj_m > 0, (video_YN == "manual" | video_YN == "manual_1"))),
                   adult_20m = sum(length_adj_m >= 3 & ifelse(year == "2019", length_adj_m > 0, (video_YN == "manual" | video_YN == "manual_1"))))

view(size_classed_3yr_20m_test)

size_classed_3yr <- joined_3yr %>%
  filter(!is.na(length_adj_m)) %>%
  filter(length_adj_m > 0) %>% 
  group_by(date) %>%
  dplyr::summarize(juvenile = sum(length_adj_m < 3), 
                   adult = sum(length_adj_m >= 3))

view(size_classed_3yr)

ggplot(data = size_classed_3yr, aes(x = adult)) +
  geom_bar()

daily_3yr_dat <- left_join(daily_3yr_dat, size_classed_3yr, by = "date")

daily_3yr_dat <- daily_3yr_dat  %>%
  mutate(juvenile = ifelse(is.na(juvenile), 0, juvenile)) %>% 
  mutate(adult = ifelse(is.na(adult), 0, adult)) %>% 
  mutate(total_sized = juvenile + adult) %>% 
  filter(detection == "Y" | detection == "N") %>%
  mutate(total_area = ifelse(year == 2019, transect_area_km2,
                             transect_area_km2 + manual_area_km2)) %>% 
  mutate(area_20m = ifelse(year == 2019, 
                           transect_area_km2, manual_area_km2))
```

## Code for columns with manual only (2020/2021) and transect only (2019) juvenile and adult counts, plus a column for the area of the appropriate survey (don't run, this is just leftover from when I thought it was needed - see above note)
```{r}
length_dat_unique <- size_dat_unique %>% 
  mutate(year = as.character(year(mdy(date)))) %>% 
  {if (.$year == "2019") filter(., length_adj_m > 0) else filter(., video_YN == "manual" | video_YN == "manual_1")}
  
length_dat_unique$date <- as.character(length_dat_unique$date)
joined_3yr_length <- left_join(daily_3yr_dat, length_dat_unique, by = "date")
view(joined_3yr_length)

length_classed_3yr <- joined_3yr_length %>%
  filter(!is.na(length_adj_m)) %>%
  filter(length_adj_m > 0) %>% 
  group_by(date) %>%
  dplyr::summarize(juvenile_count = sum(length_adj_m < 3), 
                   adult_count = sum(length_adj_m >= 3)) 

#view(size_classed_3yr)


daily_3yr_dat_length <- left_join(daily_3yr_dat, length_classed_3yr, by = "date")

daily_3yr_dat_length <- daily_3yr_dat_length  %>%
  mutate(juvenile = ifelse(is.na(juvenile_count), 0, juvenile_count)) %>% 
  mutate(adult = ifelse(is.na(adult_count), 0, adult_count)) %>% 
  mutate(total_sized_count = juvenile_count + adult_count) %>% 
  filter(detection == "Y" | detection == "N")
view(daily_3yr_dat_length)

daily_3yr_dat <- daily_3yr_dat  %>%
  mutate(total_area = ifelse(year == 2019, transect_area_km2, transect_area_km2 + manual_area_km2))

```

## Adding Scripps pier data
```{r}
scripps_dat_raw <- read.csv("data_raw/scripps_temp_dat.csv")
view(scripps_dat_raw)

scripps_dat <- scripps_dat_raw %>% 
               slice(-1) %>%
               filter(!is.na(temperature)) %>%
               mutate(temp = as.numeric(temperature)) %>% 
               mutate(mdy = as_date(time)) %>%
               group_by(mdy) %>% 
               summarize(scripps_temp = mean(temp)) %>% 
               mutate(scripps_temp_1wk = lag(scripps_temp, n = 7)) %>% 
               mutate(scripps_temp_2wk = lag(scripps_temp, n = 14)) %>%
               mutate(scripps_temp_3wk = lag(scripps_temp, n = 21)) %>%
               mutate(scripps_temp_4wk = lag(scripps_temp, n = 28))

daily_3yr_dat <- daily_3yr_dat %>% 
  mutate(mdy = mdy(date))

daily_3yr_dat <- left_join(daily_3yr_dat, scripps_dat, by = "mdy")
```

## Adding Cabrillo Point data
```{r}
cabrillo_dat_raw <- read.csv("data_raw/cabrillo_temp_dat.csv")
view(cabrillo_dat_raw)

cabrillo_dat <- cabrillo_dat_raw %>% 
               slice(-1) %>%
               filter(!is.na(sea_water_temperature)) %>%
               mutate(temp = as.numeric(sea_water_temperature)) %>% 
               mutate(mdy = as_date(time)) %>% 
               group_by(mdy) %>% 
               summarize(cabrillo_temp = mean(temp)) %>% 
               mutate(cabrillo_temp_1wk = lag(cabrillo_temp, n = 7)) %>% 
               mutate(cabrillo_temp_2wk = lag(cabrillo_temp, n = 14)) %>%
               mutate(cabrillo_temp_3wk = lag(cabrillo_temp, n = 21)) %>%
               mutate(cabrillo_temp_4wk = lag(cabrillo_temp, n = 28))
#view(cabrillo_dat)

daily_3yr_dat <- left_join(daily_3yr_dat, cabrillo_dat, by = "mdy")
```

## CSULB SST data
```{r}
padaro_sst_19_raw <- read.csv("data_raw/CSULB/CSULB Shark Buoys_SST.csv")
#view(padaro_sst)

padaro_sst_19 <- padaro_sst_19_raw %>% 
  rename(sst = 33) %>% 
  rename(datetime_UTC = 1) %>% 
  filter(!is.na(sst)) %>% 
  filter(year(datetime_UTC) == 2019)
#this looks like SST from one buoy from 2019-2020

filtered_temp_20_21 <- read.csv("data_raw/CSULB/All_2020-2021_filtered_temp_data.csv")
#view(filtered_temp_20_21)

padaro_sst_20_21 <- filtered_temp_20_21 %>% 
  filter(Station == "JWS_Padaro_Buoy") %>% 
  rename(sst = 6) %>% 
  rename(datetime_UTC = 1)

padaro_sst_raw <- bind_rows(padaro_sst_19, padaro_sst_20_21)
#str(padaro_sst)
#view(padaro_sst)
padaro_sst <- padaro_sst_raw %>%
  mutate(datetime_UTC = as_datetime(datetime_UTC)) %>% 
  mutate(datetime_PST = force_tz(datetime_UTC, tzone = "America/Los_Angeles")) %>%
  group_by(date(datetime_PST), hour(datetime_PST)) %>% 
  summarize(sst = mean(sst)) %>% 
  rename(date = 1, hour = 2) %>% 
  mutate(date_hour = make_datetime(year(date), month(date), day(date), hour))

write.csv(padaro_sst, "data/padaro_sst.csv")

daily_3yr_dat <- left_join(daily_3yr_dat, padaro_sst, by = "date_hour")
```

## ERI SONGS data
```{r}
### pre-written code from ERI website ###

# Package ID: edi.648.3 Cataloging System:https://pasta.edirepository.org.
# Data set title: UCSB SONGS Mitigation Monitoring: Wetland Performance Standard - Fish Abundance and Species Richness.
# Data set creator:  Stephen Schroeter -  
# Data set creator:  Henry Page -  
# Data set creator:  Daniel Reed -  
# Data set creator:  David Huang -  
# Data set creator:    - SONGS Mitigation Monitoring 
# Contact:    - Information Manager, SONGS Mitigation Monitoring Project   - songsmmp@lifesci.ucsb.edu
# Stylesheet v2.11 for metadata conversion into program: John H. Porter, Univ. Virginia, jporter@virginia.edu 

inUrl1  <- "https://pasta.lternet.edu/package/data/eml/edi/648/3/1d2580fa7e39c5f0e73f81fb1f58dd12" 
infile1 <- tempfile()
try(download.file(inUrl1,infile1,method="curl"))
if (is.na(file.size(infile1))) download.file(inUrl1,infile1,method="auto")

                   
 dt1 <-read.csv(infile1,header=F 
          ,skip=1
            ,sep=","  
        , col.names=c(
                    "year",     
                    "wetland_code",     
                    "module_code",     
                    "tc_mc_code",     
                    "habitat_code",     
                    "count_per_m2",     
                    "species_count"    ), check.names=TRUE)
               
unlink(infile1)
		    
# Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings
                
if (class(dt1$wetland_code)!="factor") dt1$wetland_code<- as.factor(dt1$wetland_code)
if (class(dt1$module_code)!="factor") dt1$module_code<- as.factor(dt1$module_code)
if (class(dt1$tc_mc_code)!="factor") dt1$tc_mc_code<- as.factor(dt1$tc_mc_code)
if (class(dt1$habitat_code)!="factor") dt1$habitat_code<- as.factor(dt1$habitat_code)
if (class(dt1$count_per_m2)=="factor") dt1$count_per_m2 <-as.numeric(levels(dt1$count_per_m2))[as.integer(dt1$count_per_m2) ]               
if (class(dt1$count_per_m2)=="character") dt1$count_per_m2 <-as.numeric(dt1$count_per_m2)
if (class(dt1$species_count)=="factor") dt1$species_count <-as.numeric(levels(dt1$species_count))[as.integer(dt1$species_count) ]               
if (class(dt1$species_count)=="character") dt1$species_count <-as.numeric(dt1$species_count)
                
# Convert Missing Values to NA for non-dates
                
dt1$wetland_code <- as.factor(ifelse((trimws(as.character(dt1$wetland_code))==trimws("-99999")),NA,as.character(dt1$wetland_code)))
dt1$module_code <- as.factor(ifelse((trimws(as.character(dt1$module_code))==trimws("-99999")),NA,as.character(dt1$module_code)))
dt1$tc_mc_code <- as.factor(ifelse((trimws(as.character(dt1$tc_mc_code))==trimws("-99999")),NA,as.character(dt1$tc_mc_code)))
dt1$habitat_code <- as.factor(ifelse((trimws(as.character(dt1$habitat_code))==trimws("-99999")),NA,as.character(dt1$habitat_code)))
dt1$count_per_m2 <- ifelse((trimws(as.character(dt1$count_per_m2))==trimws("-99999")),NA,dt1$count_per_m2)               
suppressWarnings(dt1$count_per_m2 <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt1$count_per_m2))==as.character(as.numeric("-99999"))),NA,dt1$count_per_m2))
dt1$species_count <- ifelse((trimws(as.character(dt1$species_count))==trimws("-99999")),NA,dt1$species_count)               
suppressWarnings(dt1$species_count <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt1$species_count))==as.character(as.numeric("-99999"))),NA,dt1$species_count))


# Here is the structure of the input data frame:
str(dt1)                            
attach(dt1)                            
# The analyses below are basic descriptions of the variables. After testing, they should be replaced.                 

summary(year)
summary(wetland_code)
summary(module_code)
summary(tc_mc_code)
summary(habitat_code)
summary(count_per_m2)
summary(species_count) 
                # Get more details on character variables
                 
summary(as.factor(dt1$wetland_code)) 
summary(as.factor(dt1$module_code)) 
summary(as.factor(dt1$tc_mc_code)) 
summary(as.factor(dt1$habitat_code))
detach(dt1)               

# Package ID: edi.647.3 Cataloging System:https://pasta.edirepository.org.
# Data set title: UCSB SONGS Mitigation Monitoring: Wetland Survey - Fish Abundance.
# Data set creator:  Stephen Schroeter -  
# Data set creator:  Henry Page -  
# Data set creator:  Daniel Reed -  
# Data set creator:  David Huang -  
# Data set creator:    - SONGS Mitigation Monitoring 
# Contact:    - Information Manager, SONGS Mitigation Monitoring Project   - songsmmp@lifesci.ucsb.edu
# Stylesheet v2.11 for metadata conversion into program: John H. Porter, Univ. Virginia, jporter@virginia.edu 

inUrl1  <- "https://pasta.lternet.edu/package/data/eml/edi/647/3/8c3e35d92cb89c98514df81d6260e270" 
infile1 <- tempfile()
try(download.file(inUrl1,infile1,method="curl"))
if (is.na(file.size(infile1))) download.file(inUrl1,infile1,method="auto")

                   
 dt1 <-read.csv(infile1,header=F 
          ,skip=1
            ,sep=","  
        , col.names=c(
                    "year",     
                    "date",     
                    "survey",     
                    "wetland_code",     
                    "area_code",     
                    "module_code",     
                    "tc_mc_code",     
                    "habitat_code",     
                    "enclosure_number",     
                    "sampling_station_label",     
                    "depth",     
                    "haul_no",     
                    "species_id",     
                    "species_code",     
                    "genus_name",     
                    "species_name",     
                    "count",     
                    "enclosure_area"    ), check.names=TRUE)
               
unlink(infile1)
		    
# Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings
                
if (class(dt1$date)!="factor") dt1$date<- as.factor(dt1$date)
if (class(dt1$survey)!="factor") dt1$survey<- as.factor(dt1$survey)
if (class(dt1$wetland_code)!="factor") dt1$wetland_code<- as.factor(dt1$wetland_code)
if (class(dt1$area_code)!="factor") dt1$area_code<- as.factor(dt1$area_code)
if (class(dt1$module_code)!="factor") dt1$module_code<- as.factor(dt1$module_code)
if (class(dt1$tc_mc_code)!="factor") dt1$tc_mc_code<- as.factor(dt1$tc_mc_code)
if (class(dt1$habitat_code)!="factor") dt1$habitat_code<- as.factor(dt1$habitat_code)
if (class(dt1$enclosure_number)!="factor") dt1$enclosure_number<- as.factor(dt1$enclosure_number)
if (class(dt1$sampling_station_label)!="factor") dt1$sampling_station_label<- as.factor(dt1$sampling_station_label)
if (class(dt1$depth)=="factor") dt1$depth <-as.numeric(levels(dt1$depth))[as.integer(dt1$depth) ]               
if (class(dt1$depth)=="character") dt1$depth <-as.numeric(dt1$depth)
if (class(dt1$haul_no)!="factor") dt1$haul_no<- as.factor(dt1$haul_no)
if (class(dt1$species_id)!="factor") dt1$species_id<- as.factor(dt1$species_id)
if (class(dt1$species_code)!="factor") dt1$species_code<- as.factor(dt1$species_code)
if (class(dt1$genus_name)!="factor") dt1$genus_name<- as.factor(dt1$genus_name)
if (class(dt1$species_name)!="factor") dt1$species_name<- as.factor(dt1$species_name)
if (class(dt1$count)=="factor") dt1$count <-as.numeric(levels(dt1$count))[as.integer(dt1$count) ]               
if (class(dt1$count)=="character") dt1$count <-as.numeric(dt1$count)
if (class(dt1$enclosure_area)=="factor") dt1$enclosure_area <-as.numeric(levels(dt1$enclosure_area))[as.integer(dt1$enclosure_area) ]               
if (class(dt1$enclosure_area)=="character") dt1$enclosure_area <-as.numeric(dt1$enclosure_area)
                
# Convert Missing Values to NA for non-dates
                
dt1$date <- as.factor(ifelse((trimws(as.character(dt1$date))==trimws("-99999")),NA,as.character(dt1$date)))
dt1$survey <- as.factor(ifelse((trimws(as.character(dt1$survey))==trimws("-99999")),NA,as.character(dt1$survey)))
dt1$wetland_code <- as.factor(ifelse((trimws(as.character(dt1$wetland_code))==trimws("-99999")),NA,as.character(dt1$wetland_code)))
dt1$area_code <- as.factor(ifelse((trimws(as.character(dt1$area_code))==trimws("-99999")),NA,as.character(dt1$area_code)))
dt1$module_code <- as.factor(ifelse((trimws(as.character(dt1$module_code))==trimws("-99999")),NA,as.character(dt1$module_code)))
dt1$tc_mc_code <- as.factor(ifelse((trimws(as.character(dt1$tc_mc_code))==trimws("-99999")),NA,as.character(dt1$tc_mc_code)))
dt1$habitat_code <- as.factor(ifelse((trimws(as.character(dt1$habitat_code))==trimws("-99999")),NA,as.character(dt1$habitat_code)))
dt1$enclosure_number <- as.factor(ifelse((trimws(as.character(dt1$enclosure_number))==trimws("-99999")),NA,as.character(dt1$enclosure_number)))
dt1$sampling_station_label <- as.factor(ifelse((trimws(as.character(dt1$sampling_station_label))==trimws("-99999")),NA,as.character(dt1$sampling_station_label)))
dt1$depth <- ifelse((trimws(as.character(dt1$depth))==trimws("-99999")),NA,dt1$depth)               
suppressWarnings(dt1$depth <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt1$depth))==as.character(as.numeric("-99999"))),NA,dt1$depth))
dt1$haul_no <- as.factor(ifelse((trimws(as.character(dt1$haul_no))==trimws("-99999")),NA,as.character(dt1$haul_no)))
dt1$species_id <- as.factor(ifelse((trimws(as.character(dt1$species_id))==trimws("-99999")),NA,as.character(dt1$species_id)))
dt1$species_code <- as.factor(ifelse((trimws(as.character(dt1$species_code))==trimws("-99999")),NA,as.character(dt1$species_code)))
dt1$genus_name <- as.factor(ifelse((trimws(as.character(dt1$genus_name))==trimws("-99999")),NA,as.character(dt1$genus_name)))
dt1$species_name <- as.factor(ifelse((trimws(as.character(dt1$species_name))==trimws("-99999")),NA,as.character(dt1$species_name)))
dt1$count <- ifelse((trimws(as.character(dt1$count))==trimws("-99999")),NA,dt1$count)               
suppressWarnings(dt1$count <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt1$count))==as.character(as.numeric("-99999"))),NA,dt1$count))
dt1$enclosure_area <- ifelse((trimws(as.character(dt1$enclosure_area))==trimws("-99999")),NA,dt1$enclosure_area)               
suppressWarnings(dt1$enclosure_area <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt1$enclosure_area))==as.character(as.numeric("-99999"))),NA,dt1$enclosure_area))


# Here is the structure of the input data frame:
str(dt1)                            
attach(dt1)                            
# The analyses below are basic descriptions of the variables. After testing, they should be replaced.                 

summary(year)
summary(date)
summary(survey)
summary(wetland_code)
summary(area_code)
summary(module_code)
summary(tc_mc_code)
summary(habitat_code)
summary(enclosure_number)
summary(sampling_station_label)
summary(depth)
summary(haul_no)
summary(species_id)
summary(species_code)
summary(genus_name)
summary(species_name)
summary(count)
summary(enclosure_area) 
                # Get more details on character variables
                 
summary(as.factor(dt1$date)) 
summary(as.factor(dt1$survey)) 
summary(as.factor(dt1$wetland_code)) 
summary(as.factor(dt1$area_code)) 
summary(as.factor(dt1$module_code)) 
summary(as.factor(dt1$tc_mc_code)) 
summary(as.factor(dt1$habitat_code)) 
summary(as.factor(dt1$enclosure_number)) 
summary(as.factor(dt1$sampling_station_label)) 
summary(as.factor(dt1$haul_no)) 
summary(as.factor(dt1$species_id)) 
summary(as.factor(dt1$species_code)) 
summary(as.factor(dt1$genus_name)) 
summary(as.factor(dt1$species_name))
detach(dt1)               
         
# seine data:
inUrl2  <- "https://pasta.lternet.edu/package/data/eml/edi/647/3/44dc7a48b659dcc41e145ee5135d0947" 
infile2 <- tempfile()
try(download.file(inUrl2,infile2,method="curl"))
if (is.na(file.size(infile2))) download.file(inUrl2,infile2,method="auto")

                   
 dt2 <-read.csv(infile2,header=F 
          ,skip=1
            ,sep=","  
        , col.names=c(
                    "year",     
                    "survey",     
                    "date",     
                    "wetland_code",     
                    "module_code",     
                    "tc_mc_code",     
                    "habitat_code",     
                    "seine_section_code",     
                    "seine_label",     
                    "seine_sample_width",     
                    "seine_sample_length",     
                    "seine_sample_area",     
                    "depth",     
                    "haul_no",     
                    "species_id",     
                    "species_code",     
                    "genus_name",     
                    "species_name",     
                    "count"    ), check.names=TRUE)
               
unlink(infile2)
		    
# Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings
                
if (class(dt2$survey)!="factor") dt2$survey<- as.factor(dt2$survey)
if (class(dt2$date)!="factor") dt2$date<- as.factor(dt2$date)
if (class(dt2$wetland_code)!="factor") dt2$wetland_code<- as.factor(dt2$wetland_code)
if (class(dt2$module_code)!="factor") dt2$module_code<- as.factor(dt2$module_code)
if (class(dt2$tc_mc_code)!="factor") dt2$tc_mc_code<- as.factor(dt2$tc_mc_code)
if (class(dt2$habitat_code)!="factor") dt2$habitat_code<- as.factor(dt2$habitat_code)
if (class(dt2$seine_section_code)!="factor") dt2$seine_section_code<- as.factor(dt2$seine_section_code)
if (class(dt2$seine_label)!="factor") dt2$seine_label<- as.factor(dt2$seine_label)
if (class(dt2$seine_sample_width)=="factor") dt2$seine_sample_width <-as.numeric(levels(dt2$seine_sample_width))[as.integer(dt2$seine_sample_width) ]               
if (class(dt2$seine_sample_width)=="character") dt2$seine_sample_width <-as.numeric(dt2$seine_sample_width)
if (class(dt2$seine_sample_length)=="factor") dt2$seine_sample_length <-as.numeric(levels(dt2$seine_sample_length))[as.integer(dt2$seine_sample_length) ]               
if (class(dt2$seine_sample_length)=="character") dt2$seine_sample_length <-as.numeric(dt2$seine_sample_length)
if (class(dt2$seine_sample_area)=="factor") dt2$seine_sample_area <-as.numeric(levels(dt2$seine_sample_area))[as.integer(dt2$seine_sample_area) ]               
if (class(dt2$seine_sample_area)=="character") dt2$seine_sample_area <-as.numeric(dt2$seine_sample_area)
if (class(dt2$depth)=="factor") dt2$depth <-as.numeric(levels(dt2$depth))[as.integer(dt2$depth) ]               
if (class(dt2$depth)=="character") dt2$depth <-as.numeric(dt2$depth)
if (class(dt2$haul_no)!="factor") dt2$haul_no<- as.factor(dt2$haul_no)
if (class(dt2$species_id)!="factor") dt2$species_id<- as.factor(dt2$species_id)
if (class(dt2$species_code)!="factor") dt2$species_code<- as.factor(dt2$species_code)
if (class(dt2$genus_name)!="factor") dt2$genus_name<- as.factor(dt2$genus_name)
if (class(dt2$species_name)!="factor") dt2$species_name<- as.factor(dt2$species_name)
if (class(dt2$count)=="factor") dt2$count <-as.numeric(levels(dt2$count))[as.integer(dt2$count) ]               
if (class(dt2$count)=="character") dt2$count <-as.numeric(dt2$count)
                
# Convert Missing Values to NA for non-dates
                
dt2$survey <- as.factor(ifelse((trimws(as.character(dt2$survey))==trimws("-99999")),NA,as.character(dt2$survey)))
dt2$date <- as.factor(ifelse((trimws(as.character(dt2$date))==trimws("-99999")),NA,as.character(dt2$date)))
dt2$wetland_code <- as.factor(ifelse((trimws(as.character(dt2$wetland_code))==trimws("-99999")),NA,as.character(dt2$wetland_code)))
dt2$module_code <- as.factor(ifelse((trimws(as.character(dt2$module_code))==trimws("-99999")),NA,as.character(dt2$module_code)))
dt2$tc_mc_code <- as.factor(ifelse((trimws(as.character(dt2$tc_mc_code))==trimws("-99999")),NA,as.character(dt2$tc_mc_code)))
dt2$habitat_code <- as.factor(ifelse((trimws(as.character(dt2$habitat_code))==trimws("-99999")),NA,as.character(dt2$habitat_code)))
dt2$seine_section_code <- as.factor(ifelse((trimws(as.character(dt2$seine_section_code))==trimws("-99999")),NA,as.character(dt2$seine_section_code)))
dt2$seine_label <- as.factor(ifelse((trimws(as.character(dt2$seine_label))==trimws("-99999")),NA,as.character(dt2$seine_label)))
dt2$seine_sample_width <- ifelse((trimws(as.character(dt2$seine_sample_width))==trimws("-99999")),NA,dt2$seine_sample_width)               
suppressWarnings(dt2$seine_sample_width <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt2$seine_sample_width))==as.character(as.numeric("-99999"))),NA,dt2$seine_sample_width))
dt2$seine_sample_length <- ifelse((trimws(as.character(dt2$seine_sample_length))==trimws("-99999")),NA,dt2$seine_sample_length)               
suppressWarnings(dt2$seine_sample_length <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt2$seine_sample_length))==as.character(as.numeric("-99999"))),NA,dt2$seine_sample_length))
dt2$seine_sample_area <- ifelse((trimws(as.character(dt2$seine_sample_area))==trimws("-99999")),NA,dt2$seine_sample_area)               
suppressWarnings(dt2$seine_sample_area <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt2$seine_sample_area))==as.character(as.numeric("-99999"))),NA,dt2$seine_sample_area))
dt2$depth <- ifelse((trimws(as.character(dt2$depth))==trimws("-99999")),NA,dt2$depth)               
suppressWarnings(dt2$depth <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt2$depth))==as.character(as.numeric("-99999"))),NA,dt2$depth))
dt2$haul_no <- as.factor(ifelse((trimws(as.character(dt2$haul_no))==trimws("-99999")),NA,as.character(dt2$haul_no)))
dt2$species_id <- as.factor(ifelse((trimws(as.character(dt2$species_id))==trimws("-99999")),NA,as.character(dt2$species_id)))
dt2$species_code <- as.factor(ifelse((trimws(as.character(dt2$species_code))==trimws("-99999")),NA,as.character(dt2$species_code)))
dt2$genus_name <- as.factor(ifelse((trimws(as.character(dt2$genus_name))==trimws("-99999")),NA,as.character(dt2$genus_name)))
dt2$species_name <- as.factor(ifelse((trimws(as.character(dt2$species_name))==trimws("-99999")),NA,as.character(dt2$species_name)))
dt2$count <- ifelse((trimws(as.character(dt2$count))==trimws("-99999")),NA,dt2$count)               
suppressWarnings(dt2$count <- ifelse(!is.na(as.numeric("-99999")) & (trimws(as.character(dt2$count))==as.character(as.numeric("-99999"))),NA,dt2$count))


# Here is the structure of the input data frame:
str(dt2) 
attach(dt2)                            
# The analyses below are basic descriptions of the variables. After testing, they should be replaced.                 

summary(year)
summary(survey)
summary(date)
summary(wetland_code)
summary(module_code)
summary(tc_mc_code)
summary(habitat_code)
summary(seine_section_code)
summary(seine_label)
summary(seine_sample_width)
summary(seine_sample_length)
summary(seine_sample_area)
summary(depth)
summary(haul_no)
summary(species_id)
summary(species_code)
summary(genus_name)
summary(species_name)
summary(count) 
                # Get more details on character variables
                 
summary(as.factor(dt2$survey)) 
summary(as.factor(dt2$date)) 
summary(as.factor(dt2$wetland_code)) 
summary(as.factor(dt2$module_code)) 
summary(as.factor(dt2$tc_mc_code)) 
summary(as.factor(dt2$habitat_code)) 
summary(as.factor(dt2$seine_section_code)) 
summary(as.factor(dt2$seine_label)) 
summary(as.factor(dt2$haul_no)) 
summary(as.factor(dt2$species_id)) 
summary(as.factor(dt2$species_code)) 
summary(as.factor(dt2$genus_name)) 
summary(as.factor(dt2$species_name))
detach(dt2)               
        
### end pre-written code from ERI website ###

view(dt2)

dt2 %>% 
  filter(wetland_code == "CSM") %>% 
  group_by(date) %>% 
  summarize(count = sum(count)) %>% 
  view()

dt2 %>% 
  filter(wetland_code == "CSM", species_code == "URHA") %>%
  group_by(date) %>% 
  summarize(count = sum(count)) %>% 
  ggplot(aes(x = date, y = count)) +
    geom_point()
```

## Dataframe without 2019
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year != 2019)
```

=============================================================================

# Analyses for Thesis

## Summary statistics
```{r}
nrow(daily_3yr_dat)
#351 total survey days

daily_3yr_dat %>% 
  filter(year == 2019) %>% 
  slice(-c(2:(n()-1)))
#2019 started on 5/30 and ended on 12/6:
as.period(mdy("5/30/2019") %--% mdy("12/6/2019"), "days")
#2019 field season was 190 days long
190/7 #roughly 27 weeks
98/(190/7) #3.6 surveys/week

daily_3yr_dat %>% 
  filter(year == 2020) %>% 
  slice(-c(2:(n()-1)))
#2020 started on 6/24 and ended on 12/11:
as.period(mdy("6/24/2020") %--% mdy("12/11/2020"), "days")
#2020 field season was 170 days long
170/7 #roughly 24 weeks
113/(170/7) #4.7 surveys/week

daily_3yr_dat %>% 
  filter(year == 2021) %>% 
  slice(-c(2:(n()-1)))
#2021 started on 4/20 and ended on 12/18:
as.period(mdy("4/20/2021") %--% mdy("12/18/2021"), "days")
#2021 field season was 242 days long
242/7 #roughly 35 weeks
140/(242/7) #4.0 surveys/week
  
sum(daily_3yr_dat$total_unique, na.rm = TRUE)
#912 total sightings

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(surveys = n(), total_sightings = sum(total_unique, na.rm = TRUE), mean_count = mean(total_unique, na.rm = TRUE))
#2019:     98 surveys, 44 total sharks, 0.449 sharks/survey
#2020:    113          271              2.42 
#2021:    140          597              4.36 

year_aov <- aov(total_unique ~ year, data = daily_3yr_dat)
summary(year_aov)
TukeyHSD(year_aov)

daily_3yr_dat %>%
  filter(year == 2019) %>% 
  summarize(mean_area = mean(transect_area_km2), mean_duration = mean(transect_duration_min))

daily_3yr_dat %>%
  filter(year != 2019) %>% 
  summarize(trans_area = mean(transect_area_km2, na.rm = TRUE), trans_duration = mean(transect_duration_min, na.rm = TRUE), man_area = mean(manual_area_km2, na.rm = TRUE), man_duration = mean(manual_duration_min, na.rm = TRUE))

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  nrow()
#203 days w/ at least one shark
203/351

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2019) %>% 
  nrow()
#17 detection days in 2019
17/98

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2020) %>% 
  nrow()
#70 detection days in 2020
70/113

daily_3yr_dat %>% 
  filter(total_unique > 0) %>% 
  filter(year == 2021) %>% 
  nrow()
#116 detection days in 2021
116/140

chisq.test(daily_3yr_dat$year, daily_3yr_dat$detection)

daily_3yr_dat %>% 
  filter(year == 2019) %>% 
  summarize(max = max(total_unique))
# max 6

daily_3yr_dat %>% 
  filter(year == 2020) %>% 
  summarize(max = max(total_unique, na.rm = TRUE))
# max 12

daily_3yr_dat %>% 
  filter(year == 2021) %>% 
  summarize(max = max(total_unique, na.rm = TRUE))
#max 15
```

## Rudimentary power/sensitivity analysis - do weekly averages change when only half of the data is used?
```{r}
pwr_dat <- dat_2021 %>% 
  select(datetime, total_unique) %>% 
  mutate(every_other = ifelse(row_number() %% 2 == 1, total_unique, NA)) %>% 
  group_by(week(datetime)) %>% 
  summarize(mean_all = mean(total_unique, na.rm = TRUE),
            sd_all = sd(total_unique, na.rm = TRUE),
            mean_eo = mean(every_other, na.rm = TRUE),
            sd_eo = sd(every_other, na.rm = TRUE)) %>% 
  rename(week = 1)
#not sure sd is relevant here, especially for eo data where there are only two or three observations per week

mean(pwr_dat$mean_all)
#4.23 when all data used

mean(pwr_dat$mean_eo, na.rm = TRUE)
#4.31 when every other survey used

t.test(pwr_dat$mean_all, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")

wilcox.test(pwr_dat$mean_all, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")

pwr_dat %>%
  pivot_longer(c(mean_all,mean_eo), names_to = "Data", values_to = "weekly_mean") %>% 
  ggplot(aes(x = week, y = weekly_mean, fill = Data)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(rows = vars(Data)) +
    theme_clean() +
    labs(x = "Week", y = "Weekly Mean") +
    theme(strip.text.y = element_blank()) +
    scale_fill_manual(labels = c("Every other survey", "All data"),
                      values = c("darkgreen", "orange3"))
```

## Investigating different metrics - raw count (each method and "unique") and density
```{r}
dat_no_19 %>%
  group_by(year(date_hour)) %>% 
  summarize("Unique count" = mean(total_unique, na.rm = TRUE),
            #"Maximum unique count" = max(total_unique, na.rm = TRUE),
            "Manual count" = mean(manual_total, na.rm = TRUE),
            #"Maximum manual count" = max(manual_total, na.rm = TRUE),
            "Transect count" = mean(transect_total, na.rm = TRUE),
            #"Maximum transect count" = max(transect_total, na.rm = TRUE),
            "Unique density" = mean(total_unique/(manual_area_m2 + transect_area_m2), na.rm = TRUE),
            #"Maximum unique density" = max(total_unique/(manual_area_km2 + transect_area_km2), na.rm = TRUE),
            "Manual density" = mean(manual_unique/manual_area_m2, na.rm = TRUE),
            #"Maximum manual density" = max(manual_unique/manual_area_km2, na.rm = TRUE),
            "Transect density" = mean(transect_total/transect_area_m2, na.rm = TRUE),
            #"Maximum transect density" = max(transect_total/transect_area_km2, na.rm = TRUE)
  )
```
* Looks like manual count was higher than transect count in 2020, but not 2021. Suggests that more sharks were close to shore (under the inner transect) in 2021 than in 2020

```{r}
ggplot(dat_no_19, aes(x = manual_unique, y = transect_total)) +
  geom_jitter() +
  geom_smooth(method = "lm")
#outliers here are potentially a big problem for only using one method in analyses

ggplot(daily_3yr_dat, aes(x = transect_total)) +
  geom_bar(stat = "count")

ggplot(daily_3yr_dat, aes(x = manual_unique)) +
  geom_bar(stat = "count")

t_vs_m.lm <- lm(transect_total ~ manual_unique, data = dat_no_19)
summary(t_vs_m.lm)
#slope = 0.60 - one shark sighted in manual = 0.6 sharks sighted in transect
#r2 = 0.2797

ggplot(dat_no_19, aes(x = manual_unique, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_m.lm <- lm(total_unique ~ manual_unique, data = dat_no_19)
summary(u_vs_m.lm)
#slope = 1.16
#r2 = 0.6746

ggplot(dat_no_19, aes(x = transect_total, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_t.lm <- lm(total_unique ~ transect_total, data = dat_no_19)
summary(u_vs_t.lm)
#slope = 1.07
#r2 = 0.76
```
*best correlation is between TRANSECT count and total count (but these numbers are obviously not independent)

```{r}
mean(dat_no_19$transect_total, na.rm = TRUE)
mean(dat_no_19$manual_unique, na.rm = TRUE)

ggplot(dat_no_19, aes(x = transect_total-manual_unique)) +
  geom_histogram(stat = "count")
```
* negative x-axis value means more sharks seen in manual - to me this is good evidence that we should use manual count: it is simply seeing more sharks

### Comparing methods
```{r}
mean(dat_no_19$transect_total/dat_no_19$transect_area_km2, na.rm = TRUE)
mean(dat_no_19$manual_unique/dat_no_19$manual_area_km2, na.rm = TRUE)
#transect actually has higher density

method_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  mutate(manual = manual_unique/manual_area_km2) %>% 
  mutate(transect = transect_total/transect_area_km2) %>% 
  pivot_longer(c(manual, transect), names_to = "method",
                                    values_to = "density")
  
ggplot(method_dat, aes(x = density)) +
  geom_histogram(binwidth = 10) +
  facet_grid(~method)
#normal distribution obviously not going to work (for ANOVA or GLM)
```

```{r}
dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  filter(manual_unique != 0) %>% 
  nrow()
#46 days where transect saw nothing and manual saw something

dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  summarize(mean = mean(manual_unique, na.rm = TRUE))
#when no sharks are detected in transect, manual detects 0.898

dat_no_19 %>% 
  filter(manual_unique == 0) %>%
  filter(transect_total != 0) %>% 
  nrow()
#14 days where manual saw nothing and transect saw something

dat_no_19 %>% 
  filter(manual_unique == 0) %>% 
  summarize(mean = mean(transect_total, na.rm = TRUE))
#when no sharks are detected in manual, transect detects 0.539
```
*This is further support for using manual - but transect densities are going to have to be used anyways for comparing across all three years?

### GLM approach to comparing methods (halting work on this while I try a paired-samples Wilcoxon, see next chunk. I don't think I'll be using GLM for this)
```{r}
method_glm_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  select(transect_area_km2, manual_area_km2,
         transect_total, manual_unique, date) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_area_km2, manual_area_km2), names_to = "area_method",
                                    values_to = "area") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0)
#selects every 1st and 4th row (remainder 1 and remainder 0)

method_glm_pois <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = "poisson")

met_res <- simulateResiduals(method_glm_pois)
plot(met_res)
#poisson no bueno (overdispersed)

method_glm_nb2 <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = nbinom2)

met_res <- simulateResiduals(method_glm_nb2)
plot(met_res)

testQuantiles(met_res)
# nb2 better, but not great

method_glm_nb1 <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = nbinom1)

met_res <- simulateResiduals(method_glm_nb1)
plot(met_res)
#nb1 worse, going to try a ZI with nbinom2:

method_glm_zi <- glmmTMB(count ~ method + offset(log(area)) + (1|year),
                         ziformula = ~1, data = method_glm_dat,
                         family = nbinom2, na.action = "na.fail")

met_res <- simulateResiduals(method_glm_zi)
plot(met_res)

dredge(method_glm_zi)
#best model doesn't even include method
```

### Wilcoxon signed-rank test on counts
```{r}
paired_dat <- method_glm_dat %>% 
  select(date, method, count) %>% 
  pivot_wider(names_from = method, values_from = count) %>%
  mutate(diff = manual - transect)
nrow(paired_dat)

ggplot(paired_dat, aes(x = diff)) +
    geom_bar()
#paired differences actually look pretty normally distributed...

shapiro.test(paired_dat$diff)
#jk, not normal
  
transect_counts <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  select(count)
nrow(transect_counts)

manual_counts <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  select(count)
nrow(manual_counts)

# explicitly testing the hypothesis that manual count is higher than transect count:
method_test <- wilcox.test(manual_counts$count, transect_counts$count, paired = TRUE, alternative = "greater")
method_test
#more likely to see more sharks in the manual survey than in the transect survey (p<0.01)
```

### Calculating effect size (basically just % of days where manual saw more, transect saw more, or they were the same)
```{r}
# number of days with no difference:
paired_dat %>%
  filter(diff == 0) %>% 
  nrow()
# 84 days

84/nrow(paired_dat)
# 38% of days

paired_dat %>%
  filter(transect == 0) %>% 
  filter(manual == 0) %>% 
  nrow()
# 61 days where BOTH were zero

61/84
# 72.6% of "manual = transect" days were days where no sharks were seen!

#number of days where manual saw more:
paired_dat %>%
  filter(diff > 0) %>% 
  nrow()
# 84 days (weird), 38%

#number of days where transect saw more:
paired_dat %>%
  filter(diff < 0) %>% 
  nrow()
# 53 days

53/nrow(paired_dat)
# 24% of days

mean(manual_counts$count)
#average manual count is 2.28

mean(transect_counts$count)
# average transect count is 1.93
```
* Manual survey count was significantly more likely to be higher than transect survey count (p < 0.01), with manual count being higher than transect count for 38% of survey days, lower than transect count for 24% of survey days, and equal to transect count for 38% of survey days. Of the 84 days where manual and transect count were equal, 72.6% (61 days) were days where no sharks were seen.

### Wilcoxon signed-rank test on density
```{r}
paired_dens_dat <- method_glm_dat %>%
  mutate(density = count/area) %>% 
  select(date, method, density) %>% 
  pivot_wider(names_from = method, values_from = density) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_dens_dat$diff)
#not normal

transect_dens <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(transect_dens)

manual_dens <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(manual_dens)

# do methods differ in density (2-sided, n = 221):
method_dens_test <- wilcox.test(manual_dens$density, transect_dens$density, paired = TRUE)
method_dens_test
# NOT more likely to see a higher density of sharks (count per unit area surveyed) in one survey method vs the other

mean(manual_dens$density)
# average manual density is 11.27 sharks/km2

mean(transect_dens$density)
# average transect density is 11.25 sharks/km2
```

### Comparing rates (count per unit time)
```{r}
method_rate_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_duration_min)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_duration_min)) %>%
  select(transect_total, manual_unique, date, transect_duration_min, manual_duration_min) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_duration_min, manual_duration_min), names_to = "time_method",
                                    values_to = "duration") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0) %>% 
  #mutate(year = as.factor(year(date))) %>% 
  mutate(method = as.factor(method))

paired_rate_dat <- method_rate_dat %>%
  mutate(rate = count/duration) %>% 
  select(date, method, rate) %>% 
  pivot_wider(names_from = method, values_from = rate) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_rate_dat$diff)
#not normal

transect_rate <- method_rate_dat %>% 
  filter(method == "transect") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(transect_rate)

manual_rate <- method_rate_dat %>% 
  filter(method == "manual") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(manual_rate)

# do methods differ in rate (2-sided, n = 221):
method_rate_test <- wilcox.test(manual_rate$rate, transect_rate$rate, paired = TRUE)
method_rate_test
# Not more likely for manual or transect to have a higher rate

mean(manual_rate$rate)
#average manual rate is 0.205 sharks/min
View(manual_counts)
mean(transect_rate$rate)
# average transect rate is 0.185 sharks/km2
```

### Visualizing manual vs transect metrics - data prep
```{r}
manual_counts <- mutate(manual_counts, value = count*10, method = "manual", metric = "count")
transect_counts <- mutate(transect_counts, value = count*10, method = "transect", metric = "count")
manual_dens <- mutate(manual_dens, value = density, method = "manual", metric = "density")
transect_dens <- mutate(transect_dens, value = density, method = "transect", metric = "density")
manual_rate <- mutate(manual_rate, value = rate*100, method = "manual", metric = "rate")
transect_rate <- mutate(transect_rate, value = rate*100, method = "transect", metric = "rate")

comp_vis_dat <- bind_rows(manual_counts, transect_counts, manual_dens, transect_dens, manual_rate, transect_rate)

View(comp_vis_dat)
```

### Manual vs Transect 3 metric comparison plot (for thesis):
```{r}
method_comp_plot_final <- ggplot(comp_vis_dat, aes(x = metric, y = value)) +
  geom_boxplot(aes(color = method)) +
  theme_cowplot() +
  scale_color_brewer(palette = "Dark2") +
  geom_point(aes(y = 139, shape = metric)) +
  scale_shape_manual(values = c(8, NA, NA)) + #significance asterisk
  guides(shape = "none") + 
  theme(panel.background = element_rect(fill = "white", colour = NA),
        plot.background = element_rect(fill = "white", colour = NA)) +
  labs(x = "Metric", y = "Value", color = "Method")

method_comp_plot_final
ggsave("plots/method_comp_plot_final.png")
# count is scaled up by a factor of 10 and rate is scaled up by a factor of 100. Only sig diff is count.
```

### Misadventures in plotting:
```{r}
#also will try to plot rate as minutes per shark to avoid the 100x:

manual_time <- mutate(manual_rate, value = 10/rate, method = "manual", metric = "time")
transect_time <- mutate(transect_rate, value = 10/rate, method = "transect", metric = "time")

comp_vis_time <- bind_rows(manual_counts, transect_counts, manual_dens, transect_dens, manual_time, transect_time)

#count is 10x and "time" is minutes per shark x10 (which is not intuitive at all - I;m definitely not going to use this
ggplot(comp_vis_time, aes(x = metric, y = value)) +
  geom_boxplot(aes(color = method))

# now let's try standardizing by the manual metric:
manual_counts_stan <- mutate(manual_counts, value = count, method = "manual", metric = "count")
transect_counts_stan <- mutate(transect_counts, value = count, method = "transect", metric = "count")

counts_stan <- bind_cols(manual_counts_stan, transect_counts_stan)
counts_stan <- mutate(counts_stan, manual = 1, transect = value...8/value...4)
counts_stan <- pivot_longer(counts_stan, 9:10)

manual_dens_stan <- mutate(manual_dens, value = density, method = "manual", metric = "density")
transect_dens_stan <- mutate(transect_dens, value = density, method = "transect", metric = "density")

density_stan <- bind_cols(manual_dens_stan, transect_dens_stan)
density_stan <- mutate(density_stan, manual = 1, transect = value...8/value...4)
density_stan <- pivot_longer(density_stan, 9:10)

count_and_dens_stan_dat <- bind_rows(counts_stan, density_stan)
View(count_and_dens_stan_dat)

ggplot(count_and_dens_stan_dat, aes(x = metric...3, y = value...10)) +
  geom_boxplot(aes(color = name))
#stupid idea lol
```

## Patterns of Abundance

### What are the average and range of transect detections?
```{r}
daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize("Total sightings" = sum(transect_total, na.rm = TRUE),
            "Survey days" = n(),
            "Average density" = 1000000*sum(transect_total/transect_area_m2, na.rm = TRUE)/n(),
            "Maximum abundance" = max(transect_total, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Average density" ~ px(80)) %>% 
    cols_align(align = "center")
```

### Do detections vary across years? (GLM) - EVERYTHING FROM HERE TO "SIZE" IS REDUNDANT WITH THE GLOBAL GAM STUFF BELOW
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = year, y = total_unique)) +
    geom_violin()

ggplot(daily_3yr_dat, aes(x = total_unique)) +
  geom_bar()
#very zero-inflated

ggplot(daily_3yr_dat, aes(x = log1p(total_unique))) +
  geom_bar()
#going to need poisson

year_poisson_mod <- glm(total_unique ~ year, family = "poisson", data = daily_3yr_dat)

summary(year_poisson_mod)
dispersiontest(year_poisson_mod)
ypm_res <- simulateResiduals(year_poisson_mod)
plot(ypm_res)
#definitely overdispersed

year_qpois_mod <- glm(total_unique ~ year, family = "quasipoisson", data = daily_3yr_dat)

summary(year_qpois_mod)

year_nb_mod <- glmmTMB(total_unique ~ year, family = nbinom2, data = daily_3yr_dat)

summary(year_nb_mod)
ynm_res <- simulateResiduals(year_nb_mod)
plot(ynm_res)
#dispersion still significant

year_nb1_mod <- glmmTMB(total_unique ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_nb1_mod)
yn1m_res <- simulateResiduals(year_nb1_mod)
plot(yn1m_res)
#Negative binomial 1 checks out

pairs(emmeans(year_nb1_mod, ~year))

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE))

year_trans_nb1_mod <- glmmTMB(transect_total ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_trans_nb1_mod)
ytn1m_res <- simulateResiduals(year_trans_nb1_mod)
plot(ytn1m_res)

pairs(emmeans(year_trans_nb1_mod, ~year))
```
* based on GLM with ONLY year, total unique sightings increased each year from 2019 to 2021 (p < 0.0001)
* based on GLM with ONLY year, transect sightings increased each year from 2019 to 2021 (p < 0.0005)


### Manual sightings only
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = year, y = manual_unique)) +
    geom_violin()

year_manual_mod <- glmmTMB(manual_unique ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_manual_mod)
ymm_res <- simulateResiduals(year_manual_mod)
plot(ymm_res)
#Negative binomial 1 checks out

pairs(emmeans(year_manual_mod, ~year))

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(mean = mean(manual_unique, na.rm = TRUE))
```
* based on GLM with ONLY year, manual unique sightings were higher in 2021 than 2020 (p < 0.005)

### Do detections vary across the field season?
```{r}
season_lm <- lm((1000000*transect_total/transect_area_m2) ~ day, data = daily_3yr_dat)
summary(season_lm)
#no apparent linear trend

season2_lm <- lm((1000000*transect_total/transect_area_m2) ~ day + day2, data = daily_3yr_dat)
summary(season2_lm)
#no quadratic trend (not sure this is a meaningful test anyways)
```

### Adding "day" to the year model
```{r}
daily_3yr_narm <- daily_3yr_dat %>%
  filter(transect_area_m2 > 10) %>% 
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  rowwise() %>% 
  mutate(total_area_km2 = sum(transect_area_km2, manual_area_km2, na.rm = TRUE)) %>% 
  mutate(total_dens = total_unique/total_area_km2) %>% 
  filter(!is.na(total_unique)) %>% 
  filter(!is.na(total_area_km2)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour)))
  
season_glm <- glmmTMB(transect_total ~ year*day + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(season_glm)

dredge(season_glm)
#day*year interaction model is best candidate - but I don't have any replication for many days due to non-overlapping survey window

sm_res <- simulateResiduals(season_glm)
plot(sm_res)
#minor issue with quantiles. Don't think it is a problem based on what I've read about that test
```
* Best model to explain transect sightings includes day, year, and their interaction.

### Alternative approach to comparing years ("day" plus looking at survey-related variables like time of day, area surveyed, etc.)
```{r}
year_glm <- glmmTMB(transect_total ~ year*hour + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")
#use log(area) because negative binomial models the log of the mean

summary(year_glm)

yr_res <- simulateResiduals(year_glm)
plot(yr_res)

dredge(year_glm)
#year*hour w/out area is best, but I'm not sure that the interaction between year and hour is meaningful (no reason a priori to think that the effect of time of day would depend on year)...
```

### Final modeling procedure - full survey window, transect sightings only
```{r}
year_glm <- glmmTMB(transect_total ~ year + day + hour + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(year_glm)

yr_res <- simulateResiduals(year_glm)
plot(yr_res)
##minor issue with quantiles in global model (not when day is included). Don't think it is a problem based on what I've read about that test

dredge(year_glm)
#best model to explain transect sightings includes year, day, and hour of survey, but not area surveyed - if this were a predictive model, I would keep area in, but since it doesn't contribute additional explanatory power to the patterns that I already saw, leaving it out will give us a better understanding of the effects of year, day, and hour:

year_glm_best <- glmmTMB(transect_total ~ year + day + hour, family = nbinom1, data = daily_3yr_narm, na.action = "na.fail") #log link

summary(year_glm_best)

#diagnose best model:
yr_bst_res <- simulateResiduals(year_glm_best)
plot(yr_bst_res)
#passes DHARMa checks

Anova(year_glm)
year_emms <- emmeans(year_glm, "year")
pwpm(year_emms, type = "response")

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(density = mean(transect_total/transect_area_km2))
```
* Best model is year + day + hour with no offset for area surveyed
* adding area surveyed increases AIC (transect area has very low variation, so it makes sense that adding it as a predictor doesn't contribute much info)

### Effects testing of final "year" GLM - full survey window, transect sightings only
```{r}
Anova(year_glm_best)
# year, day, and hour have significant effects on sightings (which we already know from the model selection process)

year_emms <- emmeans(year_glm_best, "year")
pwpm(year_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison

#compare to ANOVA (sanity check):
year_aov <- aov(transect_total ~ year, data = daily_3yr_narm)
summary(year_aov)
TukeyHSD(year_aov, which = "year")
#sightings differ significantly btwn 2021 and 2020/2019, but not between 2019 and 2020 (although p-value is lower than in the GLM because time of survey is not being taken into account in the anova)

emmeans(year_glm_best, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average sightings = 1.12 at that time based on the model?)

emmeans(year_glm_best, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average sightings = 1.12 at that time based on the model?)

allEffects(year_glm_best)
plot(allEffects(year_glm_best))
#nice little plot

emmip(year_glm_best, hour ~ year)
emmip(year_glm_best, day ~ year)

daily_3yr_narm %>% 
  filter(year == 2019) %>% 
  summarize(mean = mean(transect_total))
# 2019 mean = 0.54321

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(transect_total))
# 2020 mean = 1.17143

daily_3yr_narm %>% 
  filter(year == 2021) %>% 
  summarize(mean = mean(transect_total))
# 2021 mean = 2.82171
```
* 2019-2020 contrast isn't significant in either the simple (year-only) AOV or in the year + day + hour glm. This suggests that other factors are driving increased sightings from 2019 to 2020. We know that time of day and day of year explain sightings, so we should test if these factors vary significantly between years - if they do, they may be driving the difference in shark sightings between 2019-2020

### Are time of day and day of year different between years?
```{r}
time_btwn_yrs_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(time_btwn_yrs_aov)
TukeyHSD(time_btwn_yrs_aov)
#2019 time of day significantly earlier (by about 2.5 hrs)

day_btwn_yrs_aov <- aov(day ~ year, data = daily_3yr_narm)
summary(day_btwn_yrs_aov)
TukeyHSD(day_btwn_yrs_aov)
#day of year not significantly different between 2019 and 2020, but 2021 was significantly earlier than 2019 (24 days) and 2020 (41 days)
```

### Nice timeseries plot where each year is overlaid
```{r}
daily_3yr_dat %>% 
  filter(!is.na(total_unique)) %>% 
  ggplot(aes(x = day, y = rollmean(total_unique, 14, na.pad = TRUE, align = "right"))) +
    geom_line(aes(color = year), size = 1) +
    scale_color_brewer(palette = "Dark2") +
  labs(x = "Month", y = "Count", color = "Year") +
  scale_x_continuous(
  breaks = lubridate::yday(seq(as.Date("2019-01-01"), 
                               by = "1 month", length.out = 12)), 
  labels = month.abb) +
  scale_y_continuous(breaks = c(0,2,4,6,8,10)) +
  theme_cowplot() +
  theme(panel.background = element_rect(fill = "white", colour = NA),
      plot.background = element_rect(fill = "white", colour = NA))

ggsave("plots/abund_years_overlaid.png", width = 7, height = 4)

daily_3yr_dat %>% 
  filter(!is.na(total_unique)) %>%
  filter(ifelse(year == 2019, !is.na(transect_area_km2), !is.na(transect_area_km2+manual_area_km2))) %>%
  mutate(total_area = ifelse(year == 2019, transect_area_km2, transect_area_km2+manual_area_km2)) %>% 
  ggplot(aes(x = day, y = rollmean(total_unique/total_area, 14, na.pad = TRUE, align = "right"))) +
    geom_line(aes(color = year), size = 1) +
    scale_color_brewer(palette = "Dark2") +
  labs(x = "Month", y = "Density (sharks/km2)", color = "Year") +
  scale_x_continuous(
  breaks = lubridate::yday(seq(as.Date("2019-01-01"), 
                               by = "1 month", length.out = 12)), 
  labels = month.abb) +
  scale_y_continuous(breaks = c(0,5,10,15,20,25)) +
  theme_cowplot() +
  theme(panel.background = element_rect(fill = "white", colour = NA),
      plot.background = element_rect(fill = "white", colour = NA))

ggsave("plots/dens_years_overlaid.png", width = 7, height = 4)
```

### Final modeling procedure - full survey window, all sightings (work on this started 4/3/22)
```{r}
year_total_glm <- glmmTMB(total_unique ~ year + day + hour + offset(log(total_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")
#log(area) is used because nbinom1 uses a log link, so total_unique is on the log scale

summary(year_total_glm)

yr_res <- simulateResiduals(year_total_glm)
plot(yr_res)
##minor issue with quantiles in global model. Don't think it is a problem based on what I've read about that test

dredge(year_total_glm)

#like with transect-only, best model to explain total sightings includes year, day, and hour of survey, but not area surveyed - if this were a predictive model, I would keep area in, but since it doesn't contribute additional explanatory power to the patterns that I already saw, leaving it out will give us a better understanding of the effects of year, day, and hour:

year_total_glm_best <- glmmTMB(total_unique ~ year + day + hour, family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(year_total_glm_best)

#diagnose best model:
yr_tot_bst_res <- simulateResiduals(year_total_glm_best)
plot(yr_tot_bst_res)
#passes DHARMa checks
```

### Effects testing of full model (4/3)
```{r}
Anova(year_total_glm)
# year, day, and hour have significant effects on density (which we already know from the model selection process)

year_tot_emms <- emmeans(year_total_glm, "year")
pwpm(year_tot_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison
#like transect count, 2019-2020 contrast is not significant for total density

#compare to ANOVA (sanity check):
year_total_aov <- aov(total_unique ~ year, data = daily_3yr_narm)
summary(year_total_aov)
TukeyHSD(year_total_aov, which = "year")
#sightings differ significantly between all years

emmeans(year_total_glm, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average density = 2.02 at that time based on the model?)

emmeans(year_total_glm, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average density = 2.02 at that time based on the model?)

allEffects(year_total_glm)
plot(allEffects(year_total_glm))

emmip(year_total_glm, hour ~ year)
emmip(year_total_glm, day ~ year)
#idk what these are doing, they look exactly the same

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_dens))
# 2019 mean = 3.53
# 2020 mean = 5.12
# 2021 mean = 12.8

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(total_dens))
# no idea why this stopped working (also stopped working in transect-only chunk above)
```
* Best model is year + day + hour with no offset for area surveyed
* adding area surveyed increases AIC

### Effects testing of final "year" GLM - full survey window, all sightings
```{r}
Anova(year_total_glm_best)
# year, day, and hour have significant effects on sightings (which we already know from the model selection process)

year_total_emms <- emmeans(year_total_glm_best, "year")
pwpm(year_total_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison
#UNLIKE transect only, 2019-2020 contrast is highly significant

#compare to ANOVA (sanity check):
year_total_aov <- aov(total_unique ~ year, data = daily_3yr_narm)
summary(year_total_aov)
TukeyHSD(year_total_aov, which = "year")
#sightings differ significantly between all years

emmeans(year_total_glm_best, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average sightings = 1.66 at that time based on the model?)

emmeans(year_total_glm_best, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average sightings = 1.66 at that time based on the model?)

allEffects(year_total_glm_best)
plot(allEffects(year_total_glm_best))
#nice little plot, biggest difference from transect-only is that 2020 is now significantly higher than 2021

emmip(year_total_glm_best, hour ~ year)
emmip(year_total_glm_best, day ~ year)
#idk what these are doing, they look exactly the same

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_unique))
# 2019 mean = 0.543
# 2020 mean = 2.4
# 2021 mean = 4.32

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(total_unique))
# no idea why this stopped working (also stopped working in transect-only chunk above)
```

### Comparing survey windows
```{r}
nrow(dat_2019)
first(dat_2019$date)
last(dat_2019$date)
#107 survey days in 2019 from May 30 to November 22

nrow(dat_2020)
first(dat_2020$date)
last(dat_2020$date)
#135 survey days in 2020 from May 20 to December 11

nrow(dat_2021)
first(dat_2021$date)
last(dat_2021$date)
#142 survey days in 2021 from April 20 to December 18
```
* I definitely *could* do analyses with only the overlapping window but since day is included I think it makes more sense to keep all of the data that I have

### Copying above approach but with GAM - probably won't use GAM for year analysis
```{r}
gam_dat <- daily_3yr_dat %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(year)) %>% 
  mutate(transect_total = as.numeric(transect_total)) %>% 
  mutate(year = as.factor(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour)))

year_gam <- mgcv::gam(transect_total ~ year + s(hour) + s(day) + offset(transect_area_km2), data = gam_dat)

summary(year_gam)
# edf for "hour" = 1, so it's being modeled as linear.

mgcv::gam.check(year_gam)
plot(year_gam, residuals = TRUE, pch = 1)

year_lm <- lm(transect_total ~ year, data = gam_dat)
anova(year_lm, year_gam)

hour_gam <- mgcv::gam(transect_total ~ s(hour), data = gam_dat)
summary(hour_gam)
mgcv::gam.check(hour_gam)
plot(hour_gam, residuals = TRUE, pch = 1)
```

### Year is categorical so it doesn't make sense to use GAM unless hour or day has a non-linear effect on count:
```{r}
ggplot(gam_dat, aes(x = hour, y = total_unique))+
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = day, y = transect_total))+
  geom_jitter() +
  geom_smooth()
```
* no strong evidence that I should favor a GAM here

### Do manual counts vary across the field season?
```{r}
#adding day to the year model:
daily_3yr_narm_manual <- daily_3yr_dat %>% 
  filter(!is.na(manual_unique))
season_glm_man <- glmmTMB(manual_unique ~ year*day, family = nbinom1, data = daily_3yr_narm_manual, na.action = "na.fail")
summary(season_glm_man)

dredge(season_glm_man)
# day + year, day*year, and day-only are all equally weighted for manual count model
```

## Size 

### first off, a justification of ASL and Depth corrections:
```{r}
BOI_1_sub_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_submerged_JP_calculations.csv")
View(BOI_1_sub_dat)

BOI_1_sub_dat$test_alt <- as.factor(as.character(BOI_1_sub_dat$test_alt))

BOI_1_sub_dat %>%
  pivot_longer(cols = c(5:7), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = 1.97, linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comp_submerged.png", width = 10)
#visually apparent that at 20m, correcting for both ASL and depth get us the closest when target is submerged 1.5 m

BOI_1_surface_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_surface_JP_calculations.csv")
View(BOI_1_surface_dat)

BOI_1_surface_dat$test_alt <- as.factor(as.character(BOI_1_surface_dat$test_alt))

BOI_1_surface_dat %>%
  pivot_longer(cols = c(5:8), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = c(1.97, 2.93), linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comps_surface.png", width = 10)
#visually apparent that ASL correction gets us closer to true size at 20m (no depth correction for surface target)
```

### determining accuracy and error margins from above tests:
```{r}
sub_dat_20 <- BOI_1_sub_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(sub_dat_20$size_asl_plus_depth)
mean_size <- mean(sub_dat_20$size_asl_plus_depth)
std_err <- sd(sub_dat_20$size_asl_plus_depth)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01136 m

#95% ci:
mean_size - margin_of_error #1.925 m
mean_size + margin_of_error #1.948 m

known_size_pvc <- 1.97

sub_dat_20 %>% 
  mutate(error = size_asl_plus_depth - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average size estimate of pvc target at 1.5m depth was 0.0335 m too small 

sub_dat_20 %>% 
  mutate(error = (size_asl_plus_depth - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for submerged target is 1.702% with an sd of 1.545% - this is pretty good!

sub_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average UNADJUSTED size estimate of pvc target at 1.5m depth was 0.252 m too small!

sub_dat_20 %>% 
  mutate(error = (size_pvc - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for UNADJUSTED submerged target is 12.799% with an sd of 1.357%
# adjustments reduce mean error by over 11%!

surf_dat_20 <- BOI_1_surface_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(surf_dat_20$size_pvc_asl)
mean_size <- mean(surf_dat_20$size_pvc_asl)
std_err <- sd(surf_dat_20$size_pvc_asl)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01032 m

#95% ci:
mean_size - margin_of_error #1.989 m
mean_size + margin_of_error #2.009 m

known_size_pvc <- 1.97

surf_dat_20 %>% 
  mutate(error = size_pvc_asl - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#size estimate of pvc target at surface was 0.029 m too large on average

surf_dat_20 %>% 
  mutate(error = (size_pvc_asl - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
#mean error is 1.466%, with an sd of 1.403%

surf_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#UNADJUSTED size estimate of pvc target at surface was 0.0725 m too big on average
```
* I have SE, margin of error, confidence intervals, and average error - not sure which of these metrics is best to report

### average size of each depth category
```{r}
size_dat_3yr %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj > 0) %>% 
  summarize(mean_surface = mean(length_adj[depth..JP. == "surface"]),
            mean_shallow = mean(length_adj[depth..JP. == "shallow"]),
            mean_deep = mean(length_adj[depth..JP. == "deep"]))

#significant difference?

depth_aov_dat <- size_dat_2021 %>% 
                  filter(unique == "Y") %>%
                  filter(!is.na(length_adj)) %>% 
                  filter(length_adj > 1) %>% 
                  filter(depth..JP. == "surface" | depth..JP. == "shallow" | depth..JP. == "deep")

shapiro.test(log(depth_aov_dat$length_adj))
#def not normal but log fixes it

depth_aov <- aov(log(length_adj) ~ depth..JP., data = depth_aov_dat)

summary(depth_aov)
plot(depth_aov)

TukeyHSD(depth_aov)
```
*mean sizes not significantly different between depth categories

### histograms of each depth category
```{r}
size_dat_2021 %>% 
  filter(depth == "surface") %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>% 
  nrow()
#only 13 unique surface sharks

size_dat_2021 %>% 
  filter(!is.na(depth)) %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj)) %>%
  nrow()
#out of 291 unique manual sightings

size_dat_2021 %>% 
  filter(length_adj_m > 0) %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = length_adj)) +
    geom_histogram() +
    geom_vline(xintercept = 9.8, lty = 2, size = 1, color = "red") +
    geom_vline(xintercept = 8, lty = 2, size = 1, color = "blue") +
    theme_bw() +
    scale_x_continuous(breaks = c(4,6,8,10,12,14,16,18,20)) +
    labs(x = "Length (ft)", y = "Count")
ggsave("plots/2021_size_histo.png", height = 5, width = 7)

size_dat_3yr %>% 
  filter(length_adj_m > 0) %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = length_adj_m)) +
    geom_histogram() +
    geom_vline(xintercept = 3.5, lty = 2, size = 1, color = "red") +
    geom_vline(xintercept = 3, lty = 2, size = 1, color = "blue") +
    theme_bw() +
    scale_x_continuous(breaks = c(1,2,3,4,5)) +
    labs(x = "Length (meters)", y = "Count")
ggsave("plots/3yr_size_histo.png", height = 5, width = 7)

daily_3yr_dat %>%
  summarize(juveniles = sum(juvenile), adults = sum(adult))
#330 juveniles and 173 adults

size_dat_2021 %>% 
  filter(depth..JP. == "surface") %>% 
  ggplot(aes(x = length_raw_m)) +
    geom_histogram()
#this isn't going to be very helpful for comparing between depths

size_dat_2021 %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), position = "dodge") +
    facet_wrap(~depth..JP.)
#this stinks
```

### How do adjustments affect size distribution?
```{r}
size_dat_3yr%>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj_m > 1) %>%
  mutate(year = year(as_date(date, format = '%m/%d/%Y'))) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "Measure", values_to = "Length") %>% 
  ggplot(aes(x = Length)) +
    geom_histogram(aes(fill = Measure, color = Measure), alpha = 0.6, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    scale_color_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    theme_cowplot() +
    theme(legend.position = "bottom", legend.spacing.x = unit(1, 'cm'),
          panel.background = element_rect(fill = "white", colour = NA),
          plot.background = element_rect(fill = "white", colour = NA)) +
    guides(fill = guide_legend(label.position = "bottom")) +
    facet_grid(rows = vars(year)) +
    theme_clean() +
    #theme(strip.text.y = element_blank()) +
    labs(x = "Length (m)", y = "Count", fill = "Measure:", color = "Measure:")
ggsave("plots/3yr_size_adjustment_comp.png", height = 7, width = 4.6)
 
size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj_m > 1) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "Measure", values_to = "Length") %>% 
  ggplot(aes(x = Length)) +
    geom_histogram(aes(fill = Measure, color = Measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    scale_color_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    theme_cowplot() +
    theme(legend.position = "bottom", legend.spacing.x = unit(1, 'cm'),
          panel.background = element_rect(fill = "white", colour = NA),
          plot.background = element_rect(fill = "white", colour = NA)) +
    guides(fill = guide_legend(label.position = "bottom")) +
    labs(x = "Length (m)", y = "Count", fill = "Measure:", color = "Measure:")
ggsave("plots/2021_size_adjustment_comp.png", height = 7, width = 4.6)
#shows how peaks in raw get shifted to the right when the adjustment is made. looks cool when narrow. Probably a supplementary figure

size_dat_3yr %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj_m)) %>% 
  filter(!is.na(length_raw_m)) %>%
  filter(length_adj_m > 1) %>%
  summarize(median_adj = median(length_adj_m), median_raw = median(length_raw_m))
2.786541-2.529871
#increase in median size of 0.257 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>%
  filter(length_adj_m > 1) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average adjustment was an increase of 0.19 meters

#looking at how adjustments affect juvie:adult ratio with the 3m cutoff
size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>%
  filter(length_adj_m > 1) %>%
  mutate(class_raw = ifelse(length_raw_m < 3, "Juvenile", "Adult")) %>% 
  mutate(class_adj = ifelse(length_adj_m < 3, "Juvenile", "Adult")) %>% 
  count(class_adj)
#113 adults and 411 juveniles with raw measurements, 183 adults and 341 juveniles with adjusted measurements - adjustments bring 70 sharks from juvenile to adult length

#looking at how adjustments affect shallow sharks (overlapping histograms):
size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth..JP. == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure, color = measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
# I actually like this dodged the most, shows how peaks in raw get shifted to the right

# same as above but faceted:
size_dat_3yr %>% 
  filter(depth..JP. == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), binwidth = 0.1) +
    facet_grid(measure~.) +
    scale_fill_brewer(palette = "Dark2")
# not sure what this really tells me. Overall smoother distribution for the adjusted lengths, which I suppose makes sense. Adjusted lengths are shifted to the right...

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>% 
  summarize(median_adj = median(length_adj_m), median_raw = median(length_raw_m))
#by approximately 0.3 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average shallow adjustment was an increase of 0.33 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "surface") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average surface adjustment was an increase of 0.13 meters

size_dat_3yr %>% 
  filter(unique == "Y") %>%
  filter(depth == "deep") %>% 
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>% 
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average deep adjustment was increase of 0.47 meters
```

### Size classes
```{r}
ggplot(data = daily_3yr_dat, aes(x = juvenile/total_sized)) +
  geom_histogram()

ggplot(data = daily_3yr_dat, aes(x = date, y = juvenile/total_sized)) +
  geom_point()

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  ggplot(aes(x = year, y = juvenile/total_sized)) +
    geom_boxplot()
#not sure why 2020 looks weird 

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  filter(year == "2020") %>% 
  ggplot(aes(x = juvenile/total_sized)) +
    geom_bar()

daily_3yr_dat %>% 
  filter(total_sized != 0) %>% 
  ggplot(aes(x = juvenile/total_sized, fill = year)) +
    geom_histogram(position = "dodge", bins = 10)
#2021 has a LOT of days where there were only adult-sized sharks, and 2020 has tons of days where only juvenile-sized sharks were seen
```

## Modelling effects of environmental and detection-related variables on transect density

### Build the dataset to be used in the GAM
```{r}
gam_dat <- daily_3yr_dat %>% 
  filter(!is.na(transect_total)) %>%
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month_fac = as.factor(month)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(Salinity < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>%
  filter(!is.na(scripps_temp)) %>%
  filter(!is.na(scripps_temp_1wk)) %>%
  filter(!is.na(scripps_temp_2wk)) %>%
  filter(!is.na(scripps_temp_3wk)) %>%
  filter(!is.na(scripps_temp_4wk)) %>%
  filter(!is.na(cabrillo_temp)) %>% 
  filter(!is.na(cabrillo_temp_1wk)) %>%
  filter(!is.na(cabrillo_temp_2wk)) %>%
  filter(!is.na(cabrillo_temp_3wk)) %>%
  filter(!is.na(cabrillo_temp_4wk)) %>%
  filter(!is.na(tide)) %>% 
  mutate(presence = ifelse(total_unique > 0, 1, 0))

#view(gam_dat)
#idk how to get rid of NAs and "NA"s

gam_dat2 <- daily_3yr_dat %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month_fac = as.factor(month)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>%
  mutate(presence = ifelse(total_unique > 0, 1, 0))

gam_dat3 <- daily_3yr_dat %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month_fac = as.factor(month)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>%
  mutate(presence = ifelse(total_unique > 0, 1, 0)) %>% 
  filter(!is.na(cabrillo_temp_3wk)) %>%
  filter(!is.na(scripps_temp_3wk)) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(vis)) %>% 
  filter(!is.na(beaufort)) %>% 
  filter(!is.na(tide)) %>% 
  filter(!is.na(transect_total)) %>%
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) 
#this is to be used when dredging, as dredge() requires no na's in the dataset. Use dat2 for modelling, as n will increase from 209 to 213
```

### Correlations between continuous predictors (Rohner et al. 2013)
```{r}
cor.test(gam_dat2$scripps_temp, gam_dat2$Temp_top, method = "pearson")
#Scripps and carp temp are correlated at 0.49 (moderate correlation)

cor.test(gam_dat2$cabrillo_temp, gam_dat2$Temp_top, method = "pearson")
#Cabrillo and Carp temp are correlated at 0.19

cor.test(gam_dat2$scripps_temp_3wk, gam_dat2$Temp_top, method = "pearson")
# 3 week lagged Scripps and carp temp are correlated at 0.36

cor.test(gam_dat2$cabrillo_temp_3wk, gam_dat2$Temp_top, method = "pearson")
# 3 week lagged Cabrillo and Carp temp are correlated at 0.27

gam_dat2 %>% 
  select(c(18,22,28,52,55,78,83,85)) %>% 
  cor(use = "complete.obs")
#tide is correlated with month (0.45), LTER temp (0.29), channel temp (0.28), and hour (0.26)
#hour is correlated with tide, channel temp, month, and LTER temp
#Carp LTER temp is highly correlated with channel buoy temp (0.70), also with other temp locations

plot.new()
ggplot(gam_dat2, aes(x = month, y = tide)) +
  geom_point() +
  geom_smooth()

ggplot(gam_dat2, aes(x = tide, y = Temp_top)) +
  geom_point() +
  geom_smooth(method = "lm")
#temp increases with tide

ggplot(gam_dat2, aes(x = hour, y = tide)) +
  geom_point() +
  geom_smooth(method = "lm")
#higher tides later in the day?
```

### Temp and season correlation
```{r}
month_temp_aov <- aov(Temp_top~as_factor(month), gam_dat2)
summary(month_temp_aov)

TukeyHSD(month_temp_aov)

ggplot(aes(x = as_factor(month), y = Temp_top), data = gam_dat2) +
  geom_boxplot()

day_temp_lm <- lm(Temp_top~day, gam_dat2)
summary(day_temp_lm)

ggplot(aes(x = day, y = Temp_top), data = gam_dat2) +
  geom_point() +
  geom_smooth()
```

### Concurvity
```{r}
mgcv::concurvity(glob_gam,full=TRUE)
#all terms have high concurvity w/ the "rest of the model"

mgcv::concurvity(glob_gam,full=FALSE)
#Cabrillo x Carp = 0.234
#Scripps x Carp = 0.297
#Cabrillo x Scripps = 0.229
#Tide x WVHT is very high (0.9), WVHT is also high with all three temp variables (>0.4)
#Many environmental variables concurved (?) with the temporal ones but that is to be expected
```

### Trends with detectability variables
```{r}
ggplot(gam_dat, aes(x = as.numeric(beaufort), y = transect_total)) + 
  geom_point() +
  geom_smooth()

gam_dat %>% 
  group_by(beaufort) %>% 
  summarize(presence = mean(presence, na.rm = TRUE)) %>% 
  ggplot(aes(x = beaufort, y = presence)) +
    geom_bar(stat = "identity")
#unexpected INCREASE in presence as beaufort increases

ggplot(gam_dat, aes(x = as.numeric(vis), y = transect_total)) +
  geom_point() +
  geom_smooth()
```

### Trends with LTER variables
```{r}
ggplot(gam_dat2, aes(x = Temp_top, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = Temp_top, y = total_unique)) + 
  geom_jitter() +
  geom_smooth(method = "lm")

ggplot(gam_dat, aes(x = Salinity, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
#very tiny variation in salinity, this won't be useful

ggplot(gam_dat, aes(x = Fluoroescence, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
#I guess there are no valid fluorescence (chlorophyll) measurements
```

### Trends with tide and swell
```{r}
ggplot(gam_dat, aes(x = tide, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = WVHT, y = transect_total)) + 
  geom_jitter() +
  geom_smooth()
```
*not much going on with either of these

### transect total GAM, no interactions
```{r}
gam1 <- mgcv::gam(transect_total ~ year + 
                                   s(hour, bs = "cc", k = 10) +
                                   s(day, bs = "cc", k = 20) +
                                   beaufort + vis + 
                                   s(tide, bs = "cr", k = 10) +
                                   s(Temp_top, bs = "cr", k = 10) +
                                   s(WVHT, bs = "cr", k = 30) +
                                   offset(transect_area_km2),
                                   data = gam_dat,
                                   method = "REML",
                                   family = nb,
                                   na.action = "na.fail")
#REML used for small dataset (observations in the 100's), to prevent over-smoothing
#knots from gam2 below

summary(gam1)
# 37% of deviance explained

gam.check(gam1)
#negative binomial fixes qq plot (nb was also best for temporal GLM)

plot.gam(gam1)
#not sure why day is flat

visreg(gam1)

anova.gam(gam1)
#this is analogous to "drop1" function for GLM
#year, beaufort, hour, and wave height are significant predictors (matches dredge below)

dredge(gam1, m.min = 8)
#this takes a long time to run, especially if no limits are set on minimum number of terms
#global model has 9 terms, so m.min = 8 will return every model with just one term missing. Models with beaufort, hour, year, and wave height are all worse than global model, supporting the anova.gam result above and the full dredge result
#best model is beaufort, hour, waveheight, and year, but top 12 models (6 pairs with/without day) are all within delta of 2
#models with or without day have the same AIC, so something weird is happening
```

### transect total GAM, with year interactions
```{r}
gam2 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 10, by = year) +
                                   s(day, bs = "cc", k = 20, by = year) + 
                                   beaufort + vis + s(tide, by = year, k = 10) +
                                   s(Temp_top, by = year, k = 10) +
                                   s(WVHT, by = year, k = 30) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")
#knots hand-selected to be as large as possible without throwing 
#"more coefficients than data" error

summary(gam2)
#adding year interactions increases deviance explained from 37 to 64.3 percent
gam.check(gam2)
#issues with WVHT (k = 30 is too low)

#plot.gam(gam2)
visreg(gam2)

anova.gam(gam2)
#year and beaufort significant as parametric terms
#hour significant in all years but 2019
#day significant in all years
#tide n.s.
#temp n.s.
#WVHT n.s. (only difference from gam1)

dredge(gam2, m.min = 8)
# dropping hour, day, or year from global model make fit worse

dredge(gam2)

#wald_gam(gam2)
#report_stats(gam2)
#plot_diff2(gam2, comp = list(year = c('2019', '2020')), view = c('hour', 'tide'))
```

### interaction model but with month rather than day of year:
```{r}
gam3 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 10, by = year) +
                                   s(month, bs = "cc", k = 8, by = year) + 
                                   beaufort + vis + s(tide, by = year, k = 10) +
                                   s(Temp_top, by = year, k = 10) +
                                   s(WVHT, by = year, k = 40) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")

summary(gam3)
#month model explains 60.4% of deviance compared to 64% for day model

gam.check(gam3)
#month by year doesn't work
visreg(gam3)

anova.gam(gam3)
#year, beaufort, hour, and month significant
```

### "Local" month model with year interaction only for hour and month (no N and S temps)
```{r}
gam4 <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
#select = TRUE does automated term penalization to remove terms that are "infinitely smooth"

summary(gam4)
#55.2% of deviance explained

gam.check(gam4, rep = 500)
#month and wvht p < 0.05, but edf much lower than k'

anova.gam(gam4)
#p-values for smooth terms show that the nonlinear effect of the term is different from zero (a flat horizontal line)
#year, beaufort, vis all (marginally) significant parametric terms (see post-hoc tests below)
#month (all years) and hour (2020 and 2021) significant.
#tide, temp, and WVHT n.s.

visreg(gam4)
plot(gam4)

gam4_dredge_drop1 <- dredge(gam4, m.min = 8, fixed = "offset(transect_area_km2)")
gam4_dredge_drop1
#dropping tide, vis, waveheight, and temp all (marginally) improves fit over global model
#dropping beaufort, hour, month, and year all significantly worsen fit compared to global model

#gam4_full_dredge <- dredge(gam4, m.min = 3, fixed = "offset(transect_area_km2)")
gam4_full_dredge

#gam4_top3_dredge <- dredge(gam4, m.max = 4, fixed = "offset(transect_area_km2)")
gam4_top3_dredge

visreg(gam4)
visreg4_data <- visreg(gam4)

fits4 <- as.data.frame(visreg4_data[[3]][["fit"]][["visregFit"]])
days4 <- as.data.frame(visreg4_data[[3]][["fit"]][["month"]])

gam4_day_fits <- bind_cols(fits4, days4)
#main peak is July, secondary peak in May, and minimum is September/October

gam4_year_emms <- emmeans(gam4, "year")
pwpm(gam4_year_emms, type = "response")
#2019 lower than 2020 and 2021, but 2020 and 2021 not sig. diff:

ggplot(data = gam_dat, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

gam4_beaufort_emms <- emmeans(gam4, "beaufort")
pwpm(gam4_beaufort_emms, type = "response")
# no sig. contrasts

gam4_vis_emms <- emmeans(gam4, "vis")
pwpm(gam4_vis_emms, type = "response")
#2 marginally lower than 4 (p = 0.080)
```

### Model selection for time lag of Scripps temp dataset
```{r}
scripps0_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(scripps_temp, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps1_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(scripps_temp_1wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps2_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(scripps_temp_2wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps3_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(scripps_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps4_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(scripps_temp_4wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

AIC(scripps0_gam4, scripps1_gam4, scripps2_gam4, scripps3_gam4, scripps4_gam4)
#3-week lag lowest AIC
summary(scripps4_gam4)
#54.7, 56, 56.6, 56.6, and 54.1 DE, respectively

#three-week lag is best for Scripps Pier data

gam.check(scripps3_gam4)
visreg(scripps3_gam4)
```

### Model selection for Scripps temp dataset (local temp NOT included)
```{r}
scripps_0 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(scripps_temp, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps_1 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(scripps_temp_1wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps_2 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(scripps_temp_2wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps_3 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(scripps_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

scripps_4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(scripps_temp_4wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

MuMIn::AICc(scripps_0, scripps_1, scripps_2, scripps_3, scripps_4)
#1, 2, 3, and 4 are all very close

summary(scripps_4)
#54.5, 55.6, 56, 54.8, 53.8 DE

#pretty ambiguous here
```

### Model selection for Cabrillo Point temp dataset (local temp included)
```{r}
cabrillo0_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo1_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_1wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo2_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_2wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo3_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo4_gam4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_4wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

AIC(cabrillo0_gam4, cabrillo1_gam4, cabrillo2_gam4, cabrillo3_gam4, cabrillo4_gam4)
#3-week lag lowest AIC by far

summary(cabrillo4_gam4)
#60.4, 59, 57.3, 64.3, 53.2 DE

#3-week lag for Cabrillo Point has lowest AIC and highest DE
```

### Model selection for Cabrillo Point temp dataset (local temp NOT included)
```{r}
cabrillo_0 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo_1 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_1wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo_2 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_2wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo_3 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

cabrillo_4 <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_4wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)

MuMIn::AICc(cabrillo_0, cabrillo_1, cabrillo_2, cabrillo_3, cabrillo_4)
#3-week lag lowest AIC by far

summary(cabrillo_4)
#58.5, 57.4, 56.1, 63.1, 53.2 DE

#3-week lag for Cabrillo Point has lowest AIC and highest DE
```

### Regional GAM structure (both non-local temp datasets included)
```{r}
glob_gam <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_3wk, k = 12) +
                                   s(scripps_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb)
summary(glob_gam)
#69% Deviance Explained

gam.check(glob_gam, rep = 500)
#month p < 0.05, but edf not close to k' and k is set as high as possible

anova.gam(glob_gam)
#p-values for smooth terms show that the nonlinear effect of the term is different from zero (a flat horizontal line)
#year, beaufort, vis all significant parametric terms (see post-hoc tests below)
#month (all years) and hour (2020 and 2021) significant.
#Carp and Cabrillo temp significant, Scripps temp is not
#Waveheight is significant (P < 0.005)
#Tide n.s.

visreg(glob_gam)

visreg_data <- visreg(glob_gam)

fits <- as.data.frame(visreg_data[[3]][["fit"]][["visregFit"]])
days <- as.data.frame(visreg_data[[3]][["fit"]][["month"]])

gam4_day_fits <- bind_cols(fits4, days4)

plot(glob_gam)

glob_dredge_drop1 <- dredge(glob_gam, m.min = 10, fixed = "offset(transect_area_km2)")
glob_dredge_drop1
#model without Scripps has a weight of 0.101

AIC(gam4, scripps3_gam4, cabrillo3_gam4, glob_gam)

#glob_full_dredge <- dredge(glob_gam m.min = 3, fixed = "offset(transect_area_km2)")
glob_full_dredge

#glob_top3_dredge <- dredge(glob_gam, m.max = 4, fixed = "offset(transect_area_km2)")
glob_top3_dredge

glob_year_emms <- emmeans(glob_gam, "year")
pwpm(glob_year_emms, type = "response")
#2019 lower than 2020 and 2021, but 2020 and 2021 not sig. diff:

ggplot(data = gam_dat2, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

glob_beaufort_emms <- emmeans(glob_gam, "beaufort")
pwpm(glob_beaufort_emms, type = "response")
# 1 significantly lower than 0, no other significant contrasts

glob_vis_emms <- emmeans(glob_gam, "vis")
pwpm(glob_vis_emms, type = "response")
#2 marginally lower than 4 (ratio = 0.465, p = 0.080) and significantly lower than 5 (ratio = 0.206, p < 0.01)
```

### Comparing gam4 models (local vs regional)
```{r}
MuMIn::AICc(gam4, glob_gam, cabrillo3_gam4, scripps3_gam4, cabrillo_3, scripps_3)
#cabrillo+carp and just Cabrillo have lowest AIC

summary(gam4) #55.2% DE with just Carp temp - Cabrillo temp is definitely doing a lot of work
summary(cabrillo3_gam4) #64.3% DE with both Carp and Cabrillo temp
summary(cabrillo_3) #63.1% DE with just Cabrillo temp
```

### Global GAM, but with total unique rather than transect count (sensitivity analysis)
```{r}
glob_gam_tot <- mgcv::gam(total_unique ~ year +
                                   s(month, k = 8, by = year) +
                                   s(hour, k = 10, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(cabrillo_temp_3wk, k = 12) +
                                   s(scripps_temp_3wk, k = 12) +
                                   s(WVHT, k = 50) +
                                   offset(total_area),
                  data = gam_dat2, method = "REML", family = nb)
summary(glob_gam_tot)
#64.5% DE, n = 202
summary(glob_gam)
#69% DE, n = 208

anova.gam(glob_gam_tot)
anova.gam(glob_gam)
#same terms are significant when transect or total count is used

visreg(glob_gam_tot)
```

### determining deviance explained of each term by dropping it from the global local model - as of 5/13 
*note from Simon Wood: predictors aren't strictly orthogonal, so DE of each term might not sum to DE of global model
```{r}
summary(gam4)
#local global model explains 55.2% of deviance

gam4year <- mgcv::gam(transect_total ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4year)
55.2 - 47.2
#year DE = 8%

gam4hour <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4hour)
55.2 - 47.9
#hour DE = 7.3%

gam4month <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4month)
55.2 - 38
#month DE = 17.2%

gam4beau <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4beau)
55.2 - 50.6
#beaufort DE = 4.6%

gam4vis <- mgcv::gam(transect_total ~ year +
                                      s(hour, k = 10, by = year) + 
                                      s(month, k = 8, by = year) +
                                      beaufort +
                                      s(tide, k = 10) +
                                      s(Temp_top, k = 10) +
                                      s(WVHT, k = 50) +
                                      offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4vis)
55.2 - 48.2
#vis DE = 7

gam4tide <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4tide)
55.2 - 55.1
#tide DE = 0.1%

gam4temp <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4temp)
55.2 - 53.4
#temp DE = 1.8%

gam4wvht <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis +
                                       s(tide, k = 10) +
                                       s(Temp_top, k = 10) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(gam4wvht)
55.2 - 53.4
#wvht DE = 1.8%
```

### determining deviance explained of each term by dropping it from the global Cabrillo model - as of 5/13 
*note from Simon Wood: predictors aren't strictly orthogonal, so DE of each term might not sum to DE of global model
```{r}
summary(cabrillo_3)
#local global model explains 63.1% of deviance

anova(cabrillo_3)
#year, beaufort, vis, month (all years), hour (2020 & 2021), cabrillo temp and WVHT all significant

cabrillo3year <- mgcv::gam(transect_total ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3year)
63.1 - 50.5
#year DE = 12.6%

cabrillo3hour <- mgcv::gam(transect_total ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3hour)
63.1 - 57.5
#hour DE = 5.6%

cabrillo3month <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3month)
63.1 - 43.1
#month DE = 20%

cabrillo3beau <- mgcv::gam(transect_total ~ year +
                                            s(hour, k = 10, by = year) + 
                                            s(month, k = 8, by = year) +
                                            vis +
                                            s(tide, k = 10) +
                                            s(cabrillo_temp_3wk, k = 10) +
                                            s(WVHT, k = 50) +
                                            offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3beau)
63.1 - 55.5
#beaufort DE = 7.6%

cabrillo3vis <- mgcv::gam(transect_total ~ year +
                                      s(hour, k = 10, by = year) + 
                                      s(month, k = 8, by = year) +
                                      beaufort +
                                      s(tide, k = 10) +
                                      s(cabrillo_temp_3wk, k = 10) +
                                      s(WVHT, k = 50) +
                                      offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3vis)
63.1 - 53.2
#vis DE = 9.9

cabrillo3tide <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(cabrillo_temp_3wk, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3tide)
63.1 - 62.8
#tide DE = 0.1%

cabrillo3temp <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis+
                                       s(tide, k = 10) +
                                       s(WVHT, k = 50) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3temp)
63.1 - 53.4
#temp DE = 9.7%

cabrillo3wvht <- mgcv::gam(transect_total ~ year +
                                       s(hour, k = 10, by = year) + 
                                       s(month, k = 8, by = year) +
                                       beaufort +
                                       vis +
                                       s(tide, k = 10) +
                                       s(cabrillo_temp_3wk, k = 10) +
                                       offset(transect_area_km2),
                  data = gam_dat2, method = "REML", family = nb, select = TRUE)
summary(cabrillo3wvht)
63.1 - 58.7
#wvht DE = 4.4%
```

### month model, no interactions
```{r}
gam5 <- mgcv::gam(transect_total ~ year +
                                   s(hour, bs = "cc", k = 12) +
                                   s(month, bs = "cc", k = 8) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 20) +
                                   s(Temp_top, k = 100) +
                                   s(WVHT, k = 90) +
                                   offset(transect_area_km2), 
                    data = gam_dat, method = "REML", family = nb, na.action = "na.fail")
#knots hand-selected to be as large as possible

gam.check(gam5)
#issue with month

summary(gam5)
#only 38.1% of deviance explained

visreg(gam5)

anova.gam(gam5)
#year, beaufort, hour and wave height significant. Month and tide are being modeled as essentially linear

dredge(gam5, m.min = 8)
#dropping vis, temp, tide, month, and area all improve fit

gam5_full_dredge <- dredge(gam5, m.min = 3)
gam5_full_dredge
#tied-for-best models include beaufort, hour, wave height, and year, +/- month
```

### day model, year interactions with hour and day (like gam4 but with day)
```{r}
gam6 <- mgcv::gam(transect_total ~ year +
                                   s(hour, k = 10, by = year) +
                                   s(day, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(transect_area_km2), data = gam_dat, method = "REML", family = nb, na.action = "na.fail")

summary(gam6)
#61.2% of deviance explained
#hour and tide are being modeled as linear (edf close to 1)

gam.check(gam6, rep = 500)
#low p-value for waveheight and month, but edf not close to k' so it's not a problem with the number of knots (which are set to max anyways)

anova.gam(gam6)
#year, beaufort, hour, and day are significant

visreg(gam6)
visreg_data <- visreg(gam6)

fits <- as.data.frame(visreg_data[[3]][["fit"]][["visregFit"]])
days <- as.data.frame(visreg_data[[3]][["fit"]][["day"]])

gam6_day_fits <- bind_cols(fits, days)
#main peak is day 202 (July 21), secondary peak at day 124 (May 4), and minimum is day 275 (October 2)

#wald_gam(gam6, comp=list(beaufort=levels(gam_dat$beaufort), year=c("2019", "2020", "2021")))
#is this comparing for all years or just 2021?

gam6_year_emms <- emmeans(gam6, "year")
pwpm(gam6_year_emms, type = "response")
#2020 NOT sig. diff. from 2021 when all other factors are taken into account - but weird that 2020 estimates are higher:

ggplot(data = gam_dat, aes(x = year, y = transect_total/transect_area_km2)) +
  geom_boxplot()

gam6_beaufort_emms <- emmeans(gam6, "beaufort")
pwpm(gam6_beaufort_emms, type = "response")
#levels of beaufort not sig. diff. from each other even though beaufort is a significant parametric term?

#gam6_drop1_dredge <- dredge(gam6, m.min = 8)
gam6_drop1_dredge
#best 8-term model is the one without vis

#gam6_full_dredge <- dredge(gam6, m.min = 3, fixed = "offset(transect_area_km2)")
gam6_full_dredge
```

### comparing global models:
```{r}
MuMIn::AICc(gam1, gam2, gam3, gam4, gam5, gam6) 
# corrected AIC for small sample size

summary(gam3)#DE = 60.4%
summary(gam4)#DE = 60.7%
summary(gam5)#DE = 38.1%
summary(gam6) #DE = 61.2%

```
*gam4 and gam6 (year interaction with hour and month/day only) have lowest AIC and highest deviance explained. Differences in AIC and DE are marginal, so I should choose whichever does a better job of answering my questions

## Modelling proportion of juveniles

### Build the dataset
```{r}
prop_dat <- daily_3yr_dat %>%
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(total_sized)) %>%
  filter(total_sized > 0) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))
```

### Examine relationships
```{r}
ggplot(prop_dat, aes(x = as.numeric(beaufort), y = juvenile/total_sized)) + 
  geom_point() +
  geom_smooth()

ggplot(prop_dat, aes(x = as.numeric(vis), y = juvenile/total_sized)) +
  geom_point() +
  geom_smooth()

ggplot(prop_dat, aes(x = Temp_top, y = juvenile/total_sized)) + 
  geom_jitter() +
  geom_smooth() + 
  facet_grid(year~.)
```

### Modelling proportion of juveniles
```{r}
propgam1 <- mgcv::gam(juvenile ~ year +
                                 s(hour, k = 10, by = year) +
                                 s(month, k = 8, by = year) + 
                                 beaufort +
                                 vis +
                                 s(tide, k = 10) +
                                 s(Temp_top, k = 10) +
                                 s(WVHT, k = 50) +
                                 offset(total_sized), data = prop_dat,
                                 method = "REML", family = nb,
                                 na.action = "na.fail", select = TRUE)

summary(propgam1)
# only 11.9 % of deviance explained. almost everything is being modelled as a flat line

gam.check(propgam1)
#all good

visreg(propgam1)

anova(propgam1)
#month in 2021 is significant, hour in 2021 is marginal (p = 0.555)

#propgam1_dredge_drop_1 <- dredge(propgam1, m.min = 8, fixed = "offset(total_sized)")
propgam1_dredge_drop_1

#propgam1_dredge_full <- dredge(propgam1, m.min = 3, fixed = "offset(total_sized)")
propgam1_dredge_full
```

### determining deviance explained of each term by dropping it from the global model
```{r}
summary(propgam1)
#global model explains 11.9% of deviance

propgam1year <- mgcv::gam(juvenile ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1year)
11.9 - 10.6
#year DE = 1.3%

propgam1hour <- mgcv::gam(juvenile ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1hour)
11.9 - 8.36
#hour DE = 3.54% (not sure why it gave me an extra decimal place here)

propgam1month <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1month)
11.9 - 8.62
#month DE = 3.28%

propgam1beau <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1beau)
11.9 - 12.1
#beaufort DE = -0.2 !?!?!?!

propgam1vis <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1vis)
11.9 - 11.8
#vis DE = 0.1%

propgam1tide <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1tide)
11.9 - 10.9
#tide DE = 1.0%

propgam1temp <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(tide, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1temp)
11.9 - 11.9
#temp DE = 0%

propgam1wvht <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                       vis+
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   offset(total_sized),
                  data = prop_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(propgam1wvht)
11.9 - 11.9
#wvht DE = 0%
```

## Modelling density of juveniles

### Building the dataset
```{r}
juv_dat <- daily_3yr_dat %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))

juv_dat_all <- daily_3yr_dat %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>%
  mutate(month = as.factor(month)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000)
```

### Examine relationships
```{r}
ggplot(juv_dat, aes(x = as.numeric(beaufort), y = juvenile)) + 
  geom_point() +
  geom_smooth()

ggplot(juv_dat, aes(x = as.numeric(vis), y = juvenile)) +
  geom_point() +
  geom_smooth()

ggplot(juv_dat, aes(x = Temp_top, y = juvenile)) + 
  geom_jitter() +
  geom_smooth() + 
  facet_grid(year~.)
```

### Modelling density of juveniles
```{r}
juvgam1 <- mgcv::gam(juvenile ~ year +
                                s(hour, k = 10, by = year) +
                                s(month, k = 8, by = year) + 
                                beaufort +
                                vis +
                                s(tide, k = 10) +
                                s(Temp_top, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m), data = juv_dat,
                                method = "REML", family = nb, select = TRUE, na.action = "na.fail")

summary(juvgam1)
# 27.5% of deviance explained, n = 254

gam.check(juvgam1)
#issues with month

visreg(juvgam1)

anova(juvgam1)
#year, month in 2019 and 2021 are significant, temperature p = 0.068

juvgam1_dredge_drop_1 <- dredge(juvgam1, m.min = 8, fixed = "offset(area_20m)")
juvgam1_dredge_drop_1
# no vis and no beaufort are best two

#juvgam1_dredge_full <- dredge(juvgam1, m.min = 3, fixed = "offset(area_20m)")
juvgam1_dredge_full

juv_year_emms <- emmeans(juvgam1, "year")
pwpm(juv_year_emms, type = "response")
#2020 and 2021 significantly higher than 2019
```

### determining deviance explained of each term by dropping it from the global model
```{r}
summary(juvgam1)
#global model explains 27.5% of deviance

juvgam1year <- mgcv::gam(juvenile ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1year)
27.5 - 20.7
#year DE = 6.8%

juvgam1hour <- mgcv::gam(juvenile ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1hour)
27.5 - 25.6
#hour DE = 1.9%

juvgam1month <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1month)
27.5 - 19.4
#month DE = 8.1

juvgam1beau <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    vis +
                                    s(tide, k = 10) +
                                    s(Temp_top, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1beau)
27.5 - 26.1
#beaufort DE = 1.4

juvgam1vis <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1vis)
27.5 - 25.5
#vis DE = 2.0%

juvgam1tide <- mgcv::gam(juvenile ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1tide)
27.5 - 27.5
#tide DE = 0%

juvgam1temp <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(WVHT, k = 50) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1temp)
27.5 - 25.7
#temp DE = 1.8%

juvgam1wvht <- mgcv::gam(juvenile ~ year +
                                    s(hour, k = 10, by = year) + 
                                    s(month, k = 8, by = year) +
                                    beaufort +
                                    vis +
                                    s(tide, k = 10) +
                                    s(Temp_top, k = 10) +
                                    offset(area_20m),
                  data = juv_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(juvgam1wvht)
27.5 - 27.5
#wvht DE = 0%
```

## Modelling density of adults

### Building the dataset
```{r}
ad_dat <- daily_3yr_dat %>%
  filter(!is.na(area_20m)) %>% 
  filter(!is.na(adult)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))
```

### Modelling density of adults
```{r}
adgam1 <- mgcv::gam(adult ~ year +
                                 s(hour, k = 10, by = year) +
                                 s(month, k = 8, by = year) + 
                                 beaufort +
                                 vis +
                                 s(tide, k = 10) +
                                 s(Temp_top, k = 10) +
                                 s(WVHT, k = 50) +
                                 offset(area_20m), data = ad_dat,
                                 method = "REML", family = nb,
                                 na.action = "na.fail", select = TRUE)

summary(adgam1)
#38% DE, n = 254

gam.check(adgam1)

visreg(adgam1)

anova(adgam1)
#year, hour (2019 & 2021), month (2019 & 2021), and tide significant

#adgam1_dredge_drop_1 <- dredge(adgam1, m.min = 8, fixed = "offset(area_20m)")
adgam1_dredge_drop_1
#no beaufort and no vis are best (same as juvenile model)

#adgam1_dredge_full <- dredge(adgam1, m.min = 3, fixed = "offset(total_sized)")
adgam1_dredge_full
```

### determining deviance explained of each term by dropping it from the global model
```{r}
summary(adgam1)
#global model explains 38% of deviance

adgam1year <- mgcv::gam(adult ~ s(hour, k = 10, by = year) +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1year)
38 - 29.8
#year DE = 8.2%

adgam1hour <- mgcv::gam(adult ~ year +
                                   s(month, k = 8, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1hour)
38 - 33.8
#hour DE = 4.2

adgam1month <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1month)
38 - 32
#month DE = 6%

adgam1beau <- mgcv::gam(adult ~ year +
                                s(hour, k = 10, by = year) + 
                                s(month, k = 8, by = year) +
                                vis +
                                s(tide, k = 10) +
                                s(Temp_top, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1beau)
38 - 37.5
#beaufort DE = 0.5

adgam1vis <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                         s(month, k = 8, by = year) +
                                   beaufort +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1vis)
38 - 37.5
#vis DE = 0.5

adgam1tide <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(Temp_top, k = 10) +
                                   s(WVHT, k = 50) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1tide)
38 - 30.3
#tide DE = 7.7%

adgam1temp <- mgcv::gam(adult ~ year +
                                s(hour, k = 10, by = year) + 
                                s(month, k = 8, by = year) +
                                beaufort +
                                vis+
                                s(tide, k = 10) +
                                s(WVHT, k = 50) +
                                offset(area_20m),
                              data = ad_dat, method = "REML", family = nb,
                              na.action = "na.fail", select = TRUE)
summary(adgam1temp)
38 - 38.3
#temp DE = -0.3

adgam1wvht <- mgcv::gam(adult ~ year +
                                   s(hour, k = 10, by = year) + 
                                   s(month, k = 8, by = year) +
                                   beaufort +
                                   vis +
                                   s(tide, k = 10) +
                                   s(Temp_top, k = 10) +
                                   offset(area_20m),
                  data = ad_dat, method = "REML", family = nb, na.action = "na.fail", select = TRUE)
summary(adgam1wvht)
38 - 38.3
#wvht DE = -0.3%
```

## Modelling length of each shark
```{r}
length_dat <- daily_3yr_dat %>%
  filter(!is.na(total_area)) %>% 
  filter(!is.na(juvenile)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  filter(!is.na(beaufort)) %>% 
  mutate(beaufort = as.factor(beaufort)) %>% 
  filter(!is.na(vis)) %>% 
  mutate(vis = as.factor(vis)) %>% 
  filter(Temp_top < 9000) %>% 
  filter(!is.na(WVHT)) %>% 
  filter(!is.na(Temp_top)) %>% 
  filter(!is.na(tide))

```

===============================================================================

# Exploratory Visualizations
* Older individual year visualizations are below 3yr visualizations

## Time of day (using data for year GLM)
```{r}
ggplot(daily_3yr_narm, aes(x = hour, y = transect_total)) +
  geom_jitter() #+
  #geom_smooth(method = "lm")

hour_lm <- lm(transect_total ~ hour, data = daily_3yr_narm)
summary(hour_lm)
#see 0.3 more sharks every hour (p < 0.0001)

hour_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(hour_aov)

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean_time = mean(hour))
```

## 3yr abundance visualizations

Summary table
```{r}
yearly_abund_summary <- daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize("Total sightings" = sum(total_unique, na.rm = TRUE),
            "Survey days" = n(),
            "Sightings per day" = sum(total_unique, na.rm = TRUE)/n(),
            "Maximum abundance" = max(total_unique, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year(date)" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Sightings per day" ~ px(80)) %>% 
    cols_align(align = "center")
gtsave(yearly_abund_summary, "plots/yearly_abund_summary.png")
```

Years stacked (pretty ugly):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity", position = "stack") +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_stacked.png", width = 8, height = 5)
```

Years faceted (good option, potentially as a second graph after years averaged?):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity") +
    facet_grid(as_factor(year)~.) +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    theme(strip.background = element_blank(),
          strip.text.y = element_blank(),
          panel.spacing.y = unit(1, "lines"),
          panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_yrs_facet.png", width = 8, height = 5)
```

Years averaged:
```{r}
daily_3yr_dat %>% 
  group_by(week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)") %>%
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean)) +
    geom_bar(stat = "identity") +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Abundance") +
    theme_clean()
```

Rolling abundance:
WHAT LENGTH WINDOW DO I WANT?
```{r}
daily_3yr_dat %>%
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 10, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month")
ggsave("plots/3yr_rolling_abund_10day.png", width = 9, height = 5)
```

Rolling abundance, only including days within survey window for all three years:
```{r}
#find latest start and earliest stop dates:
daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize(min_day = min(yday(date)),
            max_day = max(yday(date)))
#2019 is latest start date (day 150) and 2021 is earliest stop date (day 312)

daily_3yr_dat %>%
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 7, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0), limits = c(0,6)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month",
                 limits = c(as_date(155), as_date(315)))
    
ggsave("plots/3yr_rolling_abund_bounded_7day.png", width = 9, height = 3)
```

## 3yr environmental variable visualizations
```{r}
daily_3yr_dat %>% 
  gather("data", "value", WDIR:WTMP) %>% 
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~data, scales = "free")

daily_3yr_dat %>%
 ggplot(aes(x = wharf_temp, y = WTMP)) +
  geom_point()

wharf_channel_temp_mod <- lm(WTMP ~ wharf_temp, data = daily_3yr_dat)
summary(wharf_channel_temp_mod)
# r2 = 0.43, p < 0.001

daily_3yr_dat %>%
 ggplot(aes(x = ((9/5)*(wharf_temp) + 32), y = water_temp_padaro)) +
  geom_point() +
  geom_smooth(method = "lm") +
  coord_cartesian(xlim = c(58, 70))

wharf_padaro_temp_mod <- lm(((9/5)*(wharf_temp) + 32) ~ water_temp_padaro, data = daily_3yr_dat)
summary(wharf_padaro_temp_mod)
#r2 = 0.69, p < 0.001
```

# Correlations of Abundance with Environmental Variables

### Channel Water Temp
```{r}
dat_no_19 %>% 
  ggplot(aes(x = WTMP)) +
    geom_histogram(binwidth = 0.5)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WTMP, y = total_unique)) +
    geom_point()

temp_mod <- lm(log1p(total_unique) ~ WTMP, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
# no linear relationship between channel water temp and total unique
plot(temp_mod)

temp_mod_transect <- 

```

### Wharf Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = wharf_temp)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

temp_mod <- lm(log(total_unique) ~ wharf_temp, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ wharf_temp, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod)

#no linear relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, wharf_temp2 = (wharf_temp)^2)

temp_mod2 <- lm(log(total_unique) ~ wharf_temp + wharf_temp2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ wharf_temp + wharf_temp2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Padaro Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = water_temp_bucket)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = log1p(total_unique))) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/bucket_lm_plot.png")

temp_mod <- lm(log(total_unique) ~ water_temp_bucket, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ water_temp_bucket, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod_0)

# significant linear relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, water_temp_bucket2 = (water_temp_bucket)^2)

temp_mod2 <- lm(log(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Swell
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = WVHT_m)) +
    geom_histogram(binwidth = 0.1)

dat_no_19 %>%
  ggplot(aes(x = total_unique)) +
    geom_histogram(binwidth = 1)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WVHT_m, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/wvht_lm_plot.png")

wvht_mod <- lm(log1p(total_unique) ~ log(WVHT_m), data = (dat_no_19))
summary(wvht_mod)
# highly significant correlation between wave height and abundance, r2 = 0.15
#slope is negative (see predictions below)
plot(wvht_mod)

shapiro.test(dat_no_19 %>% mutate(total_unique = log1p(total_unique)) %>% .$total_unique)
# not normal even w/ log transformation
dat_no_19 %>% 
  filter(total_unique > 0) %>% 
  #mutate(total_unique = log10(total_unique)) %>% 
  ggplot(aes(x = total_unique)) +
    geom_histogram()

shapiro.test(dat_no_19 %>%  mutate(WVHT_m = log(WVHT_m)) %>% .$WVHT_m)
# normal w/ log transformation

wvht_sim <- simulateResiduals(fittedModel = wvht_mod, plot = F, n = 1000)
testResiduals(wvht_sim, plot = T)
#looks good
predict(wvht_mod, data.frame(WVHT_m = 1))
# 0.5 m swell = 1.41 individuals, 1 m swell = 0.78, 1.5 m swell = 0.40, 2 m swell = 0.14

daily_3yr_dat %>% 
  ggplot(aes(x = MWD)) +
    geom_histogram(binwidth = 5)

dat_no_19 %>% 
  ggplot(aes(x = MWD, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

wvdir_mod <- lm(log1p(total_unique) ~ MWD, data = (dat_no_19))
summary(wvdir_mod)
```

### Other
```{r}
daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = pressure, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = salinity, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = chlorophyll, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
```


## 2019 Time Series of Transect Data with 8 ft. cutoff
```{r}
dat_2019 %>% 
  dplyr::slice(1:103) %>%
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>%
  filter(transect_total != 0) %>% 
  dplyr::select(Date, Small, Large) %>% 
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2019-05-30")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) +
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "1 month", date_labels = "%b %e",
                 limits = c(as.Date("2019-08-15"), as.Date("2019-12-06")))+
    scale_y_continuous(breaks = 1:7, limits = c(-0.1, 10)) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + 
    theme(plot.title = element_text(size=40), legend.text = element_text(size = 16),
          axis.text = element_text(size = 16)) +
    labs(title = "2019 Great White Shark Abundance, 5/30 - 11/22")
ggsave("plots/wsj_abund_plot_2019.png", width = 16, height = 9)
```

## Effort over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = manual_area_km2)) +
    geom_bar(stat = "identity")
```

## Abundance Over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = total_unique)) +
    geom_point() +
    geom_smooth()

dat_2020 %>% 
  dplyr::rename(Date = date, Small = small_unique, Large = large_unique) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "3 weeks", date_labels = "%b %e",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = 1:12) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + theme(plot.title = element_text(size = 40), legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - 12/18")

ggsave("plots/wsj_abund_plot.png", width = 16, height = 9)

#rolling average, right-aligned 7-day window, total transect density
dat_2020 %>%
  dplyr::rename(Date = date, Transects = transect_total) %>% 
  dplyr::select(Date, Transects) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Transects)) %>%
  ggplot(aes(x = Date, y = rollmean(Transects, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(color = "red3", size = 2) +
    scale_x_date(date_breaks = "2 weeks", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2.5, by = 0.5)) +
    theme_classic() +
    theme(legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1))
# 0.17 is survey area in km^2 and 9.6 is duration of survey in minutes

ggsave("plots/2020_abund_plot.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (used for GRFP, see updated version below)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-10-10")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 10/10/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))
ggsave("plots/grfp_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (all 2020 data)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 12/11/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))

ggsave("plots/2020_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, small sharks only
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  filter(Size == "Small") %>% 
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25))

#plotting time series of abundance by size, excluding days with no sharks
dat_2020 %>% 
  dplyr::filter(transect_total != 0) %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_point(aes(color = Size), size = 1.5) +
    geom_smooth(aes(color = Size), method = "lm", se = FALSE) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))
```

## Basic size class visualizations
```{r}
#View(size_classed_2020)

sum(size_classed_2020$juvenile)
#233 juvenile/YOY
sum(size_classed_2020$adult)
#29 adult/sub-adult

dat_2020_final %>% 
  gather("Sizeclass", "Abundance", juvenile:adult_density) %>% 
  ggplot(aes(x = Sizeclass, y = Abundance)) +
    geom_bar(stat = "identity")

sum(dat_2020_final$juvenile, na.rm = TRUE)/sum(dat_2020_final$adult, na.rm = TRUE)
# Eight times as many juveniles as adults

sum(dat_2020_final$juvenile_density, na.rm = TRUE)/sum(dat_2020_final$adult_density, na.rm = TRUE)
# Juveniles have 8.5 times the average density
```

## Size-classed time series (Abundance and Density)
```{r}
dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult), stat = "identity", fill = "red4") +
    labs(y = "Abundance", x = "Date")

dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile_density), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult_density), stat = "identity", fill = "red4") +
    labs(y = "Density (individuals/km2/min)", x = "Date")
```

## Morning vs afternoon abundance
```{r}
daily_3yr_dat %>%
  filter(hh < 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

daily_3yr_dat %>% 
  filter(hh >= 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

dat_2020 %>% 
  ggplot(aes(x = hms::as_hms(datetime), y = total_unique)) +
    geom_point()

dat_2020 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(bins = 6, color = "white")
ggsave("plots/2020_time_histogram.png", width = 7, height = 5)

dat_2019 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(binwidth = 1, color = "white")
ggsave("plots/2019_time_histogram.png", width = 7, height = 5)

View(dat_2021)

daily_3yr_dat %>%
  #filter(month(datetime) > 6, month(datetime) < 8) %>% 
  dplyr::group_by(hh) %>%
  drop_na(total_unique) %>%
  dplyr::summarize(Abundance = list(mean_se(total_unique))) %>%
  unnest(Abundance) %>% 
  dplyr::rename(Hour = 1, Abundance = y) %>% 
  ggplot(aes(x = Hour, y = Abundance)) +
    geom_bar(stat = "identity") +
    #geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 0.5, width = 0.2) +
    geom_vline(xintercept = 11.5, size = 1) +
    annotate("text", label = "Morning mean: 1.66 (n = 82)", x = 9, y = 3.5) +
    annotate("text", label = "Afternoon mean: 3.19 (n = 70)", x = 15, y = 5.2) +
    theme_classic()
ggsave("plots/3yr_am_vs_pm.png", width = 7, height = 5)
```

## Channel water temperature vs manual density, 2020
```{r}
flights_plus_channel_dat_2020 %>%
  #filter(manual_total_density != 0) %>% 
  ggplot(aes(x = mean_wtemp_previous_24, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

wtemp_quadratic <- lm(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2)

summary(wtemp_quadratic)

plot(dat_2020_full$mean_wtemp_previous_24, fitted(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2))
```

## 2020 Tide Visualizations
```{r}
tide_added_2020 %>% 
  ggplot(aes(x = tide.y)) +
    geom_histogram()

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = small_unique/total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
#huh

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_freqpoly()
#bimodal distribution of age structure

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_histogram(bins = 10)
```

## 2020 Size-Classed Visualizations
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(length)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    stat_function(fun = function(x) 
    dnorm(x, mean = mean(size_dat_2020$length), sd = sd(size_dat_2020$length)) * 0.2 * sum(!is.na(size_dat_2020$length)))

size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    geom_vline(xintercept = 3, size = 1.5, color = "red3", linetype = "dashed") +
    geom_vline(xintercept = mean(size_dat_2020$length_m, na.rm = TRUE),
               size = 1.5, color = "dodgerblue") +
    theme_classic() +
    scale_x_continuous(name = "Length (m)", breaks = seq(from = -1, to = 5, by = 0.5))
ggsave("plots/2020_size_histogram.png", width = 7, height = 6)

size_dat_2020 %>%
  drop_na(unique, size_class) %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = week(date), fill = size_class)) +
    geom_bar(position = "stack", stat = "count") +
    scale_fill_manual(values = c("red3", "dodgerblue")) +
    scale_x_continuous(name = "Week") +
    theme_classic()
ggsave("plots/2020_size_classed_time_series.png", width = 7, height = 4)
```


```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
Mode(size_dat_2020$length)

size_dat_2020 %>% 
  slice(1:201) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(1:201) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

dat_2020 %>% 
  slice(1:86) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/86)

dat_2020 %>% 
  slice(86:nrow(dat_2020)) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/((nrow(dat_2020))-86))
```

## Size over Time

### Individual years
```{r}
size_dat_2020 %>%
  drop_na(length) %>%
  filter(unique == "Y") %>% 
  dplyr::group_by(date) %>% 
  dplyr::summarize(mean_length = mean(length_m)) %>% 
  ggplot(aes(x = date, y = mean_length)) + 
    geom_point() +
    geom_smooth()

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = date, y = length_m)) + 
    geom_point() + 
    geom_smooth

#2021:
size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(length_adj_m > 0) %>% 
  ggplot(aes(x = mdy(date), y = length_adj_m)) + 
    geom_point() +
    geom_smooth(method = "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression.png", width = 5, height = 5)

size_lm_2021 <- lm(length_m ~ date, data = size_dat_2021)
summary(size_lm_2021)

#2021, quadratic:
size_dat_2021 <- mutate(size_dat_2021, date2 = (as.numeric(date))^2)

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = mdy(date), y = length_adj_m)) + 
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression_quad.png", width = 5, height = 5)

#need date as numeric or something for this to work
size_lm_2021_quad <- lm(length_adj_m ~ date + (date)^2, data = size_dat_2021)
summary(size_lm_2021_quad)
```

### Comparing size across years

Histogram of Daily Age Structure:
```{r}
size_classed_3yr %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#no 2019:
size_classed_3yr %>%
  dplyr::slice(-c(1:19)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2020 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:19, 71:144)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2021 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:70)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()
ggsave("plots/adult_swim.png")
#VERY pronounced bimodal distribution in 2021
#despite adults being just 40% of observations, there are MORE days where we saw only adults than where we saw only juveniles
#WHAT factors determine "adult swim" days???
```

Histogram faceted by year:
```{r}
size_dat_unique %>%
  filter(length_adj_m > 0) %>% 
  mutate(year = as_factor(year(mdy(date)))) %>%
  ggplot(aes(x = length_adj_m, fill = year)) +
    geom_histogram() +
    facet_grid(rows = vars(year)) +
    theme_clean() +
    labs(y = "Count", x = "Length (m)", fill = "Year") +
    theme(strip.text.y = element_blank()) +
    scale_fill_brewer(palette = "Dark2")
ggsave("plots/size_3yrs_histo.png", height = 5, width = 5)
```

looking at one month only:
```{r}
size_dat_unique %>% 
  filter(month(date) == 8) %>% 
  ggplot(aes(x = yday(date), y = length)) +
    geom_point() +
    facet_grid(cols = vars(year(date)))
```

Years overlaid:
```{r}
size_dat_unique %>%
  filter(length_adj_m > 0) %>% 
  mutate(day = as_date(yday(mdy(date)))) %>% 
  mutate(year = as_factor(year(mdy(date)))) %>% 
  ggplot(aes(x = day, y = length_adj_m)) +
    geom_point(aes(color = year), alpha = 0.8) +
    geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Dark2")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)
```

years overlaid, separate regressions:
```{r}
size_dat_unique %>% 
  ggplot(aes(x = as_date(yday(date)), y = length, color = as.factor(year))) +
    geom_point(alpha = 0.2) +
    geom_smooth(method= "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%b", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_scatter_regressions.png", height = 5, width = 5)

#years overlaid, windowed:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as_date(yday(date)), y = length)) +
    geom_point() +
    #geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)

#years separated, violin plot, July-September:
size_dat_unique %>% 
  filter(month(date) == c(7:9)) %>% 
  ggplot(aes(x = as.factor(year), y = length)) +
    geom_violin(fill = "dodgerblue") #+
    #geom_hline(yintercept = 3)
ggsave("plots/size_violin_july-sept.png")

#years separated, box plot:
size_dat_unique %>% 
  filter(length < 5) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    theme(panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box.png", width = 4, height = 5)

year_size_mod <- aov(length ~ as.factor(year(date)), data = size_dat_unique)
summary(year_size_mod)
plot(year_size_mod)
TukeyHSD(year_size_mod)
```

```{r}
#years separated, overlap window, box plot:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box_windowed.png", width = 4, height = 5)

#size summary table in FEET
size_dat_unique %>% 
  group_by(year) %>% 
  summarize(min = min(length_ft, na.rm = TRUE),
            max = max(length_ft, na.rm = TRUE),
            median = round(mean(length_ft, na.rm = TRUE), digits = 1),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = sum(size_class == "Juvenile", na.rm = TRUE)/
                                    sum(size_class == "Adult", na.rm = TRUE))

#size summary table in METERS, percentage adults
yearly_size_summary <- size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Percent Adults" = 
              sum(size_class == "Adult", na.rm = TRUE)/
                (sum(size_class == "Juvenile", na.rm = TRUE) +
                 sum(size_class == "Adult", na.rm = TRUE))) %>%
  gt() %>%
    fmt_number(2:4, decimals = 1) %>% 
    fmt_number(7, decimals = 2) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range") %>% 
    cols_width("Percent Adults" ~ px(100)) %>% 
    cols_align(align = "center")
yearly_size_summary
gtsave(yearly_size_summary, "plots/yearly_size_summary_table.png")

#size summary table in METERS, juveniles per adult
size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = 
              sum(size_class == "Juvenile", na.rm = TRUE)/
              sum(size_class == "Adult", na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(7, decimals = 1) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range")
```

## Haphazard vs Transect Visualizations
```{r}
dat_2020 %>% 
  filter(total_unique > 0) %>% 
  filter(!is.na(manual_total), !is.na(transect_total)) %>% 
  dplyr::rename(Date = date, Haphazard = manual_total, Transect = transect_total) %>% 
  dplyr::select(Date, Haphazard, Transect) %>%
  gather("Flight", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Flight = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Flight), stat = "identity", position = "dodge") +
    scale_x_date(date_breaks = "1 week", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-15")))+
    scale_y_continuous(breaks = 1:10) +
    scale_fill_manual(values = brewer.pal(8, "Dark2")[c(8,1,2)], "") + theme_wsj() + theme(plot.title = element_text(size=40), legend.text = element_text(size = 16), axis.text = element_text(size = 16)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - DD/MM")
ggsave("plots/wsj_survey_comp_plot.png", width = 16, height = 9)

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>% 
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>% 
  gather("Method", "Abundance", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Abundance = list(mean_se(Abundance))) %>%
  unnest(Mean_Abundance) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Abundance") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
      
ggsave("plots/3yr_methods_abundance.png", width = 7, height = 7)

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>% 
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>% 
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Density = list(mean_se(Density))) %>%
  unnest(Mean_Density) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
  
ggsave("plots/3yr_methods_density.png", width = 7, height = 7)

daily_3yr_dat %>%
  mutate(transect_frequency = transect_total/transect_duration_min,
         manual_frequency = manual_unique/manual_duration_min) %>% 
  select(transect_frequency, manual_frequency) %>%
  dplyr::rename(Transect = transect_frequency, Manual = manual_frequency) %>%
  gather("Method", "Frequency", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Freq = list(mean_se(Frequency))) %>%
  unnest(Mean_Freq) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Frequency (individuals/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))

# Density WITHOUT time:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
```

## Comparative Analysis of Methods
```{r}
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  t.test(Density ~ Method, .)
  
shapiro.test(daily_3yr_dat$transect_total_density) 
shapiro.test(daily_3yr_dat$manual_unique) 

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total_density)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_total_density)) +
    geom_density()

# Abundance
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual abundance (mean = 2.3, SE = 0.20) was higher than transect abundance (mean = 1.3, SE = 0.14, p < 0.001)

# Effort-Corrected Density:
daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>%
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>% 
  rename("Density" = y) %>% 
  ggplot(aes(x = Method, y = Density)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.4)
ggsave("plots/methods_comp_plot.png")

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual effort-corrected density (mean = 0.64, SE = 0.1) was higher than transect abundance (mean = 0.48, SE = 0.06, p < 0.001)

# Raw Density:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens)

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total/transect_area_km2)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_unique/manual_area_km2)) +
    geom_density()

dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)
## Manual density (mean = 7.12, SE = 1.05) was not higher than transect density (mean = 6.78, SE = 0.06, p < 0.001)
```

## Heatmaps

### All 3 years
```{r}
size_dat_3yr %>%
  filter(video_YN == "manual" | video_YN == "manual_1") %>% 
  filter(unique == "Y") %>%
  #filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point()
    #geom_smooth()
#follows a certain distance from the coastline

size_dat_3yr %>%
  filter(video_YN == "manual" | video_YN == "manual_1") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405), alpha = 0.5)
ggsave("plots/heatmap_3yrs.png", width = 10)

size_dat_3yr %>%
  filter(video_YN == "inner" | video_YN == "outer") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  dplyr::filter(lat > 0) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 15) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405))

View(size_dat_3yr)
```

### All Sharks, 2020
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point(aes(color = size_class))

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>%
  ggplot(aes(x = long, y = lat)) +
  geom_hex(bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
ggsave("grfp_hexmap.png")

register_google(key = "[AIzaSyDmz6xT3R938U6s5VWXuguMSq6ROr3X2KM]", write = TRUE)

location <- c(-120, 34, -119, 35)

carp <- get_map(location = location, source = "osm", zoom = 7)

ggmap(carp) +
  geom_hex(data = size_dat_2020, aes(x = long, y = lat), bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
```

### Juveniles, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Juvenile") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_juvenile_heatmap.png", width = 7, height = 6)
```

### Adults, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Adult") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_adult_heatmap.png", width = 7, height = 6)

mean(size_dat_2020$length_m, na.rm = TRUE)
median(size_dat_2020$length, na.rm = TRUE)

sum(size_dat_2020$size_class == "Adult" & size_dat_2020$unique == "Y", na.rm = "TRUE")/sum(size_dat_2020$unique == "Y", na.rm = "TRUE")
```



# Modelling

## Factors affecting total manual density
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year(transect_datetime) != 2019) %>% 
  mutate(hour = hour(transect_datetime))

total_density_mod <- glmmTMB(manual_unique ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = poisson, ziformula = ~1)

total_density_sim <- simulateResiduals(fittedModel = total_density_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

dredge(total_density_mod)
#time of day, wharf temp, and manual effort are all predictors in best fit model, model without hour has weight of 0.234

summary(total_density_mod)
#hour significant, wharf temp not

plot(allEffects(total_density_mod))
```

## Factors affecting ratio of juveniles to adults NOT WORKING
```{r}
dat_no_19 <- dat_no_19 %>% 
  mutate(hour = hour(transect_datetime))

View(dat_no_19)

ratio_mod <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = binomial)

ratio_sim <- simulateResiduals(fittedModel = ratio_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

ratio_mod2 <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp, data = dat_no_19, family = binomial)

AICctab(ratio_mod, ratio_mod2)

dredge(ratio_mod)
#not working?

summary(ratio_mod)
# hour significant, wharf temp not
plot(allEffects(ratio_mod))

summary(ratio_mod2)
# higher AIC than temp + hour model

emmeans(ratio_mod)
```

## Modelling Transect vs Manual abundance estimates
```{r}


```

## Modelling Time of Day Effects on Abundance, 2020
```{r}
View(dat_2020_full)
ggplot(aes(x = total_unique), data = dat_2020) +
  geom_bar()

diel_model <- glmmTMB(total_unique ~ hour(datetime.x) + mean_wtemp_previous_24 + WVHT_m + vis + beaufort, data = dat_2020_full, family = nbinom2)
dredge(diel_model)

diel_mod_1 <- glmmTMB(total_unique ~ mean_wtemp_previous_24 + WVHT_m + vis, data = dat_2020_full, family = nbinom2)

fam_list <- list(family = alist(
    binomial = binomial,
    genpois = genpois,
    poisson = poisson,
    nbinom1 = nbinom1,
    nbinom2 = nbinom2,
    ))

getAllTerms(diel_mod_1)

dredge(diel_mod_1, fixed = ~ cond(mean_wtemp_previous_24) + cond(vis) + cond(WVHT_m), varying = fam_list)

diel_mod <- update(diel_mod_1, family = nbinom1)
dredge(diel_mod)

summary(diel_mod)

diel_simulationOutput <- simulateResiduals(fittedModel = diel_mod, n = 250)
plotSimulatedResiduals(simulationOutput = diel_simulationOutput)
testOverdispersion(diel_mod)

plot(allEffects(diel_model), partial.residuals = TRUE)
```

## Abundance Histogram, 2020
```{r}
dat_2020 %>% 
  #filter(total_unique != 0) %>%
  gather("Size", "Abundance", 20:21) %>% 
  ggplot(aes(x = Abundance, fill = Size)) +
    geom_bar(position = "dodge") +
    scale_x_continuous(breaks = 0:10) +
    theme_classic()
```

## Comparing size between methods
```{r}
joined_2020 %>% 
  filter(video.y == "outer") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on outer transect is 2.88

joined_2020 %>% 
  filter(video.y == "outer") %>%
  nrow()
# only 13 sharks seen in outer video?

joined_2020 %>% 
  filter(video.y == "inner") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on inner transect is 2.49

joined_2020 %>% 
  filter(video.y == "inner") %>%
  nrow()
# 65 sharks in inner video

joined_2020 %>% 
  filter(video.y != "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size for transect flights overall is 2.55

joined_2020 %>% 
  filter(video.y == "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size on manual flights in 2.43 - bias towards larger sharks in transect flights?

#Comparing abundance estimates between methods:
aov_method_abund_dat <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      gather("method", "abundance", 2:3)

aov_method_abund_2_dat <- dat_2020 %>% 
                        dplyr::select(date, transect_total, haphazard_unique) %>% 
                        slice(23:nrow(dat_2020)) %>%
                        gather("method", "abundance", 2:3) %>% 
                        filter(!is.na(abundance))

aov_method_abund_dat_no_zero <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      filter(transect_total != 0) %>% 
                      filter(manual_unique != 0) %>%
                      gather("method", "abundance", 2:3)

pairwise.t.test(log1p(aov_method_abund_dat$abundance), aov_method_abund_dat$method)
# p = 0.017
# IMPORTANT: I'm not sure how the NAs affect this - First version removes all dates with an NA for either method, while second version only removes the method that has the NA. First version makes more sense to me.

leveneTest(log1p(abundance) ~ method, data = aov_method_abund_dat)
#approximately equal variances

qqPlot(log1p(aov_method_abund_dat$abundance))
# lol

aov_method_abund_dat %>% 
  ggplot(aes(x = abundance)) +
    geom_histogram()

pairwise.t.test(log1p(aov_method_abund_dat_no_zero$abundance), aov_method_abund_dat_no_zero$method)
#p = 0.042

leveneTest(abundance ~ method, data = aov_method_abund_dat_no_zero)
# fine

qqPlot(log(aov_method_abund_dat_no_zero$abundance))

shapiro.test(aov_method_abund_dat_no_zero$abundance)

#Comparing size estimates between methods:

size_dat_unique <- size_dat_unique %>% 
  mutate(method = ifelse(video == "manual", "manual", "transect"))

pairwise.t.test(size_dat_unique$length_m, size_dat_unique$method)
# p = 0.029

leveneTest(length_m ~ method, data = size_dat_unique)
#equal variances

qqPlot(size_dat_unique$length_m)
# lookin good

shapiro.test(size_dat_unique$length_m)
# hmm

size_dat_unique %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram()
```


## Water temperature (old)
```{r}
mean(flights$water_temp, na.rm=T)

# number of days with each temp 
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# same as above but with channel temps 
ggplot(flights, aes(x=water_temp_channel))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# HOBO temp over time + shark data
temp_plot <- ggplot(flights, aes(x=date, y=water_temp, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp), color="blue")+
  geom_smooth(se=T, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
temp_plot
ggsave("temp_plot.png", width=10, height=7)

# same as above but with channel temps
ggplot(flights, aes(x=date, y=water_temp_channel, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp_channel)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp_channel), color="blue")+
  geom_smooth(se=F, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()

# Air and water temperature over time
ggplot(temps.long, aes(x=date, y=temp, group=type, color=type))+
  geom_line()+
  geom_hline(yintercept=64.90645)

#Air vs channel water temperature
ggplot(flights, aes(x=air_temp, y=water_temp_channel))+
  geom_point()+
  coord_cartesian(xlim=c(55,80))+
  geom_hline(yintercept=64.90645, color="red")+
  stat_smooth(method=lm, se=F, fullrange=T)
```

## Wind
```{r}
wind_avg <- flights %>% ddply(.(wind_dir), summarize, mean=mean(wind_spd)) %>% na.omit(.)
#View(wind_avg)

# Average speed of each direction
ggplot(wind_avg, aes(x=wind_dir, y=mean))+
  geom_bar(stat="identity")+
  coord_polar(theta="x", start=2.7, direction=-1)

# Histogram of wind directions
flights %>% drop_na("wind_dir") %>%
  ggplot(., aes(x=wind_dir))+
    geom_bar()+
    coord_polar(theta="x",start=2.7, direction=-1)

# Wind vs water temp
ggplot(flights, aes(x=wind_spd..kts., y=water_temp))+
  geom_point()+
  geom_smooth(method="lm", se=F)

# Wind vs sky
ggplot(flights, aes(x=wind_spd, y=sky))+
  geom_count()

# Wind vs waves
ggplot(flights, aes(x=wind_spd, y=swell))+
  geom_jitter()+
  geom_smooth(method=lm)
```

## Sky
```{r}
# sky condition vs detections/DNF
ggplot(flights, aes(x=sky, fill=detection))+
  geom_bar(position="stack")+
  scale_fill_manual(values=c("black", "forestgreen", "red"))
```


## Visibility
```{r}
# beaufort vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(beaufort) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
  ggplot(aes(x = beaufort, y = y) )+
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)

# vis vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(vis) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
ggplot(aes(x = vis, y = y) )+
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)
```

## Tide
```{r}
# tide over time + shark data
ggplot(flights, aes(x=date, y=tide, group=beach))+
  geom_line(size=0.5, linetype=2)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
```

# Detection analysis

```{r}
#count(flights, detection)
ggplot(dat_2019, aes(y=wind_spd, x=detection))+
  geom_point()+
  geom_smooth(se=F)
```

## t-test of small vs large abundance, 2020
```{r}
aov_dat <- dat_2020 %>% 
            dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2020)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## t-test of small vs large abundance, 2021
```{r}
aov_dat <- dat_2021 %>% 
            dplyr::rename(Date = date, Small = transect_small,
                          Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2021)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## GLMs to test effects of water temp and sky condition on detections
```{r}
flights_DNF.rm <- filter(flights, detection!="DNF")
det_mod <- glm(detection~water_temp*sky,data=flights_DNF.rm,family=binomial(link="logit"),na.action="na.fail") 
dredge(det_mod)
```
* best model is null model, followed by ~wind_gust, then ~wind_speed

```{r}
DNF_1 <- glm(detection~wind_gust,data=flights,family=binomial(link="logit"))
summary(DNF_1)

ggplot(flights, aes(x=wind_gust, y=detection))+
  geom_count()
```
* even most-correlated variable wind_gust has insignificant effect on detection (p=0.441)

## Diagnostics
```{r}
DNF_simulationOutput <- simulateResiduals(fittedModel = DNF_1, n = 250)
plotSimulatedResiduals(simulationOutput = DNF_simulationOutput)
plotResiduals(flights$wind_spd, DNF_simulationOutput$scaledResiduals) #wonky residuals
#not sure what the differences between the two above lines are

testUniformity(simulationOutput = DNF_simulationOutput)
#testU gives QQ plot, looks normal

#effect plot
plot(allEffects(DNF_1), partial.residuals=TRUE)
```

# Swell analysis
```{r}
ggplot(flights, aes(x=log(swell)))+
  geom_histogram(binwidth=0.25)
```

## GLMs to test effects of wind speed, gust, and direction on swell height
```{r}
swell_mod <- glm(swell~wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="log"), na.action="na.fail")
dredge(swell_mod)
```
* ~wind gust is best, but only somewhat better than null model and ~wind_speed

```{r}
swell_1 <- glm(swell~wind_gust, data=flights, family=gaussian(link="log"))
summary(swell_1)
```
* 1 kt increase in wind gust speed increases log wave height by 0.028; not a significant effect

```{r}
#diagnostics
swell_simulationOutput <- simulateResiduals(fittedModel = swell_1, n = 250)
plotSimulatedResiduals(simulationOutput = swell_simulationOutput)

plot(allEffects(swell_1), partial.residuals=TRUE)
```

# Water Temperature Analysis

```{r}
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=1)
```


# GLMs to test effects of air temp, wind speed, gust and direction on water temp
```{r}
wt_mod <- glm(water_temp~air_temp*wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="identity"), na.action="na.fail")
View(dredge(wt_mod))
```
* best two models (equal AIC) are 
  + 1) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd+wind_gust*wind_dir
  + 2) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir

```{r}
wt_1 <- glm(water_temp~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir, data=flights, family=gaussian(link="identity"))
summary(wt_1)
```
* way too many coefficients, re-doing dredge with max terms = 4

```{r}
dredge(wt_mod, m.max=4)
```
* ~air_temp is best model

```{r}
wt_2 <- glm(water_temp~air_temp, data=flights, family=gaussian(link="identity"))
summary(wt_2)
```
* 1 degree increase in air temp increases water temp by 0.27 degrees, marginally significant effect (p<0.1)