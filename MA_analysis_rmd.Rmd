---
title: "MA_analysis_rmd"
author: "John Parsons"
date: "Compiled on `r format(Sys.Date(), '%B %d, %Y`"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Setup
```{r, eval=FALSE}
#install.packages(c("tidyverse", "dplyr", "lme4", "glmmTMB", "effects", "DHARMa", "MuMIn", plyr", "broom.mixed", "ggthemes", "padr", , "ptest", "maps", "mapdata", hexbin", "zoo", "car", "lubridate", "slider", "hms", "olsrr", "bbmle", "emmeans", "gt", "webshot", "cowplot", "jtools", "ggstance", "AER", "mgcv"))
#install.packages("mgcv")
library(tidyverse)
slice <- dplyr::slice
#library(plyr); library(dplyr)
library(lme4) #modelling
library(glmmTMB) #modelling
library(DHARMa) #model diagnostics
library(sjstats)
library(effects)
library(broom.mixed)
library(MuMIn)
library(ggthemes) #visualizations
library(RColorBrewer) #visualizations
library(padr)
library(ptest)
library(maps)
library(mapdata)
library(ggmap)
library(hexbin) #heatmaps
library(zoo)
library(car)
library(lubridate) #dates
library(slider)
library(hms)
year <- lubridate::year
week <- lubridate::week
library(olsrr)
library(bbmle) #AIC tables
library(emmeans)#effects testing
library(gt)#nice tables
library(webshot) #saving gt tables
library(cowplot)
library(simfit) #simulating models
library(jtools)
library(ggstance)
library(AER)
library(mgcv) #GAMs
#webshot::install_phantomjs() 
#citation("ptest")
```

==============================================================================================

# Data Tidying

## 2019 Data Tidying

### Flight-Level Data Tidying (1 row = 1 day) - 2019
```{r}
dat_2019 <- read.csv("data_raw/2019_flight_data.csv")
#view(dat_2019)

dat_2019$date[104:107] <- as.character(c(
  "10/25/2019", "10/28/2019", "10/30/2019", "11/4/2019"))

dat_2019 <- dat_2019 %>% 
  dplyr::select(!c(3:9,13,14,16:17,20,21,23:25,28,29)) %>% 
  mutate(total_unique = transect_total) %>% 
  mutate(transect_datetime = as_datetime(transect_datetime))

dat_2019[dat_2019 == ""] <- NA

#dat_2019$date <- as.Date(dat_2019$date, "%m/%d/%Y")

dat_2019 <- dat_2019 %>%
  mutate(datetime = as_datetime(transect_datetime)) %>%
  mutate(date_hour = round_date(as_datetime(datetime), "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2019
```{r}
dat_2019 <- dat_2019 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort = 
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = if_else(transect_small == 0, 0,               #transect_small/transect_effort)) %>%
  #mutate(transect_large_density = if_else(transect_large == 0,0,
                                          #transect_large/transect_effort))
#view(dat_2019)
```

### Reading in tide data (Station 9411340) - 2019
```{r}
tide_dat_2019 <- read.csv("data_raw/2019_tide_dat.txt")
#View(tide_dat_2019)
#str(tide_dat_2019)

tide_dat_2019 <- tide_dat_2019 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2019 <- left_join(dat_2019, tide_dat_2019, by = "date_hour")
#View(dat_2019)
```

### Reading in channel buoy data (Station 46053) - 2019
```{r}
channel_dat_2019_raw <- read.table("data_raw/2019_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2019_raw)
#View(channel_dat_2019_raw)e

channel_dat_2019 <- channel_dat_2019_raw %>%
                      mutate_if(is.character, as.numeric) %>% 
                      mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
                      mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
                      group_by(MM, DD, hh) %>% 
                      summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
                      ungroup() %>% 
                      filter(MM > 4) %>% 
                      add_column(year = 2019, .before = 1) %>% 
                      mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
                      mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
                      mutate(date_hour = round_date(datetime, "hour")) %>% 
                      mutate(max_wtemp_previous_24 = 
                              slide_dbl(WTMP, max, .before = 24)) %>% 
                      mutate(min_wtemp_previous_24 = 
                              slide_dbl(WTMP, min, .before = 24)) %>% 
                      mutate(mean_wtemp_previous_24 = 
                              slide_dbl(WTMP, mean, .before = 24))

dat_2019$date_hour <- as_datetime(dat_2019$date_hour)
flights_plus_channel_dat_2019 <- left_join(dat_2019, channel_dat_2019, by = "date_hour")

dat_2019 <- flights_plus_channel_dat_2019 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2)
```

### Reading in SB harbor data (Station 9411340) - 2019
```{r}
harbor_dat_2019 <- read.csv("data_raw/2019_harbor_dat.csv")
#View(harbor_dat_2019)

harbor_dat_2019 <- harbor_dat_2019 %>% 
              rename(time = 2, harbor_windspd = 3,
                     harbor_winddir = 4, harbor_windgust = 5,
                     harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2019 <- left_join(dat_2019, harbor_dat_2019, by = "date_hour")
```

### Size Data Tidying (1 row = 1 individual) - 2019
```{r}
size_dat_2019 <- read.csv("data_raw/size_dat_2019.csv")
#View(size_dat_2019)

size_dat_2019 <- size_dat_2019 %>% 
  select(1:10) %>% 
  dplyr::slice(-(48:49))

#size_dat_2019$date <- as.Date(size_dat_2019$date, "%m/%d/%Y")
#size_dat_2019$length <- as.numeric(size_dat_2019$length)

size_dat_2019 <- size_dat_2019 %>% 
                  mutate(length_raw_m = length/3.2808) %>% 
                  rename(length_raw = length) #%>% 
                  #mutate(size_class = ifelse(length_m < 3, "Juvenile", "Adult"))
#will do size classification on the 3yr dataset

size_dat_2019 %>%
  filter(length_raw_m >= 3) %>% 
  nrow()
```
*no adults! (unadjusted lengths)

## 2020 Data Tidying

### Flight-Level Data Tidying - 2020
```{r}
dat_2020 <- read.csv("data_raw/2020_flight_data.csv")
#View(dat_2020)

#dat_2020$date <- as_date(as.Date(dat_2020$date, "%m/%d/%Y"))

dat_2020$transect_total <- as.numeric(dat_2020$transect_total)

dat_2020$transect_datetime <- as_datetime(dat_2020$transect_datetime)
dat_2020$manual_datetime <- as_datetime(dat_2020$manual_datetime)

dat_2020 <- dat_2020 %>%
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  select(!c(2:8,13,14,16,17,20,21,25,26,28,29:31,38,39))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2020
```{r}
dat_2020 <- dat_2020 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_effort_density =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2020 <- dat_2020 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort =
         manual_total/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340)- 2020
```{r}
tide_dat_2020 <- read.csv("data_raw/2020_tide_dat.txt")
#View(tide_dat_2020)
str(tide_dat_2020)

tide_dat_2020 <- tide_dat_2020 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>%
  select(-c(1,3,4)) %>%
  rename(tide = 2)

tide_added_2020 <- left_join(dat_2020, tide_dat_2020, by = "date_hour")
#View(tide_added_2020)
```

### Reading in channel buoy data (Station 46053) - 2020
```{r}
channel_dat_2020_raw <- read.table("data_raw/2020_buoy_dat.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))
#str(channel_dat_2020_raw)
#View(channel_dat_2020_raw)

channel_dat_2020 <- channel_dat_2020_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 4) %>% 
  add_column(year = 2020, .before = 1) %>% 
  mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
  mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))

tide_added_2020$date_hour <- as_datetime(tide_added_2020$date_hour)
flights_plus_channel_dat_2020 <- left_join(tide_added_2020, channel_dat_2020, by = "date_hour")
#View(flights_plus_channel_dat_2020)

dat_2020 <- flights_plus_channel_dat_2020 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) %>% 
  select(-c(31,32))
#View(dat_2020)
```

### Reading in SB harbor data (Station 9411340) - 2020
```{r}
harbor_dat_2020 <- read.csv("data_raw/2020_harbor_dat.csv")
#View(harbor_dat_2020)

harbor_dat_2020 <- harbor_dat_2020 %>% 
              rename(time = 2, harbor_windspd = 3, harbor_winddir = 4, harbor_windgust = 5, harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2020 <- left_join(dat_2020, harbor_dat_2020, by = "date_hour")
#View(dat_2020)
```

### Size Data Tidying (1 row = 1 individual) - 2020
```{r}
size_dat_2020 <- read.csv("data_raw/size_dat_2020.csv")
#View(size_dat_2020)

#size_dat_2020$date <- as.Date(size_dat_2020$date, "%m/%d/%Y")

size_dat_2020 <- size_dat_2020 %>% 
                  mutate(length = as.numeric(length)) %>% 
                  mutate(order = as.numeric(order)) %>% 
                  mutate(length_m = length/3.2808) %>% 
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) %>%
                  rename(length_raw = length) %>% 
                  rename(length_raw_m = length_m) %>% 
                  slice(-363) #%>% 
                  #mutate(size_class = ifelse(length_m < 3, "Juvenile", "Adult"))
#going to do size classification on the 3yr dataset instead

size_dat_2020 %>% 
  filter(length_raw_m >= 3) %>% 
  nrow()
#34 adults

size_dat_2020 %>% 
  filter(length_raw_m < 3) %>% 
  nrow()
#324 juveniles
```
* roughly 10x more juveniles than adults (uncorrected lengths)

## 2021 Data Tidying

### Flight-Level Data Tidying - 2021
```{r}
dat_2021 <- read.csv("data_raw/2021_flight_data.csv")
#view(dat_2021)

#dat_2021 <- dplyr::rename(dat_2021, date = 1)
#dat_2021$date <- as_date(as.Date(dat_2021$date, "%m/%d/%Y"))

dat_2021$transect_total <- as.numeric(dat_2021$transect_total)
dat_2021$manual_unique <- as.numeric(dat_2021$manual_unique)
dat_2021$manual_total <- as.numeric(dat_2021$manual_total)

dat_2021$transect_datetime <- as_datetime(dat_2021$transect_datetime)
dat_2021$manual_datetime <- as_datetime(dat_2021$manual_datetime)

dat_2021 <- dat_2021 %>%
  select(!c(3:6,9,10,15,16,18,19,22,23,29:32,39:41)) %>% 
  mutate(datetime = as_datetime(
    pmin(transect_datetime, manual_datetime, na.rm = TRUE))) %>% 
  mutate(date_hour = round_date(datetime, "hour"))
```

### Calculating density (sharks/km2) and effort (sharks/km2/min) - 2021
```{r}
dat_2021 <- dat_2021 %>%
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  mutate(transect_duration_min = transect_duration_sec/60) %>% 
  mutate(transect_density_effort =
         transect_total/(transect_area_km2*transect_duration_min)) %>% 
  mutate(transect_density = transect_total/transect_area_km2) #%>% 
  #mutate(transect_small_density = transect_small/transect_effort) %>%
  #mutate(transect_large_density = transect_large/transect_effort)

dat_2021 <- dat_2021 %>%
  mutate(manual_area_km2 = manual_area_m2/1000000) %>% 
  mutate(manual_duration_min = manual_duration_sec/60) %>% 
  mutate(manual_density_effort = 
         manual_unique/(manual_area_km2*manual_duration_min)) %>% 
  mutate(manual_density = manual_unique/manual_area_km2) #%>% 
  #mutate(manual_small_density = manual_small/manual_effort) %>%
  #mutate(manual_large_density = manual_large/manual_effort)
```

### Reading in tide data (Station 9411340) - 2021
```{r}
tide_dat_2021 <- read.csv("data_raw/2021_tide_dat.txt")
#View(tide_dat_2021)
#str(tide_dat_2021)

tide_dat_2021 <- tide_dat_2021 %>% 
  rename(time = 2) %>% 
  mutate(time = as_datetime(time, format = "%H:%M")) %>%
  mutate(time = substring(time, 12)) %>% 
  mutate(date = as_datetime(Date)) %>% 
  unite(date_hour, c(date, time), sep = " ") %>% 
  mutate(date_hour = as_datetime(date_hour)) %>% 
  select(-c(1,3,4)) %>%
  rename(tide = 2)

dat_2021 <- left_join(dat_2021, tide_dat_2021, by = "date_hour")
#View(dat_2021)
```

### Reading in channel buoy data (Station 46053) - 2021
```{r}
channel_dat_2021_jan <- read.table("data_raw/2021_buoy_dat_jan.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_feb <- read.table("data_raw/2021_buoy_dat_feb.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_mar <- read.table("data_raw/2021_buoy_dat_mar.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_apr <- read.table("data_raw/2021_buoy_dat_apr.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_may <- read.table("data_raw/2021_buoy_dat_may.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jun <- read.table("data_raw/2021_buoy_dat_jun.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_jul <- read.table("data_raw/2021_buoy_dat_jul.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_aug <- read.table("data_raw/2021_buoy_dat_aug.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_sep <- read.table("data_raw/2021_buoy_dat_sep.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_oct <- read.table("data_raw/2021_buoy_dat_oct.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_nov <- read.table("data_raw/2021_buoy_dat_nov.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_dec <- read.table("data_raw/2021_buoy_dat_dec.txt", header = FALSE, 
                                   col.names = c("#YY","MM","DD","hh","mm",
                                                 "WDIR","WSPD","GST","WVHT",
                                                 "DPD","APD","MWD","PRES",
                                                 "ATMP","WTMP","DEWP","VIS",
                                                 "TIDE"))

channel_dat_2021_raw <- dplyr::bind_rows(channel_dat_2021_jan, channel_dat_2021_feb, channel_dat_2021_mar, channel_dat_2021_apr, channel_dat_2021_may, channel_dat_2021_jun, channel_dat_2021_jul, channel_dat_2021_aug, channel_dat_2021_sep, channel_dat_2021_oct, channel_dat_2021_nov, channel_dat_2021_dec)
#View(channel_dat_2021_raw)

channel_dat_2021 <- channel_dat_2021_raw %>%
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(WVHT, DPD, APD), ~na_if(., 99)) %>%  
  mutate_at(vars(MWD, ATMP, WTMP, DEWP), ~na_if(., 999)) %>%
  group_by(MM, DD, hh) %>% 
  summarize_at(vars(3:13), mean, na.rm = TRUE) %>%
  ungroup() %>% 
  filter(MM > 3) %>% 
  add_column(year = 2021, .before = 1) %>% 
  mutate(datetime = make_datetime(year, MM, DD, hh)) %>% 
  mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
  mutate(date_hour = round_date(datetime, "hour")) %>% 
  mutate(max_wtemp_previous_24 = slide_dbl(WTMP, max, .before = 24)) %>% 
  mutate(min_wtemp_previous_24 = slide_dbl(WTMP, min, .before = 24)) %>% 
  mutate(mean_wtemp_previous_24 = slide_dbl(WTMP, mean, .before = 24))
#View(channel_dat_2021)

dat_2021$date_hour <- as_datetime(dat_2021$date_hour)
flights_plus_channel_dat_2021 <- left_join(dat_2021, channel_dat_2021, by = "date_hour")
#View(flights_plus_channel_dat_2021)

dat_2021 <- flights_plus_channel_dat_2021 %>%
  mutate(wtemp_2 = mean_wtemp_previous_24^2) %>% 
  select(-c(31,32))
#View(dat_2021)
```

### Reading in SB harbor data (Station 9411340) - 2021
```{r}
harbor_dat_2021 <- read.csv("data_raw/2021_harbor_dat.txt")
#View(harbor_dat_2021)

harbor_dat_2021 <- harbor_dat_2021 %>% 
              rename(time = 2, harbor_windspd = 3,
                     harbor_winddir = 4, harbor_windgust = 5,
                     harbor_airtemp = 6, harbor_baro = 7) %>% 
              mutate(time = lubridate::hm(time)) %>% 
              mutate(YY = year(Date)) %>% 
              mutate(MM = month(Date)) %>% 
              mutate(DD = day(Date)) %>% 
              mutate(hh = hour(time)) %>% 
              mutate(mm = minute(time)) %>% 
              mutate(date_hour = make_datetime(YY, MM, DD, hh, mm)) %>% 
              select(-c(2, 8:14))
              
dat_2021 <- left_join(dat_2021, harbor_dat_2021, by = "date_hour")
```

### Size Data Tidying (1 row = 1 individual) - 2021
```{r}
size_dat_2021 <- read.csv("data_raw/size_dat_2021.csv")
#view(size_dat_2021)

#size_dat_2021$date <- as.Date(size_dat_2021$date, "%m/%d/%Y")
size_dat_2021$length_raw <- as.numeric(size_dat_2021$length_raw)
size_dat_2021$length_adj <- as.numeric(size_dat_2021$length_adj)

size_dat_2021 <- size_dat_2021 %>% 
                  mutate(length_adj_m = length_adj/3.2808) %>% 
                  mutate(length_raw_m = length_raw/3.2808) %>%
                  mutate(lat = as.numeric(lat)) %>% 
                  rename(video_YN = video) %>% 
                  rename(tagged = tagged.) #%>% 
                  #mutate(size_class = ifelse(
                    #length_adj_m < 3, "Juvenile", "Adult"))
# going to do the adult/juvenile classification on the 3yr dataset instead

size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(length_adj_m >= 3) %>% 
  nrow()
#143 adults

size_dat_2021 %>%
  filter(unique == "Y") %>% 
  filter(length_adj_m < 3) %>% 
  nrow()
#148 juveniles
```
MUCH higher % of adults than 2020 - almost 50/50 when adjusted size is used

## Three-Year ("3yr") dataset tidying

### Combine 2019, 2020, and 2021 datasets
```{r}
#str(dat_2019)
#str(dat_2020)

#dat_2020 <- dat_2020 %>% 
                  #rename(WDIR = WDIR_deg,
                         #WSPD = WSPD_ms,
                        # DPD = DPD_sec,
                         #APD = APD_sec,
                        # MWD = MWD_deg,
                         #PRES = PRES_hPa)
#above code should be moved to 2020 tidying section (if it's needed at all)

daily_2yr_dat <- full_join(dat_2019, dat_2020)
#view(daily_2yr_dat)

daily_3yr_dat <- full_join(daily_2yr_dat, dat_2021)
#view(daily_3yr_dat)
```

### Tidying 3-year data
```{r}
#daily_3yr_dat <- daily_3yr_dat %>% 
  #select(-c(1, 31, 39:42, 48, 76))

#daily_3yr_dat <- daily_3yr_dat %>% mutate(date = as.Date(date))
#daily_3yr_dat <- daily_3yr_dat %>% mutate(date = if_else(year(date) < 2000, date + years(2000), date))

daily_3yr_dat <- daily_3yr_dat %>% 
  mutate(year = as_factor(year(datetime))) %>% 
  mutate(day = yday(datetime)) %>% 
  mutate(day2 = (yday(datetime))^2)

write.csv(daily_3yr_dat, "daily_3yr_dat.csv")
```

## Building 3yr size dataset
```{r}
size_dat_2yr <- full_join(size_dat_2019, size_dat_2020)

size_dat_3yr <- full_join(size_dat_2yr, size_dat_2021)
#view(size_dat_3yr)

size_dat_3yr <- size_dat_3yr %>% 
                  select(!c(17:19))

write.csv(size_dat_3yr, "data/size_dat_3yr.csv")
```

### New 3yr df with size of each unique shark
(1 row = 1 shark, daily data is repeated for days where multiple sharks were observed)
```{r}
size_dat_3yr <- read.csv("data/size_dat_3yr.csv")

size_dat_unique <- size_dat_3yr %>%
                    filter(unique == "Y")

nrow(size_dat_unique)
```
919 sightings across 3 years

```{r}
size_dat_unique$date <- as.character(size_dat_unique$date)
joined_3yr <- left_join(daily_3yr_dat, size_dat_unique, by = "date")
#view(joined_3yr)
```

## Adding size classes to daily 3yr dataframe
```{r}
size_classed_3yr <- joined_3yr %>%
  filter(!is.na(length_adj_m)) %>% 
  group_by(date) %>%
  dplyr::summarize(juvenile = sum(length_adj_m < 3), 
                   adult = sum(length_adj_m >= 3))

#View(size_classed_3yr)

ggplot(data = size_classed_3yr, aes(x = adult)) +
  geom_bar()
#crazy outlier on 4/30/2021

daily_3yr_dat <- left_join(daily_3yr_dat, size_classed_3yr, by = "date")

#daily_3yr_dat <- daily_3yr_dat  %>%
  #mutate(juvenile_density = juvenile/(transect_effort + manual_effort)) %>%
  #mutate(adult_density = adult/(transect_effort + manual_effort))
```

## Adding wharf data - not sure why I needed this?
```{r}
wharf_dat <- read.csv("data_raw/wharf_dat.csv")

View(wharf_dat)

wharf_dat <- wharf_dat %>% 
              mutate(datetime = as_datetime(time)) %>% 
              mutate(datetime = with_tz(datetime, "US/Pacific")) %>%
              mutate(date_hour = round_date(datetime, "hour")) %>% 
              select(c(-1, -3, -5, -7, -9)) %>% 
              group_by(date_hour) %>% 
              summarize_at(vars(1:4), mean, na.rm = TRUE)

daily_3yr_dat <- read.csv("data/daily_3yr_dat.csv")
daily_3yr_dat$date_hour <- as_datetime(daily_3yr_dat$date_hour)
daily_3yr_dat <- left_join(daily_3yr_dat, wharf_dat, by = "date_hour")
daily_3yr_dat <- daily_3yr_dat %>% 
  rename(wharf_temp = temperature)
```


## Dataframe without 2019
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year(date_hour) != 2019)
```

===============================================================================

# Analyses for Thesis

## Rudimentary power analysis - do weekly averages change when only half of the data is used?
```{r}
pwr_dat <- dat_2021 %>% 
  select(datetime, total_unique) %>% 
  mutate(every_other = ifelse(row_number() %% 2 == 1, total_unique, NA)) %>% 
  group_by(week(datetime)) %>% 
  summarize(mean_total = mean(total_unique, na.rm = TRUE),
            mean_eo = mean(every_other, na.rm = TRUE)) %>% 
  rename(week = 1)

mean(pwr_dat$mean_total)
#4.23 when all data used

mean(pwr_dat$mean_eo, na.rm = TRUE)
#4.31 when every other survey used

t.test(pwr_dat$mean_total, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")

wilcox.test(pwr_dat$mean_total, pwr_dat$mean_eo, paired = TRUE, alternative = "two.sided")


pwr_dat %>%
  pivot_longer(mean_total:mean_eo, names_to = "Data", values_to = "weekly_mean") %>% 
  ggplot(aes(x = week, y = weekly_mean, fill = Data)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(rows = vars(Data)) +
    theme_clean() +
    labs(x = "Week", y = "Weekly Mean") +
    theme(strip.text.y = element_blank()) +
    scale_fill_manual(labels = c("Every other survey", "All data"),
                      values = c("darkgreen", "orange3"))
    
```

## Investigating different metrics - raw count (each method and "unique") and density
```{r}
dat_no_19 %>%
  group_by(year(date_hour)) %>% 
  summarize("Unique count" = mean(total_unique, na.rm = TRUE),
            #"Maximum unique count" = max(total_unique, na.rm = TRUE),
            "Manual count" = mean(manual_total, na.rm = TRUE),
            #"Maximum manual count" = max(manual_total, na.rm = TRUE),
            "Transect count" = mean(transect_total, na.rm = TRUE),
            #"Maximum transect count" = max(transect_total, na.rm = TRUE),
            "Unique density" = mean(total_unique/(manual_area_m2 + transect_area_m2), na.rm = TRUE),
            #"Maximum unique density" = max(total_unique/(manual_area_km2 + transect_area_km2), na.rm = TRUE),
            "Manual density" = mean(manual_unique/manual_area_m2, na.rm = TRUE),
            #"Maximum manual density" = max(manual_unique/manual_area_km2, na.rm = TRUE),
            "Transect density" = mean(transect_total/transect_area_m2, na.rm = TRUE),
            #"Maximum transect density" = max(transect_total/transect_area_km2, na.rm = TRUE)
  )
```
* Looks like manual count was higher than transect count in 2020, but not 2021

```{r}
ggplot(dat_no_19, aes(x = manual_unique, y = transect_total)) +
  geom_jitter() +
  geom_smooth(method = "lm")
#outliers are potentially a big problem

ggplot(daily_3yr_dat, aes(x = transect_total)) +
  geom_bar(stat = "count")

ggplot(daily_3yr_dat, aes(x = manual_unique)) +
  geom_bar(stat = "count")

t_vs_m.lm <- lm(transect_total ~ manual_unique, data = dat_no_19)
summary(t_vs_m.lm)
#slope = 0.60 - one shark sighted in manual = 0.6 sharks sighted in transect
#r2 = 0.2792

ggplot(dat_no_19, aes(x = manual_unique, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_m.lm <- lm(total_unique ~ manual_unique, data = dat_no_19)
summary(u_vs_m.lm)
#slope = 1.16
#r2 = 0.6746

ggplot(dat_no_19, aes(x = transect_total, y = total_unique)) +
  geom_jitter() +
  geom_smooth(method = "lm")

u_vs_t.lm <- lm(total_unique ~ transect_total, data = dat_no_19)
summary(u_vs_t.lm)
#slope = 1.07
#r2 = 0.76
```
*best correlation is between TRANSECT count and total count (but these numbers are obviously not independent)

```{r}
mean(dat_no_19$transect_total, na.rm = TRUE)
mean(dat_no_19$manual_unique, na.rm = TRUE)

ggplot(dat_no_19, aes(x = transect_total-manual_unique)) +
  geom_histogram(stat = "count")
```
* negative x-axis value means more sharks seen in manual - to me this is good evidence that we should use manual count: it is simply seeing more sharks

### Comparing methods
```{r}
mean(dat_no_19$transect_total/dat_no_19$transect_area_km2, na.rm = TRUE)
mean(dat_no_19$manual_unique/dat_no_19$manual_area_km2, na.rm = TRUE)
#transect actually has higher density

method_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  mutate(manual = manual_unique/manual_area_km2) %>% 
  mutate(transect = transect_total/transect_area_km2) %>% 
  pivot_longer(c(manual, transect), names_to = "method",
                                    values_to = "density")
  
ggplot(method_dat, aes(x = density)) +
  geom_histogram(binwidth = 10) +
  facet_grid(~method)
#normal distribution obviously not going to work (for ANOVA or GLM)
```

```{r}
dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  filter(manual_unique != 0) %>% 
  nrow()
#46 days where transect saw nothing and manual saw something

dat_no_19 %>% 
  filter(transect_total == 0) %>% 
  summarize(mean = mean(manual_unique, na.rm = TRUE))
#when no sharks are detected in transect, manual detects 0.898

dat_no_19 %>% 
  filter(manual_unique == 0) %>%
  filter(transect_total != 0) %>% 
  nrow()
#14 days where manual saw nothing and transect saw something

dat_no_19 %>% 
  filter(manual_unique == 0) %>% 
  summarize(mean = mean(transect_total, na.rm = TRUE))
#when no sharks are detected in manual, transect detects 0.539
```
*This is further support for using manual - but transect densities are going to have to be used anyways for comparing across all three years?

### GLM approach to comparing methods (halting work on this while I try a paired-samples Wilcoxon, see next chunk. I don't think I'll be using GLM for this)
```{r}
method_glm_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_area_km2)) %>%
  select(transect_area_km2, manual_area_km2, transect_total, manual_unique, date) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_area_km2, manual_area_km2), names_to = "area_method",
                                    values_to = "area") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0)
#selects every 1st and 4th row (remainder 1 and remainder 0)

method_glm_pois <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = "poisson")

met_res <- simulateResiduals(method_glm_pois)
plot(met_res)
#poisson no bueno (overdispersed)

method_glm_nb2 <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = nbinom2)

met_res <- simulateResiduals(method_glm_nb2)
plot(met_res)

testQuantiles(met_res)
# nb2 better, but not great

method_glm_nb1 <- glmmTMB(count ~ method + offset(log(area)) + (1|year), data = method_glm_dat, family = nbinom1)

met_res <- simulateResiduals(method_glm_nb1)
plot(met_res)
#nb1 worse, going to try a ZI with nbinom2:

method_glm_zi <- glmmTMB(count ~ method + offset(log(area)) + (1|year), ziformula = ~1, data = method_glm_dat, family = nbinom2, na.action = "na.fail")

met_res <- simulateResiduals(method_glm_zi)
plot(met_res)

dredge(method_glm_zi)
#best model doesn't even include method
```

### Wilcoxon signed-rank test on counts
```{r}
paired_dat <- method_glm_dat %>% 
  select(date, method, count) %>% 
  pivot_wider(names_from = method, values_from = count) %>%
  mutate(diff = manual - transect)
nrow(paired_dat)

ggplot(paired_dat, aes(x = diff)) +
    geom_bar()
#paired differences actually look pretty normally distributed...

shapiro.test(paired_dat$diff)
#jk, not normal
  
transect_counts <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  select(count)
nrow(transect_counts)

manual_counts <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  select(count)
nrow(manual_counts)

# explicitly testing the hypothesis that manual count is higher than transect count:
method_test <- wilcox.test(manual_counts$count, transect_counts$count, paired = TRUE, alternative = "greater")
method_test
#more likely to see more sharks in the manual survey than in the transect survey (p<0.01)
```

### Calculating effect size (basically just % of days where manual saw more, transect saw more, or they were the same)
```{r}
# number of days with no difference:
paired_dat %>%
  filter(diff == 0) %>% 
  nrow()
# 84 days

84/nrow(paired_dat)
# 38% of days

paired_dat %>%
  filter(transect == 0) %>% 
  filter(manual == 0) %>% 
  nrow()
# 61 days where BOTH were zero

61/84
# 72.6% of "manual = transect" days were days where no sharks were seen!

#number of days where manual saw more:
paired_dat %>%
  filter(diff > 0) %>% 
  nrow()
# 84 days (weird), 38%

#number of days where transect saw more:
paired_dat %>%
  filter(diff < 0) %>% 
  nrow()
# 53 days

53/nrow(paired_dat)
# 24% of days

mean(manual_counts$count)
#average manual count is 2.28

mean(transect_counts$count)
# average transect count is 1.93
```
* Manual survey count was significantly more likely to be higher than transect survey count (p < 0.01), with manual count being higher than transect count for 38% of survey days, lower than transect count for 24% of survey days, and equal to transect count for 38% of survey days. Of the 84 days where manual and transect count were equal, 72.6% (61 days) were days where no sharks were seen.

### Wilcoxon signed-rank test on density
```{r}
paired_dens_dat <- method_glm_dat %>%
  mutate(density = count/area) %>% 
  select(date, method, density) %>% 
  pivot_wider(names_from = method, values_from = density) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_dens_dat$diff)
#not normal

transect_dens <- method_glm_dat %>% 
  filter(method == "transect") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(transect_dens)

manual_dens <- method_glm_dat %>% 
  filter(method == "manual") %>% 
  mutate(density = count/area) %>% 
  select(density)
nrow(manual_dens)

# do methods differ in density (2-sided, n = 221):
method_dens_test <- wilcox.test(manual_dens$density, transect_dens$density, paired = TRUE)
method_dens_test
# NOT more likely to see a higher density of sharks (count per unit area surveyed) in one survey method vs the other

mean(manual_dens$density)
# average manual density is 11.27 sharks/km2

mean(transect_dens$density)
# average transect density is 11.25 sharks/km2
```

### Comparing rates (count per unit time)
```{r}
method_rate_dat <- dat_no_19 %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_duration_min)) %>% 
  filter(!is.na(manual_unique)) %>% 
  filter(!is.na(manual_duration_min)) %>%
  select(transect_total, manual_unique, date, transect_duration_min, manual_duration_min) %>%
  rename("transect" = transect_total) %>% 
  rename("manual" = manual_unique) %>% 
  pivot_longer(c(transect, manual), names_to = "method",
                                    values_to = "count") %>% 
  pivot_longer(c(transect_duration_min, manual_duration_min), names_to = "time_method",
                                    values_to = "duration") %>% 
  filter(row_number() %% 4 == 1 | row_number() %% 4 == 0) %>% 
  mutate(year = as.factor(year(date))) %>% 
  mutate(method = as.factor(method))

paired_rate_dat <- method_rate_dat %>%
  mutate(rate = count/duration) %>% 
  select(date, method, rate) %>% 
  pivot_wider(names_from = method, values_from = rate) %>%
  mutate(diff = manual - transect)

shapiro.test(paired_rate_dat$diff)
#not normal

transect_rate <- method_rate_dat %>% 
  filter(method == "transect") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(transect_rate)

manual_rate <- method_rate_dat %>% 
  filter(method == "manual") %>% 
  mutate(rate = count/duration) %>% 
  select(rate)
nrow(manual_rate)

# do methods differ in rate (2-sided, n = 221):
method_rate_test <- wilcox.test(manual_rate$rate, transect_rate$rate, paired = TRUE)
method_rate_test
# Not more likely for manual or transect to have a higher rate

mean(manual_rate$rate)
#average manual rate is 0.205 sharks/min
View(manual_counts)
mean(transect_rate$rate)
# average transect rate is 0.185 sharks/km2
```

### Visualizing manual vs transect metrics - data prep
```{r}
manual_counts <- mutate(manual_counts, value = count*10, method = "manual", metric = "count")
transect_counts <- mutate(transect_counts, value = count*10, method = "transect", metric = "count")
manual_dens <- mutate(manual_dens, value = density, method = "manual", metric = "density")
transect_dens <- mutate(transect_dens, value = density, method = "transect", metric = "density")
manual_rate <- mutate(manual_rate, value = rate*100, method = "manual", metric = "rate")
transect_rate <- mutate(transect_rate, value = rate*100, method = "transect", metric = "rate")

comp_vis_dat <- bind_rows(manual_counts, transect_counts, manual_dens, transect_dens, manual_rate, transect_rate)

View(comp_vis_dat)
```

### Manual vs Transect 3 metric comparison plot:
```{r}
method_comp_plot_final <- ggplot(comp_vis_dat, aes(x = metric, y = value)) +
  geom_boxplot(aes(color = method)) +
  theme_cowplot() +
  scale_color_brewer(palette = "Dark2") +
  geom_point(aes(y = 139, shape = metric)) +
  scale_shape_manual(values = c(8, NA, NA)) + #significance asterisk
  guides(shape = "none") + 
  theme(panel.background = element_rect(fill = "white", colour = NA),
        plot.background = element_rect(fill = "white", colour = NA)) +
  labs(x = "Metric", y = "Value", color = "Method")

method_comp_plot_final
ggsave("plots/method_comp_plot_final.png")
# count is scaled up by a factor of 10 and rate is scaled up by a factor of 100. Only sig diff is count.
```

```{r}
#also will try to plot rate as minutes per shark to avoid the 100x:

manual_time <- mutate(manual_rate, value = 10/rate, method = "manual", metric = "time")
transect_time <- mutate(transect_rate, value = 10/rate, method = "transect", metric = "time")

comp_vis_time <- bind_rows(manual_counts, transect_counts, manual_dens, transect_dens, manual_time, transect_time)

#count is 10x and "time" is minutes per shark x10 (which is not intuitive at all - I;m definitely not going to use this
ggplot(comp_vis_time, aes(x = metric, y = value)) +
  geom_boxplot(aes(color = method))

# now let's try standardizing by the manual metric:
manual_counts_stan <- mutate(manual_counts, value = count, method = "manual", metric = "count")
transect_counts_stan <- mutate(transect_counts, value = count, method = "transect", metric = "count")

counts_stan <- bind_cols(manual_counts_stan, transect_counts_stan)
counts_stan <- mutate(counts_stan, manual = 1, transect = value...8/value...4)
counts_stan <- pivot_longer(counts_stan, 9:10)

manual_dens_stan <- mutate(manual_dens, value = density, method = "manual", metric = "density")
transect_dens_stan <- mutate(transect_dens, value = density, method = "transect", metric = "density")

density_stan <- bind_cols(manual_dens_stan, transect_dens_stan)
density_stan <- mutate(density_stan, manual = 1, transect = value...8/value...4)
density_stan <- pivot_longer(density_stan, 9:10)

count_and_dens_stan_dat <- bind_rows(counts_stan, density_stan)
View(count_and_dens_stan_dat)

ggplot(count_and_dens_stan_dat, aes(x = metric...3, y = value...10)) +
  geom_boxplot(aes(color = name))
#stupid idea lol
```

## Patterns of Abundance

### What are the average and range of detections?
```{r}
daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize("Total sightings" = sum(transect_total, na.rm = TRUE),
            "Survey days" = n(),
            "Average density" = 1000000*sum(transect_total/transect_area_m2, na.rm = TRUE)/n(),
            "Maximum abundance" = max(transect_total, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year(date)" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Average density" ~ px(80)) %>% 
    cols_align(align = "center")
```

### Do detections vary across years? (GLM)
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = year, y = total_unique)) +
    geom_violin()

ggplot(daily_3yr_dat, aes(x = total_unique)) +
  geom_bar()
#very zero-inflated

ggplot(daily_3yr_dat, aes(x = log1p(total_unique))) +
  geom_bar()
#going to need poisson

year_poisson_mod <- glm(total_unique ~ year, family = "poisson", data = daily_3yr_dat)

summary(year_poisson_mod)
dispersiontest(year_poisson_mod)
ypm_res <- simulateResiduals(year_poisson_mod)
plot(ypm_res)
#definitely overdispersed

year_qpois_mod <- glm(total_unique ~ year, family = "quasipoisson", data = daily_3yr_dat)

summary(year_qpois_mod)

year_nb_mod <- glmmTMB(total_unique ~ year, family = nbinom2, data = daily_3yr_dat)

summary(year_nb_mod)
ynm_res <- simulateResiduals(year_nb_mod)
plot(ynm_res)
#dispersion still significant

year_nb1_mod <- glmmTMB(total_unique ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_nb1_mod)
yn1m_res <- simulateResiduals(year_nb1_mod)
plot(yn1m_res)
#Negative binomial 1 checks out

pairs(emmeans(year_nb1_mod, ~year))

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE))

year_trans_nb1_mod <- glmmTMB(transect_total ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_trans_nb1_mod)
ytn1m_res <- simulateResiduals(year_trans_nb1_mod)
plot(ytn1m_res)

pairs(emmeans(year_trans_nb1_mod, ~year))
```
* based on GLM with ONLY year, total unique sightings increased each year from 2019 to 2021 (p < 0.0001)
* based on GLM with ONLY year, transect sightings increased each year from 2019 to 2021 (p < 0.0005)


### Manual sightings only
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = year, y = manual_unique)) +
    geom_violin()

year_manual_mod <- glmmTMB(manual_unique ~ year, family = nbinom1, data = daily_3yr_dat)

summary(year_manual_mod)
ymm_res <- simulateResiduals(year_manual_mod)
plot(ymm_res)
#Negative binomial 1 checks out

pairs(emmeans(year_manual_mod, ~year))

daily_3yr_dat %>% 
  group_by(year) %>% 
  summarize(mean = mean(manual_unique, na.rm = TRUE))
```
* based on GLM with ONLY year, manual unique sightings were higher in 2021 than 2020 (p < 0.005)

### Do detections vary across the field season?
```{r}
season_lm <- lm((1000000*transect_total/transect_area_m2) ~ day, data = daily_3yr_dat)
summary(season_lm)
#no apparent linear trend

season2_lm <- lm((1000000*transect_total/transect_area_m2) ~ day + day2, data = daily_3yr_dat)
summary(season2_lm)
#no quadratic trend (not sure this is a meaningful test anyways)
```

### Adding "day" to the year model
```{r}
daily_3yr_narm <- daily_3yr_dat %>%
  filter(transect_area_m2 > 10) %>% 
  mutate(transect_area_km2 = transect_area_m2/1000000) %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(transect_area_km2)) %>% 
  rowwise() %>% 
  mutate(total_area_km2 = sum(transect_area_km2, manual_area_km2, na.rm = TRUE)) %>% 
  mutate(total_dens = total_unique/total_area_km2) %>% 
  filter(!is.na(total_unique)) %>% 
  filter(!is.na(total_area_km2)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour)))
  
season_glm <- glmmTMB(transect_total ~ year*day + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(season_glm)

dredge(season_glm)
#day*year interaction model is best candidate - but I don't have any replication for many days due to non-overlapping survey window

sm_res <- simulateResiduals(season_glm)
plot(sm_res)
#minor issue with quantiles. Don't think it is a problem based on what I've read about that test
```
* Best model to explain transect sightings includes day, year, and their interaction.

### Alternative approach to comparing years ("day" plus looking at survey-related variables like time of day, area surveyed, etc.)
```{r}
year_glm <- glmmTMB(transect_total ~ year*hour + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")
#use log(area) because negative binomial models the log of the mean

summary(year_glm)

yr_res <- simulateResiduals(year_glm)
plot(yr_res)

dredge(year_glm)
#year*hour w/out area is best, but I'm not sure that the interaction between year and hour is meaningful (no reason a priori to think that the effect of time of day would depend on year)...
```

### Final modeling procedure - full survey window, transect sightings only
```{r}
year_glm <- glmmTMB(transect_total ~ year + day + hour + offset(log(transect_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(year_glm)

yr_res <- simulateResiduals(year_glm)
plot(yr_res)
##minor issue with quantiles in global model (not when day is included). Don't think it is a problem based on what I've read about that test

dredge(year_glm)
#best model to explain transect sightings includes year, day, and hour of survey, but not area surveyed - if this were a predictive model, I would keep area in, but since it doesn't contribute additional explanatory power to the patterns that I already saw, leaving it out will give us a better understanding of the effects of year, day, and hour:

year_glm_best <- glmmTMB(transect_total ~ year + day + hour, family = nbinom1, data = daily_3yr_narm, na.action = "na.fail") #log link

summary(year_glm_best)

#diagnose best model:
yr_bst_res <- simulateResiduals(year_glm_best)
plot(yr_bst_res)
#passes DHARMa checks

Anova(year_glm)
year_emms <- emmeans(year_glm, "year")
pwpm(year_emms, type = "response")

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(density = mean(transect_total/transect_area_km2))
```
* Best model is year + day + hour with no offset for area surveyed
* adding area surveyed increases AIC (transect area has very low variation, so it makes sense that adding it as a predictor doesn't contribute much info)

### Effects testing of final "year" GLM - full survey window, transect sightings only
```{r}
Anova(year_glm_best)
# year, day, and hour have significant effects on sightings (which we already know from the model selection process)

year_emms <- emmeans(year_glm_best, "year")
pwpm(year_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison

#compare to ANOVA (sanity check):
year_aov <- aov(transect_total ~ year, data = daily_3yr_narm)
summary(year_aov)
TukeyHSD(year_aov, which = "year")
#sightings differ significantly btwn 2021 and 2020/2019, but not between 2019 and 2020 (although p-value is lower than in the GLM because time of survey is not being taken into account in the anova)

emmeans(year_glm_best, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average sightings = 1.12 at that time based on the model?)

emmeans(year_glm_best, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average sightings = 1.12 at that time based on the model?)

allEffects(year_glm_best)
plot(allEffects(year_glm_best))
#nice little plot

emmip(year_glm_best, hour ~ year)
emmip(year_glm_best, day ~ year)

daily_3yr_narm %>% 
  filter(year == 2019) %>% 
  summarize(mean = mean(transect_total))
# 2019 mean = 0.54321

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(transect_total))
# 2020 mean = 1.17143

daily_3yr_narm %>% 
  filter(year == 2021) %>% 
  summarize(mean = mean(transect_total))
# 2021 mean = 2.82171
```
* 2019-2020 contrast isn't significant in either the simple (year-only) AOV or in the year + day + hour glm. This suggests that other factors are driving increased sightings from 2019 to 2020. We know that time of day and day of year explain sightings, so we should test if these factors vary significantly between years - if they do, they may be driving the difference in shark sightings between 2019-2020

### Are time of day and day of year different between years?
```{r}
time_btwn_yrs_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(time_btwn_yrs_aov)
TukeyHSD(time_btwn_yrs_aov)
#2019 time of day significantly earlier (by about 2.5 hrs)

day_btwn_yrs_aov <- aov(day ~ year, data = daily_3yr_narm)
summary(day_btwn_yrs_aov)
TukeyHSD(day_btwn_yrs_aov)
#day of year not significantly different between 2019 and 2020, but 2021 was significantly earlier than 2019 (24 days) and 2020 (41 days)
```

### Nice timeseries plot where each year is overlaid
```{r}
daily_3yr_dat %>% 
  filter(!is.na(total_unique)) %>% 
  ggplot(aes(x = day, y = rollmean(total_unique, 14, na.pad = TRUE, align = "right"))) +
    geom_line(aes(color = year), size = 1) +
    scale_color_brewer(palette = "Dark2") +
  labs(x = "Month", y = "Count", color = "Year") +
  scale_x_continuous(
  breaks = lubridate::yday(seq(as.Date("2019-01-01"), 
                               by = "1 month", length.out = 12)), 
  labels = month.abb) +
  scale_y_continuous(breaks = c(0,2,4,6,8,10)) +
  theme_cowplot() +
  theme(panel.background = element_rect(fill = "white", colour = NA),
      plot.background = element_rect(fill = "white", colour = NA))

ggsave("plots/abund_years_overlaid.png", width = 7, height = 4)
```

### Final modeling procedure - full survey window, all sightings (work on this started 4/3/22)
```{r}
year_total_glm <- glmmTMB(total_unique ~ year + day + hour + offset(log(total_area_km2)), family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")
#log(area) is used because nbinom1 uses a log link, so total_unique is on the log scale

summary(year_total_glm)

yr_res <- simulateResiduals(year_total_glm)
plot(yr_res)
##minor issue with quantiles in global model. Don't think it is a problem based on what I've read about that test

dredge(year_total_glm)

#like with transect-only, best model to explain total sightings includes year, day, and hour of survey, but not area surveyed - if this were a predictive model, I would keep area in, but since it doesn't contribute additional explanatory power to the patterns that I already saw, leaving it out will give us a better understanding of the effects of year, day, and hour:

year_total_glm_best <- glmmTMB(total_unique ~ year + day + hour, family = nbinom1, data = daily_3yr_narm, na.action = "na.fail")

summary(year_total_glm_best)

#diagnose best model:
yr_tot_bst_res <- simulateResiduals(year_total_glm_best)
plot(yr_tot_bst_res)
#passes DHARMa checks
```

### Effects testing of full model (4/3)
```{r}
Anova(year_total_glm)
# year, day, and hour have significant effects on density (which we already know from the model selection process)

year_tot_emms <- emmeans(year_total_glm, "year")
pwpm(year_tot_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison
#like transect count, 2019-2020 contrast is not significant for total density

#compare to ANOVA (sanity check):
year_total_aov <- aov(total_unique ~ year, data = daily_3yr_narm)
summary(year_total_aov)
TukeyHSD(year_total_aov, which = "year")
#sightings differ significantly between all years

emmeans(year_total_glm, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average density = 2.02 at that time based on the model?)

emmeans(year_total_glm, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average density = 2.02 at that time based on the model?)

allEffects(year_total_glm)
plot(allEffects(year_total_glm))

emmip(year_total_glm, hour ~ year)
emmip(year_total_glm, day ~ year)
#idk what these are doing, they look exactly the same

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_dens))
# 2019 mean = 3.53
# 2020 mean = 5.12
# 2021 mean = 12.8

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(total_dens))
# no idea why this stopped working (also stopped working in transect-only chunk above)
```
* Best model is year + day + hour with no offset for area surveyed
* adding area surveyed increases AIC

### Effects testing of final "year" GLM - full survey window, all sightings
```{r}
Anova(year_total_glm_best)
# year, day, and hour have significant effects on sightings (which we already know from the model selection process)

year_total_emms <- emmeans(year_total_glm_best, "year")
pwpm(year_total_emms, type = "response") #on the response, not the log, scale
#I really like this summary matrix for year - includes emmeans for each level, comparisons of said estimates between levels, and tukey p-values for each pairwise comparison
#UNLIKE transect only, 2019-2020 contrast is highly significant

#compare to ANOVA (sanity check):
year_total_aov <- aov(total_unique ~ year, data = daily_3yr_narm)
summary(year_total_aov)
TukeyHSD(year_total_aov, which = "year")
#sightings differ significantly between all years

emmeans(year_total_glm_best, "hour", type = "response")
#this is just saying that average time of survey is at 11:48 AM (and average sightings = 1.66 at that time based on the model?)

emmeans(year_total_glm_best, "day", type = "response")
#this is just saying that average day of survey is August 26th (ish) (and average sightings = 1.66 at that time based on the model?)

allEffects(year_total_glm_best)
plot(allEffects(year_total_glm_best))
#nice little plot, biggest difference from transect-only is that 2020 is now significantly higher than 2021

emmip(year_total_glm_best, hour ~ year)
emmip(year_total_glm_best, day ~ year)
#idk what these are doing, they look exactly the same

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean = mean(total_unique))
# 2019 mean = 0.543
# 2020 mean = 2.4
# 2021 mean = 4.32

daily_3yr_narm %>% 
  filter(year == 2020) %>% 
  summarize(mean = mean(total_unique))
# no idea why this stopped working (also stopped working in transect-only chunk above)
```

### Comparing survey windows
```{r}
nrow(dat_2019)
first(dat_2019$date)
last(dat_2019$date)
#107 survey days in 2019 from May 30 to November 22

nrow(dat_2020)
first(dat_2020$date)
last(dat_2020$date)
#135 survey days in 2020 from May 20 to December 11

nrow(dat_2021)
first(dat_2021$date)
last(dat_2021$date)
#142 survey days in 2021 from April 20 to December 18
```
* I definitely *could* do analyses with only the overlapping window but since day is included I think it makes more sense to keep all of the data that I have

### Copying above approach but with GAM - probably won't use GAM for year analysis
```{r}
gam_dat <- daily_3yr_dat %>% 
  filter(!is.na(transect_total)) %>% 
  filter(!is.na(year)) %>% 
  mutate(transect_total = as.numeric(transect_total)) %>% 
  mutate(year = as.factor(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour)))

year_gam <- mgcv::gam(transect_total ~ year + s(hour) + s(day) + offset(transect_area_km2), data = gam_dat)

summary(year_gam)
# edf for "hour" = 1, so it's being modeled as linear.

mgcv::gam.check(year_gam)
plot(year_gam, residuals = TRUE, pch = 1)

year_lm <- lm(transect_total ~ year, data = gam_dat)
anova(year_lm, year_gam)

hour_gam <- mgcv::gam(transect_total ~ s(hour), data = gam_dat)
summary(hour_gam)
mgcv::gam.check(hour_gam)
plot(hour_gam, residuals = TRUE, pch = 1)
```

### Year is categorical so it doesn't make sense to use GAM unless hour or day has a non-linear effect on count:
```{r}
ggplot(gam_dat, aes(x = hour, y = total_unique))+
  geom_jitter() +
  geom_smooth()

ggplot(gam_dat, aes(x = day, y = transect_total))+
  geom_jitter() +
  geom_smooth()
```
* no strong evidence that I should favor a GAM here

### Do manual counts vary across the field season?
```{r}
#adding day to the year model:
daily_3yr_narm_manual <- daily_3yr_dat %>% 
  filter(!is.na(manual_unique))
season_glm_man <- glmmTMB(manual_unique ~ year*day, family = nbinom1, data = daily_3yr_narm_manual, na.action = "na.fail")
summary(season_glm_man)

dredge(season_glm_man)
# day + year, day*year, and day-only are all equally weighted for manual count model
```

## Size 

### first off, a justification of ASL and Depth corrections:
```{r}
BOI_1_sub_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_submerged_JP_calculations.csv")
View(BOI_1_sub_dat)

BOI_1_sub_dat$test_alt <- as.factor(as.character(BOI_1_sub_dat$test_alt))

BOI_1_sub_dat %>%
  pivot_longer(cols = c(5:7), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = 1.97, linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comp_submerged.png", width = 10)
#visually apparent that at 20m, correcting for both ASL and depth get us the closest when target is submerged 1.5 m

BOI_1_surface_dat <- read.csv("data_raw/csulb_tests_nov_14/BOI_1_surface_JP_calculations.csv")
View(BOI_1_surface_dat)

BOI_1_surface_dat$test_alt <- as.factor(as.character(BOI_1_surface_dat$test_alt))

BOI_1_surface_dat %>%
  pivot_longer(cols = c(5:8), names_to = "height_type", values_to = "length") %>%
  filter(method == "video") %>% 
  ggplot(aes(x = test_alt, y = length, fill = height_type)) +
    geom_boxplot() +
    geom_hline(yintercept = c(1.97, 2.93), linetype = 2, color = "red")
ggsave("plots/calibration_tests/height_comps_surface.png", width = 10)
#visually apparent that ASL correction gets us closer to true size at 20m (no depth correction for surface target)
```

### determining accuracy and error margins from above tests:
```{r}
sub_dat_20 <- BOI_1_sub_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(sub_dat_20$size_asl_plus_depth)
mean_size <- mean(sub_dat_20$size_asl_plus_depth)
std_err <- sd(sub_dat_20$size_asl_plus_depth)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01136 m

#95% ci:
mean_size - margin_of_error #1.925 m
mean_size + margin_of_error #1.948 m

known_size_pvc <- 1.97

sub_dat_20 %>% 
  mutate(error = size_asl_plus_depth - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average size estimate of pvc target at 1.5m depth was 0.0335 m too small 

sub_dat_20 %>% 
  mutate(error = (size_asl_plus_depth - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for submerged target is 1.702% with an sd of 1.545% - this is pretty good!

sub_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#average UNADJUSTED size estimate of pvc target at 1.5m depth was 0.252 m too small!

sub_dat_20 %>% 
  mutate(error = (size_pvc - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
# mean error for UNADJUSTED submerged target is 12.799% with an sd of 1.357%
# adjustments reduce mean error by over 11%!

surf_dat_20 <- BOI_1_surface_dat %>%
  filter(method == "video") %>% 
  filter(test_alt == "20")

n <- length(surf_dat_20$size_pvc_asl)
mean_size <- mean(surf_dat_20$size_pvc_asl)
std_err <- sd(surf_dat_20$size_pvc_asl)/sqrt(n)
crit_val <- qt(0.975, df=(n-1))
margin_of_error <- std_err * crit_val
margin_of_error # 0.01032 m

#95% ci:
mean_size - margin_of_error #1.989 m
mean_size + margin_of_error #2.009 m

known_size_pvc <- 1.97

surf_dat_20 %>% 
  mutate(error = size_pvc_asl - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#size estimate of pvc target at surface was 0.029 m too large on average

surf_dat_20 %>% 
  mutate(error = (size_pvc_asl - known_size_pvc)/known_size_pvc) %>% 
  summarize(mean_error = mean(error), sd_error = sd(error))
#mean error is 1.466%, with an sd of 1.403%

surf_dat_20 %>% 
  mutate(error = size_pvc - known_size_pvc) %>% 
  summarize(mean_error = mean(error))
#UNADJUSTED size estimate of pvc target at surface was 0.0725 m too big on average
```
* I have SE, margin of error, confidence intervals, and average error - not sure which of these metrics is best to report

### average size of each depth category
```{r}
size_dat_2021 %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj > 1) %>% 
  summarize(mean_surface = mean(length_adj[depth == "surface"]),
            mean_shallow = mean(length_adj[depth == "shallow"]),
            mean_deep = mean(length_adj[depth == "deep"]))

#significant difference?

depth_aov_dat <- size_dat_2021 %>% 
                  filter(unique == "Y") %>%
                  filter(!is.na(length_adj)) %>% 
                  filter(length_adj > 1) %>% 
                  filter(depth == "surface" | depth == "shallow" | depth == "deep")

shapiro.test(log(depth_aov_dat$length_adj))
#def not normal

depth_aov <- aov(log(length_adj) ~ depth, data = depth_aov_dat)

summary(depth_aov)
plot(depth_aov)

TukeyHSD(depth_aov)
```
*mean sizes not significantly different between depth categories, BUT deep is over a foot bigger on average

### histograms from 2021 of each depth category
```{r}
size_dat_2021 %>% 
  filter(depth == "surface") %>%
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>% 
  nrow()
#only 13 unique surface sharks

size_dat_2021 %>% 
  filter(!is.na(depth)) %>%
  filter(unique == "Y") %>%
  filter(!is.na(length_adj)) %>%
  nrow()
#out of 291 unique manual sightings

size_dat_2021 %>% 
  filter(length_adj_m > 1) %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = length_adj)) +
    geom_histogram() +
    geom_vline(xintercept = 9.8, lty = 2, size = 1, color = "red") +
    geom_vline(xintercept = 8, lty = 2, size = 1, color = "blue") +
    theme_bw() +
    scale_x_continuous(breaks = c(4,6,8,10,12,14,16,18,20)) +
    labs(x = "Length (ft)", y = "Count")

size_dat_2021 %>% 
  filter(depth == "surface") %>% 
  ggplot(aes(x = length_raw_m)) +
    geom_histogram()
#this isn't going to be very helpful for comparing between depths

size_dat_2021 %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), position = "dodge") +
    facet_wrap(~depth)
#this stinks

#how do adjustments affect size distribution?
size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj)) %>%
  filter(length_adj_m > 1) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "Measure", values_to = "Length") %>% 
  ggplot(aes(x = Length)) +
    geom_histogram(aes(fill = Measure, color = Measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    scale_color_brewer(palette = "Dark2", labels=c("Adjusted","Unadjusted")) +
    theme_cowplot() +
    theme(legend.position = "bottom", legend.spacing.x = unit(1, 'cm'),
          panel.background = element_rect(fill = "white", colour = NA),
          plot.background = element_rect(fill = "white", colour = NA)) +
    guides(fill = guide_legend(label.position = "bottom")) +
    labs(x = "Length (m)", y = "Count", fill = "Measure:", color = "Measure:")
ggsave("plots/2021_size_adjustment_comp.png", height = 7, width = 4.6)
#shows how peaks in raw get shifted to the right when the adjustment is made. looks cool when narrow. Probably a supplementary figure

size_dat_2021 %>% 
  filter(unique == "Y") %>% 
  filter(!is.na(length_adj_m)) %>% 
  filter(!is.na(length_raw_m)) %>% 
  summarize(median_adj = median(length_adj_m), median_raw = median(length_raw_m))
#increase in median size of approximately 0.3 meters (about a foot)

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average adjustment was an increase of 0.29 meters

#looking at how adjustments affect juvie:adult ratio with the 3m cutoff
size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>%
  mutate(class_raw = ifelse(length_raw_m < 3, "Juvenile", "Adult")) %>% 
  mutate(class_adj = ifelse(length_adj_m < 3, "Juvenile", "Adult")) %>% 
  count(class_adj)
#94 adults and 189 juveniles with raw measurements, 143 adults and 140 juveniles with adjusted measurements - adjustments bring 49 sharks from juvenile to adult length

#looking at how adjustments affect shallow sharks (overlapping histograms):
size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure, color = measure), alpha = .5, position = "dodge", binwidth = 0.1) +
    scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2")
# I actually like this dodged the most, shows how peaks in raw get shifted to the right

# same as above but faceted:
size_dat_2021 %>% 
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj)) %>% 
  pivot_longer(c(length_raw_m, length_adj_m), names_to = "measure", values_to = "length_meters") %>% 
  ggplot(aes(x = length_meters)) +
    geom_histogram(aes(fill = measure), binwidth = 0.1) +
    facet_grid(measure~.) +
    scale_fill_brewer(palette = "Dark2")
# not sure what this really tells me. Overall smoother distribution for the adjusted lengths, which I suppose makes sense. Adjusted lengths are shifted to the right...

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>% 
  summarize(median_adj = median(length_adj_m), median_raw = median(length_raw_m))
#by approximately 0.24 meters, or about ten inches

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(depth == "shallow") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average shallow adjustment was an increase of 0.25 meters

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(depth == "surface") %>% 
  filter(!is.na(length_adj_m)) %>%
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average surface adjustment was an increase of 0.14 meters

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  filter(depth == "deep") %>% 
  filter(!is.na(length_adj_m)) %>%
  filter(!is.na(length_raw_m)) %>% 
  mutate(adj_diff = length_adj_m - length_raw_m) %>% 
  summarize(mean_adj_diff = mean(adj_diff))
#average deep adjustment was increase of 0.37 meters
```

## Modelling effects of environmental and detection-related variables
```{r}
gam_dat <- daily_3yr_dat %>% 
  filter(!is.na(transect_total)) %>%
  filter(!is.na(transect_area_km2)) %>% 
  filter(!is.na(year)) %>% 
  filter(!is.na(date_hour)) %>% 
  mutate(hour = as.numeric(hour(date_hour))) %>% 
  filter(!is.na(day)) %>% 
  #filter(!is.na(sky)) %>% 
  filter(!is.na(beaufort)) %>% 
  filter(!is.na(vis))
    
gam <- mgcv::gam(transect_total ~ year + s(hour, bs = "cc") + s(day, bs = "cc", k = 20) + s(beaufort, k = 5) + s(vis, k = 5) + offset(transect_area_km2), data = gam_dat)

summary(gam)
```

===============================================================================

# Exploratory Visualizations
* Older individual year visualizations are below 3yr visualizations

## Time of day (using data for year GLM)
```{r}
ggplot(daily_3yr_narm, aes(x = hour, y = transect_total)) +
  geom_jitter() #+
  #geom_smooth(method = "lm")

hour_lm <- lm(transect_total ~ hour, data = daily_3yr_narm)
summary(hour_lm)
#see 0.3 more sharks every hour (p < 0.0001)

hour_aov <- aov(hour ~ year, data = daily_3yr_narm)
summary(hour_aov)

daily_3yr_narm %>% 
  group_by(year) %>% 
  summarize(mean_time = mean(hour))
```

## 3yr abundance visualizations

Summary table
```{r}
yearly_abund_summary <- daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize("Total sightings" = sum(total_unique, na.rm = TRUE),
            "Survey days" = n(),
            "Sightings per day" = sum(total_unique, na.rm = TRUE)/n(),
            "Maximum abundance" = max(total_unique, na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(columns = 4, decimals = 2) %>% 
    cols_label("year(date)" = "Year") %>% 
    cols_width("Total sightings" ~ px(80),
               "Maximum abundance" ~ px(80),
               "Sightings per day" ~ px(80)) %>% 
    cols_align(align = "center")
gtsave(yearly_abund_summary, "plots/yearly_abund_summary.png")
```

Years stacked (pretty ugly):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity", position = "stack") +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_stacked.png", width = 8, height = 5)
```

Years faceted (good option, potentially as a second graph after years averaged?):
```{r}
daily_3yr_dat %>% 
  group_by(year(date), week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)", year = "year(date)") %>% 
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean, fill = as_factor(year))) +
    geom_bar(stat = "identity") +
    facet_grid(as_factor(year)~.) +
    scale_fill_brewer("Year", palette = "Set1") +
    theme_clean() +
    theme(strip.background = element_blank(),
          strip.text.y = element_blank(),
          panel.spacing.y = unit(1, "lines"),
          panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    expand_limits(y = 0) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month") +
    scale_y_continuous("Abundance", expand = c(0,0))
ggsave("plots/3yr_abund_yrs_facet.png", width = 8, height = 5)
```

Years averaged:
```{r}
daily_3yr_dat %>% 
  group_by(week(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(week = "week(date)") %>%
  ungroup() %>% 
  ggplot(aes(x = as.Date(paste(week, 1, sep="-"), "%U-%u"), y = mean)) +
    geom_bar(stat = "identity") +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Abundance") +
    theme_clean()
```

Rolling abundance:
WHAT LENGTH WINDOW DO I WANT?
```{r}
daily_3yr_dat %>%
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 10, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month")
ggsave("plots/3yr_rolling_abund_10day.png", width = 9, height = 5)
```

Rolling abundance, only including days within survey window for all three years:
```{r}
#find latest start and earliest stop dates:
daily_3yr_dat %>% 
  group_by(year(date)) %>% 
  summarize(min_day = min(yday(date)),
            max_day = max(yday(date)))
#2019 is latest start date (day 150) and 2021 is earliest stop date (day 312)

daily_3yr_dat %>%
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  group_by(yday(date)) %>% 
  summarize(mean = mean(total_unique, na.rm = TRUE)) %>%
  rename(day = "yday(date)") %>%
  ungroup() %>%
  filter(!is.na(mean)) %>% 
  ggplot(aes(x = as_date(day), y = rollmean(mean, 7, na.pad = TRUE, align = "right"))) +
    geom_line(size = 1) +
    theme_clean() +
    expand_limits(y = 0) +
    scale_y_continuous("Abundance", expand = c(0, 0), limits = c(0,6)) +
    scale_x_date("Date", date_labels = "%B", breaks = "1 month",
                 limits = c(as_date(155), as_date(315)))
    
ggsave("plots/3yr_rolling_abund_bounded_7day.png", width = 9, height = 3)
```

## 3yr environmental variable visualizations
```{r}
daily_3yr_dat %>% 
  gather("data", "value", WDIR:WTMP) %>% 
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~data, scales = "free")

daily_3yr_dat %>%
 ggplot(aes(x = wharf_temp, y = WTMP)) +
  geom_point()

wharf_channel_temp_mod <- lm(WTMP ~ wharf_temp, data = daily_3yr_dat)
summary(wharf_channel_temp_mod)
# r2 = 0.43, p < 0.001

daily_3yr_dat %>%
 ggplot(aes(x = ((9/5)*(wharf_temp) + 32), y = water_temp_padaro)) +
  geom_point() +
  geom_smooth(method = "lm") +
  coord_cartesian(xlim = c(58, 70))

wharf_padaro_temp_mod <- lm(((9/5)*(wharf_temp) + 32) ~ water_temp_padaro, data = daily_3yr_dat)
summary(wharf_padaro_temp_mod)
#r2 = 0.69, p < 0.001
```

# Correlations of Abundance with Environmental Variables

### Channel Water Temp
```{r}
dat_no_19 %>% 
  ggplot(aes(x = WTMP)) +
    geom_histogram(binwidth = 0.5)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WTMP, y = total_unique)) +
    geom_point()

temp_mod <- lm(log1p(total_unique) ~ WTMP, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
# no linear relationship between channel water temp and total unique
plot(temp_mod)

temp_mod_transect <- 

```

### Wharf Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = wharf_temp)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

temp_mod <- lm(log(total_unique) ~ wharf_temp, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ wharf_temp, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod)

#no linear relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, wharf_temp2 = (wharf_temp)^2)

temp_mod2 <- lm(log(total_unique) ~ wharf_temp + wharf_temp2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ wharf_temp + wharf_temp2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between wharf temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = wharf_temp, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = wharf_temp, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Padaro Water Temp
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = water_temp_bucket)) +
    geom_histogram(binwidth = 0.5)

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = log1p(total_unique))) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/bucket_lm_plot.png")

temp_mod <- lm(log(total_unique) ~ water_temp_bucket, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod)
plot(temp_mod)

temp_mod_0 <- lm(log1p(total_unique) ~ water_temp_bucket, data = daily_3yr_dat)
summary(temp_mod_0)
plot(temp_mod_0)

# significant linear relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat <- mutate(daily_3yr_dat, water_temp_bucket2 = (water_temp_bucket)^2)

temp_mod2 <- lm(log(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = (daily_3yr_dat %>% filter(total_unique != 0)))
summary(temp_mod2)

temp_mod2_0 <- lm(log1p(total_unique) ~ water_temp_bucket + water_temp_bucket2, data = daily_3yr_dat)
summary(temp_mod2_0)

#no quadratic relationship between padaro temp and total unique (with or without total = 0)

daily_3yr_dat %>%
  filter(year(transect_datetime) == 2020) %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(transect_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = transect_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

daily_3yr_dat %>% 
  filter(manual_total_density != 0) %>% 
  ggplot(aes(x = water_temp_bucket, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "loess")
```

### Swell
```{r}
daily_3yr_dat %>% 
  ggplot(aes(x = WVHT_m)) +
    geom_histogram(binwidth = 0.1)

dat_no_19 %>%
  ggplot(aes(x = total_unique)) +
    geom_histogram(binwidth = 1)

dat_no_19 %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = WVHT_m, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
ggsave("plots/wvht_lm_plot.png")

wvht_mod <- lm(log1p(total_unique) ~ log(WVHT_m), data = (dat_no_19))
summary(wvht_mod)
# highly significant correlation between wave height and abundance, r2 = 0.15
#slope is negative (see predictions below)
plot(wvht_mod)

shapiro.test(dat_no_19 %>% mutate(total_unique = log1p(total_unique)) %>% .$total_unique)
# not normal even w/ log transformation
dat_no_19 %>% 
  filter(total_unique > 0) %>% 
  #mutate(total_unique = log10(total_unique)) %>% 
  ggplot(aes(x = total_unique)) +
    geom_histogram()

shapiro.test(dat_no_19 %>%  mutate(WVHT_m = log(WVHT_m)) %>% .$WVHT_m)
# normal w/ log transformation

wvht_sim <- simulateResiduals(fittedModel = wvht_mod, plot = F, n = 1000)
testResiduals(wvht_sim, plot = T)
#looks good
predict(wvht_mod, data.frame(WVHT_m = 1))
# 0.5 m swell = 1.41 individuals, 1 m swell = 0.78, 1.5 m swell = 0.40, 2 m swell = 0.14

daily_3yr_dat %>% 
  ggplot(aes(x = MWD)) +
    geom_histogram(binwidth = 5)

dat_no_19 %>% 
  ggplot(aes(x = MWD, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

wvdir_mod <- lm(log1p(total_unique) ~ MWD, data = (dat_no_19))
summary(wvdir_mod)
```

### Other
```{r}
daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = pressure, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  #filter(total_unique != 0) %>% 
  ggplot(aes(x = salinity, y = total_unique)) +
    geom_point()

daily_3yr_dat %>% 
  filter(total_unique != 0) %>% 
  ggplot(aes(x = chlorophyll, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
```


## 2019 Time Series of Transect Data with 8 ft. cutoff
```{r}
dat_2019 %>% 
  dplyr::slice(1:103) %>%
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>%
  filter(transect_total != 0) %>% 
  dplyr::select(Date, Small, Large) %>% 
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2019-05-30")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) +
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "1 month", date_labels = "%b %e",
                 limits = c(as.Date("2019-08-15"), as.Date("2019-12-06")))+
    scale_y_continuous(breaks = 1:7, limits = c(-0.1, 10)) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + 
    theme(plot.title = element_text(size=40), legend.text = element_text(size = 16),
          axis.text = element_text(size = 16)) +
    labs(title = "2019 Great White Shark Abundance, 5/30 - 11/22")
ggsave("plots/wsj_abund_plot_2019.png", width = 16, height = 9)
```

## Effort over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = manual_area_km2)) +
    geom_bar(stat = "identity")
```

## Abundance Over Time, 2020
```{r}
dat_2020 %>% 
  ggplot(aes(x = date, y = total_unique)) +
    geom_point() +
    geom_smooth()

dat_2020 %>% 
  dplyr::rename(Date = date, Small = small_unique, Large = large_unique) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  gather("Size", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Size = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Size), stat = "identity") +
    scale_x_date(date_breaks = "3 weeks", date_labels = "%b %e",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = 1:12) +
    scale_fill_manual(values = c("black", "red3", "dodgerblue4"), "") + theme_wsj() + theme(plot.title = element_text(size = 40), legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - 12/18")

ggsave("plots/wsj_abund_plot.png", width = 16, height = 9)

#rolling average, right-aligned 7-day window, total transect density
dat_2020 %>%
  dplyr::rename(Date = date, Transects = transect_total) %>% 
  dplyr::select(Date, Transects) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Transects)) %>%
  ggplot(aes(x = Date, y = rollmean(Transects, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(color = "red3", size = 2) +
    scale_x_date(date_breaks = "2 weeks", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2.5, by = 0.5)) +
    theme_classic() +
    theme(legend.text = element_text(size = 16), axis.text = element_text(size = 12)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1))
# 0.17 is survey area in km^2 and 9.6 is duration of survey in minutes

ggsave("plots/2020_abund_plot.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (used for GRFP, see updated version below)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-10-10")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 10/10/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))
ggsave("plots/grfp_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, size-classed transect density (all 2020 data)
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25)) +
    theme_classic() +
    theme(legend.title = element_text(size = 14),
          legend.text = element_text(size = 14),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.caption = element_text(size = 14, hjust = 0.2)) +
    labs(y = bquote('Individuals'%.%'km'^-2%.%'min'^-1), caption = "Figure 1. Seven-day rolling mean of size-classed density of white sharks at \n              Padaro Beach aggregation zone from 6/27/2020 to 12/11/2020.") +
    scale_color_manual(labels = c(bquote(''>='8 ft'), bquote(''< '8 ft')),values = c("red3", "dodgerblue4"))

ggsave("plots/2020_abund_plot_sized.png", width = 7, height = 3)

#rolling average, right-aligned 7-day window, small sharks only
dat_2020 %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  slice(23:nrow(dat_2020)) %>%
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  filter(Size == "Small") %>% 
  ggplot(aes(x = Date, y = rollmean(Abundance, 7, na.pad = TRUE, align = "right")/(0.17*9.6))) + 
    geom_line(aes(color = Size), size = 1.5) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b %y",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))+
    scale_y_continuous(breaks = seq(from = 0, to = 2, by = 0.25))

#plotting time series of abundance by size, excluding days with no sharks
dat_2020 %>% 
  dplyr::filter(transect_total != 0) %>% 
  dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
  dplyr::select(Date, Small, Large) %>% 
  filter(!is.na(Small)) %>%
  gather("Size", "Abundance", 2:3) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_point(aes(color = Size), size = 1.5) +
    geom_smooth(aes(color = Size), method = "lm", se = FALSE) +
    scale_x_date(date_breaks = "4 weeks", date_labels = "%e %b",
                 limits = c(as.Date("2020-06-27"), as.Date("2020-12-11")))
```

## Basic size class visualizations
```{r}
#View(size_classed_2020)

sum(size_classed_2020$juvenile)
#233 juvenile/YOY
sum(size_classed_2020$adult)
#29 adult/sub-adult

dat_2020_final %>% 
  gather("Sizeclass", "Abundance", juvenile:adult_density) %>% 
  ggplot(aes(x = Sizeclass, y = Abundance)) +
    geom_bar(stat = "identity")

sum(dat_2020_final$juvenile, na.rm = TRUE)/sum(dat_2020_final$adult, na.rm = TRUE)
# Eight times as many juveniles as adults

sum(dat_2020_final$juvenile_density, na.rm = TRUE)/sum(dat_2020_final$adult_density, na.rm = TRUE)
# Juveniles have 8.5 times the average density
```

## Size-classed time series (Abundance and Density)
```{r}
dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult), stat = "identity", fill = "red4") +
    labs(y = "Abundance", x = "Date")

dat_2020_final %>% 
  ggplot(aes(x = week(datetime.x))) +
    geom_bar(aes(y = juvenile_density), stat = "identity", fill = "dodgerblue") +
    geom_bar(aes(y = adult_density), stat = "identity", fill = "red4") +
    labs(y = "Density (individuals/km2/min)", x = "Date")
```

## Morning vs afternoon abundance
```{r}
daily_3yr_dat %>%
  filter(hh < 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

daily_3yr_dat %>% 
  filter(hh >= 12) %>% 
  drop_na(total_unique) %>% 
  dplyr::summarize(mean = mean(total_unique), days = n()) 

dat_2020 %>% 
  ggplot(aes(x = hms::as_hms(datetime), y = total_unique)) +
    geom_point()

dat_2020 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(bins = 6, color = "white")
ggsave("plots/2020_time_histogram.png", width = 7, height = 5)

dat_2019 %>% 
  ggplot(aes(x = hour(datetime))) +
    geom_histogram(binwidth = 1, color = "white")
ggsave("plots/2019_time_histogram.png", width = 7, height = 5)

View(dat_2021)

daily_3yr_dat %>%
  #filter(month(datetime) > 6, month(datetime) < 8) %>% 
  dplyr::group_by(hh) %>%
  drop_na(total_unique) %>%
  dplyr::summarize(Abundance = list(mean_se(total_unique))) %>%
  unnest(Abundance) %>% 
  dplyr::rename(Hour = 1, Abundance = y) %>% 
  ggplot(aes(x = Hour, y = Abundance)) +
    geom_bar(stat = "identity") +
    #geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 0.5, width = 0.2) +
    geom_vline(xintercept = 11.5, size = 1) +
    annotate("text", label = "Morning mean: 1.66 (n = 82)", x = 9, y = 3.5) +
    annotate("text", label = "Afternoon mean: 3.19 (n = 70)", x = 15, y = 5.2) +
    theme_classic()
ggsave("plots/3yr_am_vs_pm.png", width = 7, height = 5)
```

## Channel water temperature vs manual density, 2020
```{r}
flights_plus_channel_dat_2020 %>%
  #filter(manual_total_density != 0) %>% 
  ggplot(aes(x = mean_wtemp_previous_24, y = manual_total_density)) +
    geom_point() +
    geom_smooth(method = "lm")

wtemp_quadratic <- lm(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2)

summary(wtemp_quadratic)

plot(dat_2020_full$mean_wtemp_previous_24, fitted(dat_2020_full$manual_total_density ~ dat_2020_full$mean_wtemp_previous_24 + dat_2020_full$wtemp_2))
```

## 2020 Tide Visualizations
```{r}
tide_added_2020 %>% 
  ggplot(aes(x = tide.y)) +
    geom_histogram()

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")

tide_added_2020 %>% 
  ggplot(aes(x = tide.y, y = small_unique/total_unique)) +
    geom_point() +
    geom_smooth(method = "lm")
#huh

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_freqpoly()
#bimodal distribution of age structure

tide_added_2020 %>% 
  ggplot(aes(x = small_unique/total_unique)) +
    geom_histogram(bins = 10)
```

## 2020 Size-Classed Visualizations
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(length)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    stat_function(fun = function(x) 
    dnorm(x, mean = mean(size_dat_2020$length), sd = sd(size_dat_2020$length)) * 0.2 * sum(!is.na(size_dat_2020$length)))

size_dat_2020 %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram(binwidth = 0.2, boundary = 0, closed = "left") +
    geom_vline(xintercept = 3, size = 1.5, color = "red3", linetype = "dashed") +
    geom_vline(xintercept = mean(size_dat_2020$length_m, na.rm = TRUE),
               size = 1.5, color = "dodgerblue") +
    theme_classic() +
    scale_x_continuous(name = "Length (m)", breaks = seq(from = -1, to = 5, by = 0.5))
ggsave("plots/2020_size_histogram.png", width = 7, height = 6)

size_dat_2020 %>%
  drop_na(unique, size_class) %>% 
  filter(unique == "Y") %>% 
  ggplot(aes(x = week(date), fill = size_class)) +
    geom_bar(position = "stack", stat = "count") +
    scale_fill_manual(values = c("red3", "dodgerblue")) +
    scale_x_continuous(name = "Week") +
    theme_classic()
ggsave("plots/2020_size_classed_time_series.png", width = 7, height = 4)
```


```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
Mode(size_dat_2020$length)

size_dat_2020 %>% 
  slice(1:201) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(1:201) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
   ggplot(aes(x = length))+
    geom_histogram()

size_dat_2020 %>% 
  slice(202:nrow(size_dat_2020)) %>% 
  summarize(mean = mean(length, na.rm = TRUE))

dat_2020 %>% 
  slice(1:86) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/86)

dat_2020 %>% 
  slice(86:nrow(dat_2020)) %>% 
  summarize(daily_avg = sum(total_unique, na.rm = TRUE)/((nrow(dat_2020))-86))
```

## Size over Time

### Individual years
```{r}
size_dat_2020 %>%
  drop_na(length) %>%
  filter(unique == "Y") %>% 
  dplyr::group_by(date) %>% 
  dplyr::summarize(mean_length = mean(length_m)) %>% 
  ggplot(aes(x = date, y = mean_length)) + 
    geom_point() +
    geom_smooth()

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = date, y = length_m)) + 
    geom_point() + 
    geom_smooth

#2021:
size_dat_2021 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = date, y = length_m)) + 
    geom_point() +
    geom_smooth(method = "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression.png", width = 5, height = 5)

size_lm_2021 <- lm(length_m ~ date, data = size_dat_2021)
summary(size_lm_2021)

#2021, quadratic:
size_dat_2021 <- mutate(size_dat_2021, date2 = (as.numeric(date))^2)

size_dat_2021 %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = date, y = length_m)) + 
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)")
ggsave("plots/2021_size_regression_quad.png", width = 5, height = 5)

size_lm_2021_quad <- lm(length_m ~ date + date2, data = size_dat_2021)
summary(size_lm_2021_quad)
```

### Comparing size across years

Histogram of Daily Age Structure:
```{r}
size_classed_3yr %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#no 2019:
size_classed_3yr %>%
  dplyr::slice(-c(1:19)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2020 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:19, 71:144)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()

#2021 only:
size_classed_3yr %>%
  dplyr::slice(-c(1:70)) %>% 
  mutate(prop = juvenile/(juvenile + adult)) %>% 
  ggplot(aes(x = prop)) +
    geom_histogram()
ggsave("plots/adult_swim.png")
#VERY pronounced bimodal distribution in 2021
#despite adults being just 40% of observations, there are MORE days where we saw only adults than where we saw only juveniles
#WHAT factors determine "adult swim" days???
```

Histogram faceted by year:
```{r}
size_dat_unique %>% 
  ggplot(aes(x = length, fill = as.factor(year(date)))) +
    geom_histogram() +
    facet_grid(rows = vars(year(date))) +
    theme_clean() +
    labs(y = "Count", x = "Length (m)", fill = "Year") +
    theme(strip.text.y = element_blank()) +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_histo.png", height = 5, width = 5)
```

looking at one month only:
```{r}
size_dat_unique %>% 
  filter(month(date) == 8) %>% 
  ggplot(aes(x = yday(date), y = length)) +
    geom_point() +
    facet_grid(cols = vars(year(date)))
```

Years overlaid:
```{r}
size_dat_unique %>% 
  ggplot(aes(x = as_date(yday(date)), y = length)) +
    geom_point(aes(color = as.factor(year)), alpha = 0.8) +
    geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Set1")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)
```

years overlaid, separate regressions:
```{r}
size_dat_unique %>% 
  ggplot(aes(x = as_date(yday(date)), y = length, color = as.factor(year))) +
    geom_point(alpha = 0.2) +
    geom_smooth(method= "lm", se = F) +
    theme_clean() +
    scale_x_date(date_labels = "%b", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)", color = "Year") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_scatter_regressions.png", height = 5, width = 5)

#years overlaid, windowed:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as_date(yday(date)), y = length)) +
    geom_point() +
    #geom_smooth() +
    theme_clean() +
    scale_x_date(date_labels = "%B", breaks = "1 month") +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
    #geom_hline(yintercept = 3)
ggsave("plots/size_3yrs_scatter.png", height = 5, width = 9)

#years separated, violin plot, July-September:
size_dat_unique %>% 
  filter(month(date) == c(7:9)) %>% 
  ggplot(aes(x = as.factor(year), y = length)) +
    geom_violin(fill = "dodgerblue") #+
    #geom_hline(yintercept = 3)
ggsave("plots/size_violin_july-sept.png")

#years separated, box plot:
size_dat_unique %>% 
  filter(length < 5) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    theme(panel.background = element_blank(),
          plot.background = element_blank(),
          legend.background = element_rect(fill = 'transparent'),
          panel.border = element_blank()) +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box.png", width = 4, height = 5)

year_size_mod <- aov(length ~ as.factor(year(date)), data = size_dat_unique)
summary(year_size_mod)
plot(year_size_mod)
TukeyHSD(year_size_mod)
```

```{r}
#years separated, overlap window, box plot:
size_dat_unique %>% 
  filter(yday(date) > 149 & yday(date) < 313) %>% 
  ggplot(aes(x = as.factor(year(date)), y = length, color = as.factor(year(date)))) +
    geom_boxplot(show.legend = FALSE) +
    theme_clean() +
    labs(x = "Date", y = "Length (m)") +
    scale_color_brewer(palette = "Set1")
ggsave("plots/size_3yrs_box_windowed.png", width = 4, height = 5)

#size summary table in FEET
size_dat_unique %>% 
  group_by(year) %>% 
  summarize(min = min(length_ft, na.rm = TRUE),
            max = max(length_ft, na.rm = TRUE),
            median = round(mean(length_ft, na.rm = TRUE), digits = 1),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = sum(size_class == "Juvenile", na.rm = TRUE)/
                                    sum(size_class == "Adult", na.rm = TRUE))

#size summary table in METERS, percentage adults
yearly_size_summary <- size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Percent Adults" = 
              sum(size_class == "Adult", na.rm = TRUE)/
                (sum(size_class == "Juvenile", na.rm = TRUE) +
                 sum(size_class == "Adult", na.rm = TRUE))) %>%
  gt() %>%
    fmt_number(2:4, decimals = 1) %>% 
    fmt_number(7, decimals = 2) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range") %>% 
    cols_width("Percent Adults" ~ px(100)) %>% 
    cols_align(align = "center")
yearly_size_summary
gtsave(yearly_size_summary, "plots/yearly_size_summary_table.png")

#size summary table in METERS, juveniles per adult
size_dat_unique %>% 
  group_by(year) %>% 
  summarize("Mean TL" = mean(length, na.rm = TRUE),
            min = min(length, na.rm = TRUE),
            max = max(length, na.rm = TRUE),
            "Juveniles" = sum(size_class == "Juvenile", na.rm = TRUE),
            "Adults" = sum(size_class == "Adult", na.rm = TRUE),
            "Juveniles per Adult" = 
              sum(size_class == "Juvenile", na.rm = TRUE)/
              sum(size_class == "Adult", na.rm = TRUE)) %>% 
  gt() %>% 
    fmt_number(7, decimals = 1) %>% 
    fmt_number(5:6, decimals = 0) %>% 
    cols_merge_range(3,4) %>% 
    cols_label("year" = "Year", "min" = "Range")
```

## Haphazard vs Transect Visualizations
```{r}
dat_2020 %>% 
  filter(total_unique > 0) %>% 
  filter(!is.na(manual_total), !is.na(transect_total)) %>% 
  dplyr::rename(Date = date, Haphazard = manual_total, Transect = transect_total) %>% 
  dplyr::select(Date, Haphazard, Transect) %>%
  gather("Flight", "Abundance", 2:3) %>%
  pad(interval = "day", start_val = as.Date("2020-06-24")) %>%
  replace_na(list(Flight = "DNF", Abundance = -0.1)) %>%
  ggplot(aes(x = Date, y = Abundance)) + 
    geom_bar(aes(fill = Flight), stat = "identity", position = "dodge") +
    scale_x_date(date_breaks = "1 week", date_labels = "%b %e", limits = c(as.Date("2020-06-27"), as.Date("2020-12-15")))+
    scale_y_continuous(breaks = 1:10) +
    scale_fill_manual(values = brewer.pal(8, "Dark2")[c(8,1,2)], "") + theme_wsj() + theme(plot.title = element_text(size=40), legend.text = element_text(size = 16), axis.text = element_text(size = 16)) +
    labs(title = "2020 Great White Shark Abundance, 6/24 - DD/MM")
ggsave("plots/wsj_survey_comp_plot.png", width = 16, height = 9)

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>% 
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>% 
  gather("Method", "Abundance", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Abundance = list(mean_se(Abundance))) %>%
  unnest(Mean_Abundance) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Abundance") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
      
ggsave("plots/3yr_methods_abundance.png", width = 7, height = 7)

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>% 
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>% 
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Density = list(mean_se(Density))) %>%
  unnest(Mean_Density) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
  
ggsave("plots/3yr_methods_density.png", width = 7, height = 7)

daily_3yr_dat %>%
  mutate(transect_frequency = transect_total/transect_duration_min,
         manual_frequency = manual_unique/manual_duration_min) %>% 
  select(transect_frequency, manual_frequency) %>%
  dplyr::rename(Transect = transect_frequency, Manual = manual_frequency) %>%
  gather("Method", "Frequency", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Freq = list(mean_se(Frequency))) %>%
  unnest(Mean_Freq) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Frequency (individuals/min)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))

# Density WITHOUT time:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens) %>% 
    ggplot(aes(x = Method, y = y, fill = Method)) +
      geom_bar(stat = "identity") +
      geom_errorbar(aes(ymax = ymax, ymin = ymin), size = 1.5, width = 0.2) +
      scale_fill_manual(values = c("red3", "dodgerblue")) +
      scale_y_continuous(name = "Mean Density (individuals/km2)") +
      theme_classic() +
      theme(legend.position = "none") +
      theme(axis.title = element_text(size = 14), axis.text = element_text(size = 14))
```

## Comparative Analysis of Methods
```{r}
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  t.test(Density ~ Method, .)
  
shapiro.test(daily_3yr_dat$transect_total_density) 
shapiro.test(daily_3yr_dat$manual_unique) 

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total_density)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_total_density)) +
    geom_density()

# Abundance
daily_3yr_dat %>%
  select(transect_total, manual_unique) %>%
  dplyr::rename(Transect = transect_total, Manual = manual_unique) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual abundance (mean = 2.3, SE = 0.20) was higher than transect abundance (mean = 1.3, SE = 0.14, p < 0.001)

# Effort-Corrected Density:
daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>% 
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>%
  mutate(se = ymax - y) %>% 
  View()

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  group_by(Method) %>%
  summarize(mean = list(mean_se(Density))) %>% 
  unnest(mean) %>% 
  rename("Density" = y) %>% 
  ggplot(aes(x = Method, y = Density)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.4)
ggsave("plots/methods_comp_plot.png")

daily_3yr_dat %>%
  select(transect_total_density, manual_total_density) %>%
  dplyr::rename(Transect = transect_total_density, Manual = manual_total_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)

# Manual effort-corrected density (mean = 0.64, SE = 0.1) was higher than transect abundance (mean = 0.48, SE = 0.06, p < 0.001)

# Raw Density:
dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>% 
  group_by(Method) %>% 
  dplyr::summarize(Mean_Dens = list(mean_se(Density))) %>%
  unnest(Mean_Dens)

daily_3yr_dat %>% 
  ggplot(aes(x = transect_total/transect_area_km2)) +
    geom_density()

daily_3yr_dat %>% 
  ggplot(aes(x = manual_unique/manual_area_km2)) +
    geom_density()

dat_2020 %>%
  mutate(transect_density = transect_total/transect_area_km2,
         manual_density = manual_unique/manual_area_km2) %>% 
  select(transect_density, manual_density) %>%
  dplyr::rename(Transect = transect_density, Manual = manual_density) %>%
  gather("Method", "Density", 1:2) %>%
  drop_na() %>%
  wilcox.test(Density ~ Method, ., conf.int = TRUE)
## Manual density (mean = 7.12, SE = 1.05) was not higher than transect density (mean = 6.78, SE = 0.06, p < 0.001)
```

## Heatmaps

### All 3 years
```{r}
size_dat_3yr %>%
  filter(video == "manual" | video == "manual_1") %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point(aes(color = size_class)) +
    geom_smooth()
#follows a certain distance from the coastline

size_dat_3yr %>%
  filter(video == "manual" | video == "manual_1") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 15) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405))
ggsave("plots/heatmap_3yrs.png", width = 10)

size_dat_3yr %>%
  filter(video == "inner" | video == "outer") %>%
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>%
  dplyr::filter(lat > 0) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 15) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    geom_point(aes(x = -119.56, y = 34.405))

View(size_dat_3yr)
```

### All Sharks, 2020
```{r}
size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>% 
  ggplot(aes(x = long, y = lat)) +
    geom_point(aes(color = size_class))

size_dat_2020 %>% 
  filter(unique == "Y") %>%
  filter(!is.na(length)) %>%
  ggplot(aes(x = long, y = lat)) +
  geom_hex(bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
ggsave("grfp_hexmap.png")

register_google(key = "[AIzaSyDmz6xT3R938U6s5VWXuguMSq6ROr3X2KM]", write = TRUE)

location <- c(-120, 34, -119, 35)

carp <- get_map(location = location, source = "osm", zoom = 7)

ggmap(carp) +
  geom_hex(data = size_dat_2020, aes(x = long, y = lat), bins = 12) +
  scale_fill_distiller(palette = "YlOrRd", direction = 1) +
  theme_classic()
```

### Juveniles, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Juvenile") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_juvenile_heatmap.png", width = 7, height = 6)
```

### Adults, 2020
```{r}
size_dat_2020 %>%
  drop_na(lat, size_class) %>%
  filter(size_class == "Adult") %>% 
  filter(unique == "Y") %>%
  ggplot(aes(x = long, y = lat)) +
    geom_hex(bins = 12) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1) +
    theme_classic() +
    theme(panel.background = element_rect(fill = "black"))
ggsave("plots/2020_adult_heatmap.png", width = 7, height = 6)

mean(size_dat_2020$length_m, na.rm = TRUE)
median(size_dat_2020$length, na.rm = TRUE)

sum(size_dat_2020$size_class == "Adult" & size_dat_2020$unique == "Y", na.rm = "TRUE")/sum(size_dat_2020$unique == "Y", na.rm = "TRUE")
```



# Modelling

## Factors affecting total manual density
```{r}
dat_no_19 <- daily_3yr_dat %>% 
  filter(year(transect_datetime) != 2019) %>% 
  mutate(hour = hour(transect_datetime))

total_density_mod <- glmmTMB(manual_unique ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = poisson, ziformula = ~1)

total_density_sim <- simulateResiduals(fittedModel = total_density_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

dredge(total_density_mod)
#time of day, wharf temp, and manual effort are all predictors in best fit model, model without hour has weight of 0.234

summary(total_density_mod)
#hour significant, wharf temp not

plot(allEffects(total_density_mod))
```

## Factors affecting ratio of juveniles to adults NOT WORKING
```{r}
dat_no_19 <- dat_no_19 %>% 
  mutate(hour = hour(transect_datetime))

View(dat_no_19)

ratio_mod <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp + hour, data = dat_no_19, family = binomial)

ratio_sim <- simulateResiduals(fittedModel = ratio_mod, plot = F, n = 1000)
testResiduals(ratio_sim, plot = T)

ratio_mod2 <- glmmTMB(cbind(manual_small, manual_large) ~ offset(log(manual_effort)) + wharf_temp, data = dat_no_19, family = binomial)

AICctab(ratio_mod, ratio_mod2)

dredge(ratio_mod)
#not working?

summary(ratio_mod)
# hour significant, wharf temp not
plot(allEffects(ratio_mod))

summary(ratio_mod2)
# higher AIC than temp + hour model

emmeans(ratio_mod)
```

## Modelling Transect vs Manual abundance estimates
```{r}


```

## Modelling Time of Day Effects on Abundance, 2020
```{r}
View(dat_2020_full)
ggplot(aes(x = total_unique), data = dat_2020) +
  geom_bar()

diel_model <- glmmTMB(total_unique ~ hour(datetime.x) + mean_wtemp_previous_24 + WVHT_m + vis + beaufort, data = dat_2020_full, family = nbinom2)
dredge(diel_model)

diel_mod_1 <- glmmTMB(total_unique ~ mean_wtemp_previous_24 + WVHT_m + vis, data = dat_2020_full, family = nbinom2)

fam_list <- list(family = alist(
    binomial = binomial,
    genpois = genpois,
    poisson = poisson,
    nbinom1 = nbinom1,
    nbinom2 = nbinom2,
    ))

getAllTerms(diel_mod_1)

dredge(diel_mod_1, fixed = ~ cond(mean_wtemp_previous_24) + cond(vis) + cond(WVHT_m), varying = fam_list)

diel_mod <- update(diel_mod_1, family = nbinom1)
dredge(diel_mod)

summary(diel_mod)

diel_simulationOutput <- simulateResiduals(fittedModel = diel_mod, n = 250)
plotSimulatedResiduals(simulationOutput = diel_simulationOutput)
testOverdispersion(diel_mod)

plot(allEffects(diel_model), partial.residuals = TRUE)
```

## Abundance Histogram, 2020
```{r}
dat_2020 %>% 
  #filter(total_unique != 0) %>%
  gather("Size", "Abundance", 20:21) %>% 
  ggplot(aes(x = Abundance, fill = Size)) +
    geom_bar(position = "dodge") +
    scale_x_continuous(breaks = 0:10) +
    theme_classic()
```

## Comparing size between methods
```{r}
joined_2020 %>% 
  filter(video.y == "outer") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on outer transect is 2.88

joined_2020 %>% 
  filter(video.y == "outer") %>%
  nrow()
# only 13 sharks seen in outer video?

joined_2020 %>% 
  filter(video.y == "inner") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
# mean size on inner transect is 2.49

joined_2020 %>% 
  filter(video.y == "inner") %>%
  nrow()
# 65 sharks in inner video

joined_2020 %>% 
  filter(video.y != "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size for transect flights overall is 2.55

joined_2020 %>% 
  filter(video.y == "manual") %>% 
  summarize(mean = mean(length_m, na.rm = TRUE))
#mean size on manual flights in 2.43 - bias towards larger sharks in transect flights?

#Comparing abundance estimates between methods:
aov_method_abund_dat <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      gather("method", "abundance", 2:3)

aov_method_abund_2_dat <- dat_2020 %>% 
                        dplyr::select(date, transect_total, haphazard_unique) %>% 
                        slice(23:nrow(dat_2020)) %>%
                        gather("method", "abundance", 2:3) %>% 
                        filter(!is.na(abundance))

aov_method_abund_dat_no_zero <- dat_2020 %>% 
                      dplyr::select(date, transect_total, manual_unique) %>% 
                      slice(23:nrow(dat_2020)) %>%
                      filter(!is.na(transect_total)) %>%
                      filter(!is.na(manual_unique)) %>%
                      filter(transect_total != 0) %>% 
                      filter(manual_unique != 0) %>%
                      gather("method", "abundance", 2:3)

pairwise.t.test(log1p(aov_method_abund_dat$abundance), aov_method_abund_dat$method)
# p = 0.017
# IMPORTANT: I'm not sure how the NAs affect this - First version removes all dates with an NA for either method, while second version only removes the method that has the NA. First version makes more sense to me.

leveneTest(log1p(abundance) ~ method, data = aov_method_abund_dat)
#approximately equal variances

qqPlot(log1p(aov_method_abund_dat$abundance))
# lol

aov_method_abund_dat %>% 
  ggplot(aes(x = abundance)) +
    geom_histogram()

pairwise.t.test(log1p(aov_method_abund_dat_no_zero$abundance), aov_method_abund_dat_no_zero$method)
#p = 0.042

leveneTest(abundance ~ method, data = aov_method_abund_dat_no_zero)
# fine

qqPlot(log(aov_method_abund_dat_no_zero$abundance))

shapiro.test(aov_method_abund_dat_no_zero$abundance)

#Comparing size estimates between methods:

size_dat_unique <- size_dat_unique %>% 
  mutate(method = ifelse(video == "manual", "manual", "transect"))

pairwise.t.test(size_dat_unique$length_m, size_dat_unique$method)
# p = 0.029

leveneTest(length_m ~ method, data = size_dat_unique)
#equal variances

qqPlot(size_dat_unique$length_m)
# lookin good

shapiro.test(size_dat_unique$length_m)
# hmm

size_dat_unique %>% 
  ggplot(aes(x = length_m)) +
    geom_histogram()
```


## Water temperature (old)
```{r}
mean(flights$water_temp, na.rm=T)

# number of days with each temp 
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# same as above but with channel temps 
ggplot(flights, aes(x=water_temp_channel))+
  geom_histogram(binwidth=0.5)+
  geom_vline(xintercept=64.90645, color="red")

# HOBO temp over time + shark data
temp_plot <- ggplot(flights, aes(x=date, y=water_temp, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp), color="blue")+
  geom_smooth(se=T, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
temp_plot
ggsave("temp_plot.png", width=10, height=7)

# same as above but with channel temps
ggplot(flights, aes(x=date, y=water_temp_channel, group=beach))+
  geom_line(size=0.5, linetype=5)+
  #geom_line(aes(y=water_temp_channel)) +
  geom_hline(yintercept=64.90645, color="red") +
  #geom_hline(yintercept=mean(flights$water_temp_channel), color="blue")+
  geom_smooth(se=F, color="blue", size=0.5)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()

# Air and water temperature over time
ggplot(temps.long, aes(x=date, y=temp, group=type, color=type))+
  geom_line()+
  geom_hline(yintercept=64.90645)

#Air vs channel water temperature
ggplot(flights, aes(x=air_temp, y=water_temp_channel))+
  geom_point()+
  coord_cartesian(xlim=c(55,80))+
  geom_hline(yintercept=64.90645, color="red")+
  stat_smooth(method=lm, se=F, fullrange=T)
```

## Wind
```{r}
wind_avg <- flights %>% ddply(.(wind_dir), summarize, mean=mean(wind_spd)) %>% na.omit(.)
#View(wind_avg)

# Average speed of each direction
ggplot(wind_avg, aes(x=wind_dir, y=mean))+
  geom_bar(stat="identity")+
  coord_polar(theta="x", start=2.7, direction=-1)

# Histogram of wind directions
flights %>% drop_na("wind_dir") %>%
  ggplot(., aes(x=wind_dir))+
    geom_bar()+
    coord_polar(theta="x",start=2.7, direction=-1)

# Wind vs water temp
ggplot(flights, aes(x=wind_spd..kts., y=water_temp))+
  geom_point()+
  geom_smooth(method="lm", se=F)

# Wind vs sky
ggplot(flights, aes(x=wind_spd, y=sky))+
  geom_count()

# Wind vs waves
ggplot(flights, aes(x=wind_spd, y=swell))+
  geom_jitter()+
  geom_smooth(method=lm)
```

## Sky
```{r}
# sky condition vs detections/DNF
ggplot(flights, aes(x=sky, fill=detection))+
  geom_bar(position="stack")+
  scale_fill_manual(values=c("black", "forestgreen", "red"))
```


## Visibility
```{r}
# beaufort vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(beaufort) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
  ggplot(aes(x = beaufort, y = y) )+
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)

# vis vs total abundance
dat_2020 %>% 
  slice(23:73) %>%
  group_by(vis) %>% 
  summarize(mean = list(mean_se(total_unique))) %>%
  unnest(mean) %>% 
ggplot(aes(x = vis, y = y) )+
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymax = ymax, ymin = ymin), width = 0.2)
```

## Tide
```{r}
# tide over time + shark data
ggplot(flights, aes(x=date, y=tide, group=beach))+
  geom_line(size=0.5, linetype=2)+
  geom_point(aes(color=detection, shape=detection), size=2) +
  scale_color_manual(values=c("black","forestgreen","red"))+
  scale_shape_manual(values=c(4,19,19))+
  theme_bw()
```

# Detection analysis

```{r}
#count(flights, detection)
ggplot(dat_2019, aes(y=wind_spd, x=detection))+
  geom_point()+
  geom_smooth(se=F)
```

## t-test of small vs large abundance, 2020
```{r}
aov_dat <- dat_2020 %>% 
            dplyr::rename(Date = date, Small = transect_small, Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2020)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## t-test of small vs large abundance, 2021
```{r}
aov_dat <- dat_2021 %>% 
            dplyr::rename(Date = date, Small = transect_small,
                          Large = transect_large) %>% 
            dplyr::select(Date, Small, Large) %>% 
            slice(23:nrow(dat_2021)) %>%
            filter(!is.na(Small)) %>%
            gather("Size", "Abundance", 2:3)

pairwise.t.test(aov_dat$Abundance, aov_dat$Size)

leveneTest(Abundance ~ Size, data = aov_dat)
```

## GLMs to test effects of water temp and sky condition on detections
```{r}
flights_DNF.rm <- filter(flights, detection!="DNF")
det_mod <- glm(detection~water_temp*sky,data=flights_DNF.rm,family=binomial(link="logit"),na.action="na.fail") 
dredge(det_mod)
```
* best model is null model, followed by ~wind_gust, then ~wind_speed

```{r}
DNF_1 <- glm(detection~wind_gust,data=flights,family=binomial(link="logit"))
summary(DNF_1)

ggplot(flights, aes(x=wind_gust, y=detection))+
  geom_count()
```
* even most-correlated variable wind_gust has insignificant effect on detection (p=0.441)

## Diagnostics
```{r}
DNF_simulationOutput <- simulateResiduals(fittedModel = DNF_1, n = 250)
plotSimulatedResiduals(simulationOutput = DNF_simulationOutput)
plotResiduals(flights$wind_spd, DNF_simulationOutput$scaledResiduals) #wonky residuals
#not sure what the differences between the two above lines are

testUniformity(simulationOutput = DNF_simulationOutput)
#testU gives QQ plot, looks normal

#effect plot
plot(allEffects(DNF_1), partial.residuals=TRUE)
```

# Swell analysis
```{r}
ggplot(flights, aes(x=log(swell)))+
  geom_histogram(binwidth=0.25)
```

## GLMs to test effects of wind speed, gust, and direction on swell height
```{r}
swell_mod <- glm(swell~wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="log"), na.action="na.fail")
dredge(swell_mod)
```
* ~wind gust is best, but only somewhat better than null model and ~wind_speed

```{r}
swell_1 <- glm(swell~wind_gust, data=flights, family=gaussian(link="log"))
summary(swell_1)
```
* 1 kt increase in wind gust speed increases log wave height by 0.028; not a significant effect

```{r}
#diagnostics
swell_simulationOutput <- simulateResiduals(fittedModel = swell_1, n = 250)
plotSimulatedResiduals(simulationOutput = swell_simulationOutput)

plot(allEffects(swell_1), partial.residuals=TRUE)
```

# Water Temperature Analysis

```{r}
ggplot(flights, aes(x=water_temp))+
  geom_histogram(binwidth=1)
```


# GLMs to test effects of air temp, wind speed, gust and direction on water temp
```{r}
wt_mod <- glm(water_temp~air_temp*wind_spd*wind_gust*wind_dir, data=flights, family=gaussian(link="identity"), na.action="na.fail")
View(dredge(wt_mod))
```
* best two models (equal AIC) are 
  + 1) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd+wind_gust*wind_dir
  + 2) ~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir

```{r}
wt_1 <- glm(water_temp~air_temp*wind_dir*wind_spd+wind_gust*wind_spd*wind_dir, data=flights, family=gaussian(link="identity"))
summary(wt_1)
```
* way too many coefficients, re-doing dredge with max terms = 4

```{r}
dredge(wt_mod, m.max=4)
```
* ~air_temp is best model

```{r}
wt_2 <- glm(water_temp~air_temp, data=flights, family=gaussian(link="identity"))
summary(wt_2)
```
* 1 degree increase in air temp increases water temp by 0.27 degrees, marginally significant effect (p<0.1)